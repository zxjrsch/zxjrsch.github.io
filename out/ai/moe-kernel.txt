1:"$Sreact.fragment"
2:I[48523,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"default"]
3:I[82281,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"default"]
4:I[1565,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/e5d291186d0cbb54.js","/_next/static/chunks/bc166ea53d390db7.js"],"default"]
5:I[96045,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/e5d291186d0cbb54.js","/_next/static/chunks/bc166ea53d390db7.js"],"default"]
12:I[75067,[],"default"]
:HL["/_next/static/chunks/1fcb9d377ba4db76.css","style"]
:HL["/_next/static/chunks/4057cc9dbcc744c0.css","style"]
:HL["/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
0:{"P":null,"b":"7aVZSWynhsXNLOyh7kz7D","c":["","ai","moe-kernel"],"q":"","i":false,"f":[[["",{"children":["ai",{"children":["moe-kernel",{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/1fcb9d377ba4db76.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/chunks/4057cc9dbcc744c0.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased","children":["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[["$","div",null,{"className":"flex flex-col justify-center min-h-screen","children":[["$","div",null,{"className":"flex flex-row justify-center pt-15","children":["$","$L4",null,{}]}],["$","div",null,{"className":"grow flex justify-center w-full pt-10 md:pt-15 px-3","children":["$","div",null,{"className":"flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl","children":[["$","div",null,{"className":"text-3xl font-bold flex justify-center","children":"GPU Kernel Level Optimization for Efficient MoE Training"}],["$","div",null,{"className":"text-sm flex justify-center pt-5","children":"Jan 20"}],["$","div",null,{"className":"","children":[["$","div","0",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Variable Length Grouped General Matrix Multiply"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Recall that the definition of the general matrix multiply (GEMM) is the operation ",["$","$L5",null,{"latex":"\\alpha AB + \\beta Z","displayMode":true,"className":"mt-10"}]," on matrices ",["$","$L5",null,{"latex":"A, B, Z","className":"mt-10"}]," and scalers ",["$","$L5",null,{"latex":"\\alpha, \\beta","className":"mt-10"}],". We shall be concerned with the special case ",["$","$L5",null,{"latex":"C = AB","displayMode":true,"className":"mt-10"}]," where ",["$","$L5",null,{"latex":"A","className":"mt-10"}]," has dimensions ",["$","$L5",null,{"latex":"M\\times K","className":"mt-10"}]," and ",["$","$L5",null,{"latex":"B","className":"mt-10"}]," has dimensions ",["$","$L5",null,{"latex":"K\\times N","className":"mt-10"}],", and we say will denote the problem size by ",["$","$L5",null,{"latex":"(M, N, K)","className":"mt-10"}],"."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["This special case is relevant in the mixture of experts layers where we have a set of such multiplications for the different tokens routed to the various experts. Within an MoE layer, these multiplications have possibly with different shapes due to the tokens routed to each expert. For example if ","$L6"," is the token dimension, we shall refer to the problem as variable length M grouped GEMM."]}],"$L7"]}]]}],"$L8","$L9","$La","$Lb"]}]]}]}],"$Lc"]}],["$Ld","$Le","$Lf"],"$L10"]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],"$L11",false]],"m":"$undefined","G":["$12",[]],"S":true}
13:I[56691,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/e5d291186d0cbb54.js","/_next/static/chunks/bc166ea53d390db7.js"],"default"]
14:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"OutletBoundary"]
15:"$Sreact.suspense"
17:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"ViewportBoundary"]
19:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"MetadataBoundary"]
6:["$","$L5",null,{"latex":"M","className":"mt-10"}]
7:["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"The up-projection of a given expert requires tokens routed to it to be gathered, while the down-projection has all tokens already contiguously packaged."}]
8:["$","div","1",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Forward and Backward MoE Kernels"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Let us turn to the kernel design in the SonicMoE paper.  A total of ",["$","$L5",null,{"latex":"8","className":"mt-10"}]," kernels are associated with the MoE layer. The forward computation has up-projection, down-projection, and expert aggregation kernels. Recall that in general, backwards gradients are computed with respective to the activations as well as for the weights. In the case of MoE, the backwards activation gradients are computed with respect to down-projection, up-projection and the MoE input. Two gradients with respect to up and down projection weight matrices are also computed. Moreover a top-k routing kernel is given in SonicMoE."]}]}]]}]
9:["$","div","2",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Kernel (Epilogue) Fusion"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"The first strategy is a kernel fusion of the input gather (during token routing) with input loading from global memory (HBM) to shared memory (SMEM). During this phase the token indices are first gathered, then the activations at those indicies are obtained via the cp.async PTX instruction. The authors of SonicMoE note that on Blackwell architecture, the 2-CTA GEMM requires the leader CTA to wait for gather to be complete in both CTAs, which leads to the following pipeline structure of the two CTAs: 1 warp to fetch token indices, 4 warps to gather, then 1 warp to relay the signal and perform MMA."}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"The second fusion occurs during the epilogue associated with the MoE layer. Specifically, the SwiGLU is fused with forward up-projection, while the backward of SwiGLU is fused with the down-projection gradient computation."}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Thirdly fusion involves the ",["$","$L5",null,{"latex":"dS","className":"mt-10"}]," and ",["$","$L5",null,{"latex":"dH","className":"mt-10"}]," associated with  the scores ",["$","$L5",null,{"latex":"S","className":"mt-10"}]," and down-project ",["$","$L5",null,{"latex":"H","className":"mt-10"}],", respectively."]}]]}]]}]
a:["$","div","3",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Computation and Async IO Overlap"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"A useful strategy for achieving high tensor core throughput with intense epilogue is called ping-pong scheduling where warp groups that collectively issues WGMMA overlap IO and GEMM at a given moment and exchange roles at a future time. Such a strategy was used in Flash Attention 3, and is used in SonicMoE's down projection activation gradient computation, and the down project forward epilogue."}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The paper also leverages async TMA for data movement between SMEM and GMEM in a subset of the ",["$","$L5",null,{"latex":"8","className":"mt-10"}]," kernels, for example to store forward down project, among other places."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"On Blackwell, the ping-poing strategy is used together with the TMEM and UMMA for better pipelining."}]]}]]}]
b:["$","div","4",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Top-K Sorting Kernel"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"The SonicMoE paper also provides a top-K kernel using bitonic sort, and has optional softmax fusion."}]}]]}]
c:["$","$L13",null,{}]
d:["$","script","script-0",{"src":"/_next/static/chunks/988ea292de4c4c73.js","async":true,"nonce":"$undefined"}]
e:["$","script","script-1",{"src":"/_next/static/chunks/e5d291186d0cbb54.js","async":true,"nonce":"$undefined"}]
f:["$","script","script-2",{"src":"/_next/static/chunks/bc166ea53d390db7.js","async":true,"nonce":"$undefined"}]
10:["$","$L14",null,{"children":["$","$15",null,{"name":"Next.MetadataOutlet","children":"$@16"}]}]
11:["$","$1","h",{"children":[null,["$","$L17",null,{"children":"$L18"}],["$","div",null,{"hidden":true,"children":["$","$L19",null,{"children":["$","$15",null,{"name":"Next.Metadata","children":"$L1a"}]}]}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]
18:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
1b:I[87718,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"IconMark"]
16:null
1a:[["$","title","0",{"children":"Claire Zhao Blog"}],["$","meta","1",{"name":"description","content":"Claire Zhao's Blog on AI and Math"}],["$","link","2",{"rel":"icon","href":"/favicon.ico?favicon.e10ff5c7.ico","sizes":"1176x1032","type":"image/x-icon"}],["$","$L1b","3",{}]]
