<!DOCTYPE html><!--0CqBaPu4ZjMxMPWU9ZJ5h--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/chunks/bbc7fa474c9f0ed8.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/4057cc9dbcc744c0.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/8775542cebd82754.js"/><script src="/_next/static/chunks/a931c44be7038752.js" async=""></script><script src="/_next/static/chunks/748df87cf5306c08.js" async=""></script><script src="/_next/static/chunks/turbopack-32b09ac787c4e8b4.js" async=""></script><script src="/_next/static/chunks/988ea292de4c4c73.js" async=""></script><script src="/_next/static/chunks/7a959126392b4956.js" async=""></script><script src="/_next/static/chunks/8292805af7f359b4.js" async=""></script><script src="/_next/static/chunks/9b725c66512530e6.js" async=""></script><script src="/_next/static/chunks/5ad9eb95768fc0a4.js" async=""></script><meta name="next-size-adjust" content=""/><title>Claire Zhao Blog</title><meta name="description" content="Claire Zhao&#x27;s Blog on AI and Math"/><link rel="icon" href="/favicon.ico?favicon.e10ff5c7.ico" sizes="1176x1032" type="image/x-icon"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased"><div hidden=""><!--$--><!--/$--></div><div class="flex flex-col justify-center min-h-screen"><div class="flex flex-row justify-center pt-15"><div class="flex flex-row w-full md:max-w-5/10 relative"><div class="flex flex-row md:basis-1/3 "><a href="/"><div class="text-2xl font-bold pl-5 md:pl-0 pt-5 md:pt-0">Claire Zhao</div></a></div><div class="md:hidden grow flex justify-end items-center pr-10"><button><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="size-5"><path d="M8 2H13.5C13.7761 2 14 2.22386 14 2.5V12.5C14 12.7761 13.7761 13 13.5 13H8V2ZM7 2H1.5C1.22386 2 1 2.22386 1 2.5V12.5C1 12.7761 1.22386 13 1.5 13H7V2ZM0 2.5C0 1.67157 0.671573 1 1.5 1H13.5C14.3284 1 15 1.67157 15 2.5V12.5C15 13.3284 14.3284 14 13.5 14H1.5C0.671573 14 0 13.3284 0 12.5V2.5Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg></button></div><nav class="hidden md:flex basis-2/3 flex-row justify-end gap-x-8"><a href="/ai"><div class="flex items-center gap-2 font-medium hover:underline">Machine Learning <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9 4L9 11L4.5 7.5L9 4Z" fill="currentColor"></path></svg></div></a><a href="/math"><div class="flex items-center gap-2 font-medium hover:underline">Mathematics </div></a></nav></div></div><div class="grow flex justify-center w-full pt-10 md:pt-15 px-3"><div class="flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl"><div class="text-3xl font-bold flex justify-center">Engram</div><div class="text-sm flex justify-center pt-5">Feb 23</div><div class=""><div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Engram is a newly proposed memory module for foundation models. The basic idea is to replace on-the-fly knowledge recomputation during forward pass by efficient knowledge lookup. In some sense engram is a new paradigm of neural network sparsity in addition to mixture of experts.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">A central problem explored by the engram proposal, which leads to a new scaling law is this: under constant model parameter count, how should capacility (whatever this may mean, reasoning capacity, knowledge capacity etc.) be divided amongst engram memory and MoE experts?</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Architectural Description</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">The engram conditional memory module performs the two operations of <span class="font-semibold">retrieval and fusion</span> for every token position in a token sequence.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Roughly speaking, the retrieval operator associates sequence local context with static memory entries through a deterministic hash function. Let us enter into a precise description of the steps.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Each word token is associated with an integer called its token ID, this is just recalling standard byte pair encoding (BPE) tokenization. Now we want to further merge sematically similar (normalized textual equivalence, lowercasing) tokens to achieve compression. With <span class="mt-10"></span> being the BPE vocabulary and <span class="mt-10"></span> being the compressed vocabular, the tokenizer compression is simply a map <span class="mt-10"></span>. Every token is first re-tokenized <span class="mt-10"></span> <span class="mt-10"></span> For each <span class="mt-10"></span> and <span class="mt-10"></span> consider the <span class="mt-10"></span>-grams <span class="mt-10"></span> which we <span class="font-semibold">hash</span> some <span class="mt-10"></span> times via the mappings <span class="mt-10"></span> <span class="mt-10"></span> The use of multiple hash maps is to prevent hash collision. In implementation, each <span class="mt-10"></span> is a multiplicative XOR hash. The <span class="mt-10"></span> is an index to a memory lookup table <span class="mt-10"></span> from which we retrieve embedding vectors <span class="mt-10"></span> Such vectors are then <span class="italic">concatenated</span> (denoted by <span class="mt-10"></span>) to give a <span class="font-semibold">memory embedding vector</span> <span class="mt-10"></span> associated with token <span class="mt-10"></span> <span class="mt-10"></span></div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">This is the retrieval stage, the other fusion stage invovles so called <span class="font-semibold">context aware gating</span>. The operation performed is as follows. Analogous to the QKV matrices, we project the memory embedding <span class="mt-10"></span> for each token position <span class="mt-10"></span> (we will now omit the index and write <span class="mt-10"></span> and likewise <span class="mt-10"></span> is the hidden embedding for token <span class="mt-10"></span> while agreeing that the operation is performed per token) through linear layers <span class="mt-10"></span> and <span class="mt-10"></span> and get <span class="mt-10"></span> We then  compute <span class="mt-10"></span> where <span class="mt-10"></span> is the usual root mean square normalizatio layer, and <span class="mt-10"></span> is the sigmoid. The <span class="font-semibold">gating</span> is simply the scalar vector product <span class="mt-10"></span> Let <span class="mt-10"></span> be the matrix whose rows are the token gated values <span class="mt-10"></span>, the following operation involves 1D convolution with kernel size <span class="mt-10"></span> and dilation <span class="mt-10"></span> <span class="mt-10"></span> where <span class="mt-10"></span>. This is followed by residual connection, namesly, if we denote the matrix of hidden embeddings <span class="mt-10"></span> (its <span class="mt-10"></span>-th row is <span class="mt-10"></span>) that is inputted to the engram layer, then output of the engram layer is <span class="mt-10"></span>.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify"><figure id="engram-arc" class="group flex flex-col items-center gap-4 py-10"><div role="button" tabindex="0" aria-label="Enlarge figure: Engram architecture" class="relative cursor-zoom-in overflow-hidden rounded-xl shadow-md dark:shadow-black/40 transition-all duration-300 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-indigo-500 dark:focus-visible:ring-indigo-400"><img alt="Engram architecture" loading="lazy" width="500" height="500" decoding="async" data-nimg="1" class="block w-full max-w-4xl h-auto transition-all duration-300 group-hover:brightness-[0.4] group-hover:scale-[1.015]" style="color:transparent" src="/_next/static/media/engram.f99d378f.png"/><div class="absolute inset-0 flex flex-col items-center justify-center gap-3 opacity-0 group-hover:opacity-100 transition-opacity duration-300 pointer-events-none select-none"><span class="text-white/90"><svg width="22" height="22" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.75" stroke-linecap="round" stroke-linejoin="round"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" y1="3" x2="14" y2="10"></line><line x1="3" y1="21" x2="10" y2="14"></line></svg></span><span class="text-white text-xs font-semibold tracking-[0.12em] uppercase">Click to enlarge</span></div></div><figcaption class="text-center text-lg italic text-zinc-500 dark:text-zinc-400 max-w-prose px-4">Engram architecture</figcaption></figure></div></div></div></div></div></div><div class="flex justify-center my-5 md:my-10">Claire Zhao Â© 2026</div></div><!--$--><!--/$--><script src="/_next/static/chunks/8775542cebd82754.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[48523,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"default\"]\n3:I[82281,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"default\"]\n4:I[1565,[\"/_next/static/chunks/8292805af7f359b4.js\",\"/_next/static/chunks/9b725c66512530e6.js\",\"/_next/static/chunks/5ad9eb95768fc0a4.js\"],\"default\"]\n10:I[75067,[],\"default\"]\n:HL[\"/_next/static/chunks/bbc7fa474c9f0ed8.css\",\"style\"]\n:HL[\"/_next/static/chunks/4057cc9dbcc744c0.css\",\"style\"]\n:HL[\"/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"0CqBaPu4ZjMxMPWU9ZJ5h\",\"c\":[\"\",\"ai\",\"engram\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"ai\",{\"children\":[\"engram\",{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/bbc7fa474c9f0ed8.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/4057cc9dbcc744c0.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col justify-center min-h-screen\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-row justify-center pt-15\",\"children\":[\"$\",\"$L4\",null,{}]}],[\"$\",\"div\",null,{\"className\":\"grow flex justify-center w-full pt-10 md:pt-15 px-3\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-3xl font-bold flex justify-center\",\"children\":\"Engram\"}],[\"$\",\"div\",null,{\"className\":\"text-sm flex justify-center pt-5\",\"children\":\"Feb 23\"}],[\"$\",\"div\",null,{\"className\":\"\",\"children\":[[\"$\",\"div\",\"0\",{\"children\":[\"$undefined\",[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":\"Engram is a newly proposed memory module for foundation models. The basic idea is to replace on-the-fly knowledge recomputation during forward pass by efficient knowledge lookup. In some sense engram is a new paradigm of neural network sparsity in addition to mixture of experts.\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":\"A central problem explored by the engram proposal, which leads to a new scaling law is this: under constant model parameter count, how should capacility (whatever this may mean, reasoning capacity, knowledge capacity etc.) be divided amongst engram memory and MoE experts?\"}]]}]]}],[\"$\",\"div\",\"1\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Architectural Description\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"The engram conditional memory module performs the two operations of \",\"$L5\",\" for every token position in a token sequence.\"]}],\"$L6\",\"$L7\",\"$L8\",\"$L9\"]}]]}]]}]]}]}],\"$La\"]}],[\"$Lb\",\"$Lc\",\"$Ld\"],\"$Le\"]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],\"$Lf\",false]],\"m\":\"$undefined\",\"G\":[\"$10\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"11:I[96045,[\"/_next/static/chunks/8292805af7f359b4.js\",\"/_next/static/chunks/9b725c66512530e6.js\",\"/_next/static/chunks/5ad9eb95768fc0a4.js\"],\"default\"]\n12:I[24237,[\"/_next/static/chunks/8292805af7f359b4.js\",\"/_next/static/chunks/9b725c66512530e6.js\",\"/_next/static/chunks/5ad9eb95768fc0a4.js\"],\"default\"]\n13:I[56691,[\"/_next/static/chunks/8292805af7f359b4.js\",\"/_next/static/chunks/9b725c66512530e6.js\",\"/_next/static/chunks/5ad9eb95768fc0a4.js\"],\"default\"]\n14:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"OutletBoundary\"]\n15:\"$Sreact.suspense\"\n17:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"ViewportBoundary\"]\n19:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"MetadataBoundary\"]\n5:[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"retrieval and fusion\"}]\n6:[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":\"Roughly speaking, the retrieval operator associates sequence local context with static memory entries through a deterministic hash function. Let us enter into a precise description of the steps.\"}]\n"])</script><script>self.__next_f.push([1,"7:[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"Each word token is associated with an integer called its token ID, this is just recalling standard byte pair encoding (BPE) tokenization. Now we want to further merge sematically similar (normalized textual equivalence, lowercasing) tokens to achieve compression. With \",[\"$\",\"$L11\",null,{\"latex\":\"V\",\"className\":\"mt-10\"}],\" being the BPE vocabulary and \",[\"$\",\"$L11\",null,{\"latex\":\"W\",\"className\":\"mt-10\"}],\" being the compressed vocabular, the tokenizer compression is simply a map \",[\"$\",\"$L11\",null,{\"latex\":\"\\\\mathcal{P}: V\\\\rightarrow W\",\"className\":\"mt-10\"}],\". Every token is first re-tokenized \",[\"$\",\"$L11\",null,{\"latex\":\"y_i=\\\\mathcal{P}(x_i)\",\"className\":\"mt-10\"}],\" \",[\"$\",\"$L11\",null,{\"latex\":\"x_1, \\\\dots, x_L \\\\mapsto y_1 , \\\\dots, y_L\",\"displayMode\":true,\"className\":\"mt-10\"}],\" For each \",[\"$\",\"$L11\",null,{\"latex\":\"1\\\\leq i \\\\leq L\",\"className\":\"mt-10\"}],\" and \",[\"$\",\"$L11\",null,{\"latex\":\"1\u003cn \\\\leq N\",\"className\":\"mt-10\"}],\" consider the \",[\"$\",\"$L11\",null,{\"latex\":\"n\",\"className\":\"mt-10\"}],\"-grams \",[\"$\",\"$L11\",null,{\"latex\":\"y_{i, n}=y_{i-(n-1)},\\\\dots,y_{i-1}, y_i\\\\in W^n\",\"displayMode\":true,\"className\":\"mt-10\"}],\" which we \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"hash\"}],\" some \",[\"$\",\"$L11\",null,{\"latex\":\"K\",\"className\":\"mt-10\"}],\" times via the mappings \",[\"$\",\"$L11\",null,{\"latex\":\"\\\\{h_{n, k}: W^n\\\\rightarrow \\\\mathbb{N}: 1\\\\leq k \\\\leq K\\\\}\",\"className\":\"mt-10\"}],\" \",[\"$\",\"$L11\",null,{\"latex\":\"z_{i, n, k} = h_{n,k}(y_{i,n})\",\"displayMode\":true,\"className\":\"mt-10\"}],\" The use of multiple hash maps is to prevent hash collision. In implementation, each \",[\"$\",\"$L11\",null,{\"latex\":\"h_k\",\"className\":\"mt-10\"}],\" is a multiplicative XOR hash. The \",[\"$\",\"$L11\",null,{\"latex\":\"z_{i, n, k}\",\"className\":\"mt-10\"}],\" is an index to a memory lookup table \",[\"$\",\"$L11\",null,{\"latex\":\"E_{n,k}\",\"className\":\"mt-10\"}],\" from which we retrieve embedding vectors \",[\"$\",\"$L11\",null,{\"latex\":\"e_{i,n,k}=E_{n,k}(z_{i,n,k})\",\"displayMode\":true,\"className\":\"mt-10\"}],\" Such vectors are then \",[\"$\",\"span\",null,{\"className\":\"italic\",\"children\":\"concatenated\"}],\" (denoted by \",[\"$\",\"$L11\",null,{\"latex\":\"\\\\oplus\",\"className\":\"mt-10\"}],\") to give a \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"memory embedding vector\"}],\" \",[\"$\",\"$L11\",null,{\"latex\":\"e_i\",\"className\":\"mt-10\"}],\" associated with token \",[\"$\",\"$L11\",null,{\"latex\":\"i\",\"className\":\"mt-10\"}],\" \",[\"$\",\"$L11\",null,{\"latex\":\"e_{i}=\\\\bigoplus_{n, k} e_{i,n,k}\",\"displayMode\":true,\"className\":\"mt-10\"}]]}]\n"])</script><script>self.__next_f.push([1,"8:[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"This is the retrieval stage, the other fusion stage invovles so called \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"context aware gating\"}],\". The operation performed is as follows. Analogous to the QKV matrices, we project the memory embedding \",[\"$\",\"$L11\",null,{\"latex\":\"e_i\",\"className\":\"mt-10\"}],\" for each token position \",[\"$\",\"$L11\",null,{\"latex\":\"i\",\"className\":\"mt-10\"}],\" (we will now omit the index and write \",[\"$\",\"$L11\",null,{\"latex\":\"e=e_i\",\"className\":\"mt-10\"}],\" and likewise \",[\"$\",\"$L11\",null,{\"latex\":\"h=h_i\",\"className\":\"mt-10\"}],\" is the hidden embedding for token \",[\"$\",\"$L11\",null,{\"latex\":\"i\",\"className\":\"mt-10\"}],\" while agreeing that the operation is performed per token) through linear layers \",[\"$\",\"$L11\",null,{\"latex\":\"A\",\"className\":\"mt-10\"}],\" and \",[\"$\",\"$L11\",null,{\"latex\":\"B\",\"className\":\"mt-10\"}],\" and get \",[\"$\",\"$L11\",null,{\"latex\":\"a = Ae,\\\\quad b = Be\",\"displayMode\":true,\"className\":\"mt-10\"}],\" We then  compute \",[\"$\",\"$L11\",null,{\"latex\":\"\\\\lambda = \\\\sigma\\\\left(\\\\frac{\\\\text{RMS}(h)^\\\\top\\\\, \\\\text{RMS}(a)}{\\\\sqrt{d}}\\\\right)\",\"displayMode\":true,\"className\":\"mt-10\"}],\" where \",[\"$\",\"$L11\",null,{\"latex\":\"\\\\textbf{RMS}\",\"className\":\"mt-10\"}],\" is the usual root mean square normalizatio layer, and \",[\"$\",\"$L11\",null,{\"latex\":\"\\\\sigma\",\"className\":\"mt-10\"}],\" is the sigmoid. The \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"gating\"}],\" is simply the scalar vector product \",[\"$\",\"$L11\",null,{\"latex\":\"c = \\\\lambda b\",\"displayMode\":true,\"className\":\"mt-10\"}],\" Let \",[\"$\",\"$L11\",null,{\"latex\":\"C\",\"className\":\"mt-10\"}],\" be the matrix whose rows are the token gated values \",[\"$\",\"$L11\",null,{\"latex\":\"c=c_i\",\"className\":\"mt-10\"}],\", the following operation involves 1D convolution with kernel size \",[\"$\",\"$L11\",null,{\"latex\":\"\\\\omega=4\",\"className\":\"mt-10\"}],\" and dilation \",[\"$\",\"$L11\",null,{\"latex\":\"\\\\delta = N\",\"className\":\"mt-10\"}],\" \",[\"$\",\"$L11\",null,{\"latex\":\"D = C + \\\\varphi(C)\",\"displayMode\":true,\"className\":\"mt-10\"}],\" where \",[\"$\",\"$L11\",null,{\"latex\":\"\\\\varphi=\\\\text{SiLU}\\\\circ\\\\text{Conv1D}_{\\\\omega, \\\\delta}\\\\circ\\\\text{RMS}\",\"className\":\"mt-10\"}],\". This is followed by residual connection, namesly, if we denote the matrix of hidden embeddings \",[\"$\",\"$L11\",null,{\"latex\":\"H\",\"className\":\"mt-10\"}],\" (its \",[\"$\",\"$L11\",null,{\"latex\":\"i\",\"className\":\"mt-10\"}],\"-th row is \",[\"$\",\"$L11\",null,{\"latex\":\"h_i\",\"className\":\"mt-10\"}],\") that is inputted to the engram layer, then output of the engram layer is \",[\"$\",\"$L11\",null,{\"latex\":\"H+D\",\"className\":\"mt-10\"}],\".\"]}]\n"])</script><script>self.__next_f.push([1,"9:[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"$\",\"$L12\",null,{\"src\":{\"src\":\"/_next/static/media/engram.f99d378f.png\",\"width\":1734,\"height\":1032,\"blurWidth\":8,\"blurHeight\":5,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAfUlEQVR42i2OWw6CMAAEuf/1iCKSqn0CRdMWaPo5VmS/NpvJZhpqUgzE8KGUwvL2jNNECJF932l+wOIn/GjIOaOtYXgIpDaklP6An2dG546Hl1Bc2o5r29cuT8BptLgdD1IqjLIMd4HRlmaNG/apUHXwbkZ0ffVJrGmrDpkvh8yVpJdLsacAAAAASUVORK5CYII=\"},\"caption\":\"Engram architecture\"}]}]\na:[\"$\",\"$L13\",null,{}]\nb:[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/8292805af7f359b4.js\",\"async\":true,\"nonce\":\"$undefined\"}]\nc:[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/9b725c66512530e6.js\",\"async\":true,\"nonce\":\"$undefined\"}]\nd:[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/5ad9eb95768fc0a4.js\",\"async\":true,\"nonce\":\"$undefined\"}]\ne:[\"$\",\"$L14\",null,{\"children\":[\"$\",\"$15\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@16\"}]}]\nf:[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L17\",null,{\"children\":\"$L18\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$L19\",null,{\"children\":[\"$\",\"$15\",null,{\"name\":\"Next.Metadata\",\"children\":\"$L1a\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}]\n"])</script><script>self.__next_f.push([1,"18:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"1b:I[87718,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"IconMark\"]\n16:null\n1a:[[\"$\",\"title\",\"0\",{\"children\":\"Claire Zhao Blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Claire Zhao's Blog on AI and Math\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/favicon.ico?favicon.e10ff5c7.ico\",\"sizes\":\"1176x1032\",\"type\":\"image/x-icon\"}],[\"$\",\"$L1b\",\"3\",{}]]\n"])</script></body></html>