<!DOCTYPE html><!--voNjnk_1CqFxO63BuH1js--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/chunks/bbc7fa474c9f0ed8.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/4057cc9dbcc744c0.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/8775542cebd82754.js"/><script src="/_next/static/chunks/a931c44be7038752.js" async=""></script><script src="/_next/static/chunks/748df87cf5306c08.js" async=""></script><script src="/_next/static/chunks/turbopack-32b09ac787c4e8b4.js" async=""></script><script src="/_next/static/chunks/988ea292de4c4c73.js" async=""></script><script src="/_next/static/chunks/7a959126392b4956.js" async=""></script><script src="/_next/static/chunks/8292805af7f359b4.js" async=""></script><script src="/_next/static/chunks/9b725c66512530e6.js" async=""></script><script src="/_next/static/chunks/5ad9eb95768fc0a4.js" async=""></script><meta name="next-size-adjust" content=""/><title>Claire Zhao Blog</title><meta name="description" content="Claire Zhao&#x27;s Blog on AI and Math"/><link rel="icon" href="/favicon.ico?favicon.e10ff5c7.ico" sizes="1176x1032" type="image/x-icon"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased"><div hidden=""><!--$--><!--/$--></div><div class="flex flex-col justify-center min-h-screen"><div class="flex flex-row justify-center pt-15"><div class="flex flex-row w-full md:max-w-5/10 relative"><div class="flex flex-row md:basis-1/3 "><a href="/"><div class="text-2xl font-bold pl-5 md:pl-0 pt-5 md:pt-0">Claire Zhao</div></a></div><div class="md:hidden grow flex justify-end items-center pr-10"><button><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="size-5"><path d="M8 2H13.5C13.7761 2 14 2.22386 14 2.5V12.5C14 12.7761 13.7761 13 13.5 13H8V2ZM7 2H1.5C1.22386 2 1 2.22386 1 2.5V12.5C1 12.7761 1.22386 13 1.5 13H7V2ZM0 2.5C0 1.67157 0.671573 1 1.5 1H13.5C14.3284 1 15 1.67157 15 2.5V12.5C15 13.3284 14.3284 14 13.5 14H1.5C0.671573 14 0 13.3284 0 12.5V2.5Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg></button></div><nav class="hidden md:flex basis-2/3 flex-row justify-end gap-x-8"><a href="/ai"><div class="flex items-center gap-2 font-medium hover:underline">Machine Learning <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9 4L9 11L4.5 7.5L9 4Z" fill="currentColor"></path></svg></div></a><a href="/math"><div class="flex items-center gap-2 font-medium hover:underline">Mathematics </div></a></nav></div></div><div class="grow flex justify-center w-full pt-10 md:pt-15 px-3"><div class="flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl"><div class="text-3xl font-bold flex justify-center">Engram and LLM Memory</div><div class="text-sm flex justify-center pt-5">Feb 23</div><div class=""><div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Engram is a newly proposed memory module for foundation models characterized in a recent DeepSeek paper. The basic idea is to replace on-the-fly knowledge recomputation during forward pass by efficient knowledge lookup. In some sense engram is a new paradigm of neural network sparsity in addition to mixture of experts.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">A central problem explored by the engram proposal, which leads to a new scaling law is this: under constant model parameter count, how should sparsely activated parameters be divided amongst engram memory and MoE experts?</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Architectural Description</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">The engram conditional memory module performs the two operations of <span class="font-semibold">retrieval and fusion</span> for every token position in a token sequence.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Roughly speaking, the retrieval operator associates sequence local context with static memory entries through a deterministic hash function. Let us enter into a precise description of the steps.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Each word token is associated with an integer called its token ID, this is just recalling standard byte pair encoding (BPE) tokenization. Now we want to further merge sematically similar (normalized textual equivalence, lowercasing) tokens to achieve compression. With <span class="mt-10"></span> being the BPE vocabulary and <span class="mt-10"></span> being the compressed vocabular, the tokenizer compression is simply a map <span class="mt-10"></span>. Every token is first re-tokenized <span class="mt-10"></span> <span class="mt-10"></span> For each <span class="mt-10"></span> and <span class="mt-10"></span> consider the <span class="mt-10"></span>-grams <span class="mt-10"></span> which we <span class="font-semibold">hash</span> some <span class="mt-10"></span> times via the mappings <span class="mt-10"></span> <span class="mt-10"></span> The use of multiple hash maps is to prevent hash collision. In implementation, each <span class="mt-10"></span> is a multiplicative XOR hash. The <span class="mt-10"></span> is an index to a memory lookup table <span class="mt-10"></span> from which we retrieve embedding vectors <span class="mt-10"></span> Such vectors are then <span class="italic">concatenated</span> (denoted by <span class="mt-10"></span>) to give a <span class="font-semibold">memory embedding vector</span> <span class="mt-10"></span> associated with token <span class="mt-10"></span> <span class="mt-10"></span></div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">This is the retrieval stage, the other fusion stage invovles so called <span class="font-semibold">context aware gating</span>. The operation performed is as follows. Analogous to the QKV matrices, we project the memory embedding <span class="mt-10"></span> for each token position <span class="mt-10"></span> (we will now omit the index and write <span class="mt-10"></span> and likewise <span class="mt-10"></span> is the hidden embedding for token <span class="mt-10"></span> while agreeing that the operation is performed per token) through linear layers <span class="mt-10"></span> and <span class="mt-10"></span> and get <span class="mt-10"></span> We then  compute <span class="mt-10"></span> where <span class="mt-10"></span> is the usual root mean square normalizatio layer, <span class="mt-10"></span> is the dimension of the vectors <span class="mt-10"></span> and <span class="mt-10"></span>, and <span class="mt-10"></span> is the sigmoid. The <span class="font-semibold">gating</span> is simply the scalar vector product <span class="mt-10"></span> Let <span class="mt-10"></span> be the matrix whose rows are the token gated values <span class="mt-10"></span>, the following operation involves 1D convolution with kernel size <span class="mt-10"></span> and dilation <span class="mt-10"></span> <span class="mt-10"></span> where <span class="mt-10"></span>. This is followed by residual connection, namesly, if we denote the matrix of hidden embeddings <span class="mt-10"></span> (its <span class="mt-10"></span>-th row is <span class="mt-10"></span>) that is inputted to the engram layer, then output of the engram layer is <span class="mt-10"></span>.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">The engram paper uses <span class="mt-10"></span> residual streams, each called a <span class="font-semibold">branch</span>. At each engram layer, and for each token, there are multiple hidden vectors <span class="mt-10"></span>, each corresponding to a branch. The above operations are repeated for each hidden vector <span class="mt-10"></span> with the condition that the memory embedding tables <span class="mt-10"></span> and the matrix <span class="mt-10"></span> are shared across all <span class="mt-10"></span> branches, whereas the matrices <span class="mt-10"></span> used for computing <span class="mt-10"></span> is distinct across branches. The architecture also uses <span class="italic">manifold constrained hyper connections</span>, which we will discuss in a separate blog.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify"><figure id="engram-arc" class="group flex flex-col items-center gap-4 py-10"><div role="button" tabindex="0" aria-label="Enlarge figure: Engram architecture" class="relative cursor-zoom-in overflow-hidden rounded-xl shadow-md dark:shadow-black/40 transition-all duration-300 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-indigo-500 dark:focus-visible:ring-indigo-400"><img alt="Engram architecture" loading="lazy" width="500" height="500" decoding="async" data-nimg="1" class="block w-full max-w-4xl h-auto transition-all duration-300 group-hover:brightness-[0.4] group-hover:scale-[1.015]" style="color:transparent" src="/_next/static/media/engram.f99d378f.png"/><div class="absolute inset-0 flex flex-col items-center justify-center gap-3 opacity-0 group-hover:opacity-100 transition-opacity duration-300 pointer-events-none select-none"><span class="text-white/90"><svg width="22" height="22" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.75" stroke-linecap="round" stroke-linejoin="round"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" y1="3" x2="14" y2="10"></line><line x1="3" y1="21" x2="10" y2="14"></line></svg></span><span class="text-white text-xs font-semibold tracking-[0.12em] uppercase">Click to enlarge</span></div></div><figcaption class="text-center text-lg italic text-zinc-500 dark:text-zinc-400 max-w-prose px-4">Engram architecture</figcaption></figure></div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Scaling Laws for Memory</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">In an engram model the number of inactive parameters (per token) is divided between the inactive engram parameters and the inactive MoE expert parameters. Define the ratio <span class="mt-10"></span> In experiments two compute regimes were explored, with  <span class="mt-10"></span> as many total parameters as activated parameters per token. First <span class="mt-10"></span> floating point operations, with constant total parameter <span class="mt-10"></span> billion parameters, and constant of <span class="mt-10"></span> million activated parameters per token. The pure MoE model corresponding the case <span class="mt-10"></span> consists of <span class="mt-10"></span> experts. The second compute regime has a fixed compute <span class="mt-10"></span> FLOPs, with fixed <span class="mt-10"></span> and in the pure MoE case <span class="mt-10"></span> there are <span class="mt-10"></span> experts.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify"><figure id="scaling-la" class="group flex flex-col items-center gap-4 py-10"><div role="button" tabindex="0" aria-label="Enlarge figure: Scaling law for engram: does adding memory improve model intelligence?" class="relative cursor-zoom-in overflow-hidden rounded-xl shadow-md dark:shadow-black/40 transition-all duration-300 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-indigo-500 dark:focus-visible:ring-indigo-400"><img alt="Scaling law for engram: does adding memory improve model intelligence?" loading="lazy" width="500" height="500" decoding="async" data-nimg="1" class="block w-full max-w-4xl h-auto transition-all duration-300 group-hover:brightness-[0.4] group-hover:scale-[1.015]" style="color:transparent" src="/_next/static/media/scaling_law.c5070287.png"/><div class="absolute inset-0 flex flex-col items-center justify-center gap-3 opacity-0 group-hover:opacity-100 transition-opacity duration-300 pointer-events-none select-none"><span class="text-white/90"><svg width="22" height="22" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.75" stroke-linecap="round" stroke-linejoin="round"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" y1="3" x2="14" y2="10"></line><line x1="3" y1="21" x2="10" y2="14"></line></svg></span><span class="text-white text-xs font-semibold tracking-[0.12em] uppercase">Click to enlarge</span></div></div><figcaption class="text-center text-lg italic text-zinc-500 dark:text-zinc-400 max-w-prose px-4">Scaling law for engram: does adding memory improve model intelligence?</figcaption></figure></div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Another set of experiments scaled up the number <span class="mt-10"></span> of memory slots over two order magnitudes from <span class="mt-10"></span>  to <span class="mt-10"></span>, with a <span class="mt-10"></span> model with <span class="mt-10"></span> activated parameters trained on <span class="mt-10"></span> tokens. The result follows a power law closely.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify"><figure id="scalaing-m" class="group flex flex-col items-center gap-4 py-10"><div role="button" tabindex="0" aria-label="Enlarge figure: Scalaing memory to practical infinity" class="relative cursor-zoom-in overflow-hidden rounded-xl shadow-md dark:shadow-black/40 transition-all duration-300 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-indigo-500 dark:focus-visible:ring-indigo-400"><img alt="Scalaing memory to practical infinity" loading="lazy" width="500" height="500" decoding="async" data-nimg="1" class="block w-full max-w-4xl h-auto transition-all duration-300 group-hover:brightness-[0.4] group-hover:scale-[1.015]" style="color:transparent" src="/_next/static/media/infinite_memory.f5bf1af4.png"/><div class="absolute inset-0 flex flex-col items-center justify-center gap-3 opacity-0 group-hover:opacity-100 transition-opacity duration-300 pointer-events-none select-none"><span class="text-white/90"><svg width="22" height="22" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.75" stroke-linecap="round" stroke-linejoin="round"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" y1="3" x2="14" y2="10"></line><line x1="3" y1="21" x2="10" y2="14"></line></svg></span><span class="text-white text-xs font-semibold tracking-[0.12em] uppercase">Click to enlarge</span></div></div><figcaption class="text-center text-lg italic text-zinc-500 dark:text-zinc-400 max-w-prose px-4">Scalaing memory to practical infinity</figcaption></figure></div></div></div></div></div></div><div class="flex justify-center my-5 md:my-10">Claire Zhao Â© 2026</div></div><!--$--><!--/$--><script src="/_next/static/chunks/8775542cebd82754.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[48523,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"default\"]\n3:I[82281,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"default\"]\n4:I[1565,[\"/_next/static/chunks/8292805af7f359b4.js\",\"/_next/static/chunks/9b725c66512530e6.js\",\"/_next/static/chunks/5ad9eb95768fc0a4.js\"],\"default\"]\n12:I[75067,[],\"default\"]\n:HL[\"/_next/static/chunks/bbc7fa474c9f0ed8.css\",\"style\"]\n:HL[\"/_next/static/chunks/4057cc9dbcc744c0.css\",\"style\"]\n:HL[\"/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"voNjnk-1CqFxO63BuH1js\",\"c\":[\"\",\"ai\",\"engram\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"ai\",{\"children\":[\"engram\",{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/bbc7fa474c9f0ed8.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/4057cc9dbcc744c0.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col justify-center min-h-screen\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-row justify-center pt-15\",\"children\":[\"$\",\"$L4\",null,{}]}],[\"$\",\"div\",null,{\"className\":\"grow flex justify-center w-full pt-10 md:pt-15 px-3\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-3xl font-bold flex justify-center\",\"children\":\"Engram and LLM Memory\"}],[\"$\",\"div\",null,{\"className\":\"text-sm flex justify-center pt-5\",\"children\":\"Feb 23\"}],[\"$\",\"div\",null,{\"className\":\"\",\"children\":[[\"$\",\"div\",\"0\",{\"children\":[\"$undefined\",[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":\"Engram is a newly proposed memory module for foundation models characterized in a recent DeepSeek paper. The basic idea is to replace on-the-fly knowledge recomputation during forward pass by efficient knowledge lookup. In some sense engram is a new paradigm of neural network sparsity in addition to mixture of experts.\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":\"A central problem explored by the engram proposal, which leads to a new scaling law is this: under constant model parameter count, how should sparsely activated parameters be divided amongst engram memory and MoE experts?\"}]]}]]}],[\"$\",\"div\",\"1\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Architectural Description\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"The engram conditional memory module performs the two operations of \",\"$L5\",\" for every token position in a token sequence.\"]}],\"$L6\",\"$L7\",\"$L8\",\"$L9\",\"$La\"]}]]}],\"$Lb\"]}]]}]}],\"$Lc\"]}],[\"$Ld\",\"$Le\",\"$Lf\"],\"$L10\"]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],\"$L11\",false]],\"m\":\"$undefined\",\"G\":[\"$12\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"13:I[96045,[\"/_next/static/chunks/8292805af7f359b4.js\",\"/_next/static/chunks/9b725c66512530e6.js\",\"/_next/static/chunks/5ad9eb95768fc0a4.js\"],\"default\"]\n14:I[24237,[\"/_next/static/chunks/8292805af7f359b4.js\",\"/_next/static/chunks/9b725c66512530e6.js\",\"/_next/static/chunks/5ad9eb95768fc0a4.js\"],\"default\"]\n15:I[56691,[\"/_next/static/chunks/8292805af7f359b4.js\",\"/_next/static/chunks/9b725c66512530e6.js\",\"/_next/static/chunks/5ad9eb95768fc0a4.js\"],\"default\"]\n16:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"OutletBoundary\"]\n17:\"$Sreact.suspense\"\n19:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"ViewportBoundary\"]\n1b:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"MetadataBoundary\"]\n5:[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"retrieval and fusion\"}]\n6:[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":\"Roughly speaking, the retrieval operator associates sequence local context with static memory entries through a deterministic hash function. Let us enter into a precise description of the steps.\"}]\n"])</script><script>self.__next_f.push([1,"7:[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"Each word token is associated with an integer called its token ID, this is just recalling standard byte pair encoding (BPE) tokenization. Now we want to further merge sematically similar (normalized textual equivalence, lowercasing) tokens to achieve compression. With \",[\"$\",\"$L13\",null,{\"latex\":\"V\",\"className\":\"mt-10\"}],\" being the BPE vocabulary and \",[\"$\",\"$L13\",null,{\"latex\":\"W\",\"className\":\"mt-10\"}],\" being the compressed vocabular, the tokenizer compression is simply a map \",[\"$\",\"$L13\",null,{\"latex\":\"\\\\mathcal{P}: V\\\\rightarrow W\",\"className\":\"mt-10\"}],\". Every token is first re-tokenized \",[\"$\",\"$L13\",null,{\"latex\":\"y_i=\\\\mathcal{P}(x_i)\",\"className\":\"mt-10\"}],\" \",[\"$\",\"$L13\",null,{\"latex\":\"x_1, \\\\dots, x_L \\\\mapsto y_1 , \\\\dots, y_L\",\"displayMode\":true,\"className\":\"mt-10\"}],\" For each \",[\"$\",\"$L13\",null,{\"latex\":\"1\\\\leq i \\\\leq L\",\"className\":\"mt-10\"}],\" and \",[\"$\",\"$L13\",null,{\"latex\":\"1\u003cn \\\\leq N\",\"className\":\"mt-10\"}],\" consider the \",[\"$\",\"$L13\",null,{\"latex\":\"n\",\"className\":\"mt-10\"}],\"-grams \",[\"$\",\"$L13\",null,{\"latex\":\"y_{i, n}=y_{i-(n-1)},\\\\dots,y_{i-1}, y_i\\\\in W^n\",\"displayMode\":true,\"className\":\"mt-10\"}],\" which we \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"hash\"}],\" some \",[\"$\",\"$L13\",null,{\"latex\":\"K\",\"className\":\"mt-10\"}],\" times via the mappings \",[\"$\",\"$L13\",null,{\"latex\":\"\\\\{h_{n, k}: W^n\\\\rightarrow \\\\mathbb{N}: 1\\\\leq k \\\\leq K\\\\}\",\"className\":\"mt-10\"}],\" \",[\"$\",\"$L13\",null,{\"latex\":\"z_{i, n, k} = h_{n,k}(y_{i,n})\",\"displayMode\":true,\"className\":\"mt-10\"}],\" The use of multiple hash maps is to prevent hash collision. In implementation, each \",[\"$\",\"$L13\",null,{\"latex\":\"h_k\",\"className\":\"mt-10\"}],\" is a multiplicative XOR hash. The \",[\"$\",\"$L13\",null,{\"latex\":\"z_{i, n, k}\",\"className\":\"mt-10\"}],\" is an index to a memory lookup table \",[\"$\",\"$L13\",null,{\"latex\":\"E_{n,k}\",\"className\":\"mt-10\"}],\" from which we retrieve embedding vectors \",[\"$\",\"$L13\",null,{\"latex\":\"e_{i,n,k}=E_{n,k}(z_{i,n,k})\",\"displayMode\":true,\"className\":\"mt-10\"}],\" Such vectors are then \",[\"$\",\"span\",null,{\"className\":\"italic\",\"children\":\"concatenated\"}],\" (denoted by \",[\"$\",\"$L13\",null,{\"latex\":\"\\\\oplus\",\"className\":\"mt-10\"}],\") to give a \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"memory embedding vector\"}],\" \",[\"$\",\"$L13\",null,{\"latex\":\"e_i\",\"className\":\"mt-10\"}],\" associated with token \",[\"$\",\"$L13\",null,{\"latex\":\"i\",\"className\":\"mt-10\"}],\" \",[\"$\",\"$L13\",null,{\"latex\":\"e_{i}=\\\\bigoplus_{n, k} e_{i,n,k}\",\"displayMode\":true,\"className\":\"mt-10\"}]]}]\n"])</script><script>self.__next_f.push([1,"8:[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"This is the retrieval stage, the other fusion stage invovles so called \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"context aware gating\"}],\". The operation performed is as follows. Analogous to the QKV matrices, we project the memory embedding \",[\"$\",\"$L13\",null,{\"latex\":\"e_i\",\"className\":\"mt-10\"}],\" for each token position \",[\"$\",\"$L13\",null,{\"latex\":\"i\",\"className\":\"mt-10\"}],\" (we will now omit the index and write \",[\"$\",\"$L13\",null,{\"latex\":\"e=e_i\",\"className\":\"mt-10\"}],\" and likewise \",[\"$\",\"$L13\",null,{\"latex\":\"h=h_i\",\"className\":\"mt-10\"}],\" is the hidden embedding for token \",[\"$\",\"$L13\",null,{\"latex\":\"i\",\"className\":\"mt-10\"}],\" while agreeing that the operation is performed per token) through linear layers \",[\"$\",\"$L13\",null,{\"latex\":\"A\",\"className\":\"mt-10\"}],\" and \",[\"$\",\"$L13\",null,{\"latex\":\"B\",\"className\":\"mt-10\"}],\" and get \",[\"$\",\"$L13\",null,{\"latex\":\"a = Ae,\\\\quad b = Be\",\"displayMode\":true,\"className\":\"mt-10\"}],\" We then  compute \",[\"$\",\"$L13\",null,{\"latex\":\"\\\\lambda = \\\\sigma\\\\left(\\\\frac{\\\\text{RMS}(h)^\\\\top\\\\, \\\\text{RMS}(a)}{\\\\sqrt{d}}\\\\right)\",\"displayMode\":true,\"className\":\"mt-10\"}],\" where \",[\"$\",\"$L13\",null,{\"latex\":\"\\\\textbf{RMS}\",\"className\":\"mt-10\"}],\" is the usual root mean square normalizatio layer, \",[\"$\",\"$L13\",null,{\"latex\":\"d\",\"className\":\"mt-10\"}],\" is the dimension of the vectors \",[\"$\",\"$L13\",null,{\"latex\":\"h\",\"className\":\"mt-10\"}],\" and \",[\"$\",\"$L13\",null,{\"latex\":\"a\",\"className\":\"mt-10\"}],\", and \",[\"$\",\"$L13\",null,{\"latex\":\"\\\\sigma\",\"className\":\"mt-10\"}],\" is the sigmoid. The \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"gating\"}],\" is simply the scalar vector product \",[\"$\",\"$L13\",null,{\"latex\":\"c = \\\\lambda b\",\"displayMode\":true,\"className\":\"mt-10\"}],\" Let \",[\"$\",\"$L13\",null,{\"latex\":\"C\",\"className\":\"mt-10\"}],\" be the matrix whose rows are the token gated values \",[\"$\",\"$L13\",null,{\"latex\":\"c=c_i\",\"className\":\"mt-10\"}],\", the following operation involves 1D convolution with kernel size \",[\"$\",\"$L13\",null,{\"latex\":\"\\\\omega=4\",\"className\":\"mt-10\"}],\" and dilation \",[\"$\",\"$L13\",null,{\"latex\":\"\\\\delta = N\",\"className\":\"mt-10\"}],\" \",[\"$\",\"$L13\",null,{\"latex\":\"D = C + \\\\varphi(C)\",\"displayMode\":true,\"className\":\"mt-10\"}],\" where \",[\"$\",\"$L13\",null,{\"latex\":\"\\\\varphi=\\\\text{SiLU}\\\\circ\\\\text{Conv1D}_{\\\\omega, \\\\delta}\\\\circ\\\\text{RMS}\",\"className\":\"mt-10\"}],\". This is followed by residual connection, namesly, if we denote the matrix of hidden embeddings \",[\"$\",\"$L13\",null,{\"latex\":\"H\",\"className\":\"mt-10\"}],\" (its \",[\"$\",\"$L13\",null,{\"latex\":\"i\",\"className\":\"mt-10\"}],\"-th row is \",[\"$\",\"$L13\",null,{\"latex\":\"h_i\",\"className\":\"mt-10\"}],\") that is inputted to the engram layer, then output of the engram layer is \",[\"$\",\"$L13\",null,{\"latex\":\"H+D\",\"className\":\"mt-10\"}],\".\"]}]\n"])</script><script>self.__next_f.push([1,"9:[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"The engram paper uses \",[\"$\",\"$L13\",null,{\"latex\":\"M=4\",\"className\":\"mt-10\"}],\" residual streams, each called a \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"branch\"}],\". At each engram layer, and for each token, there are multiple hidden vectors \",[\"$\",\"$L13\",null,{\"latex\":\"\\\\{h_m\\\\}_{m=1}^M\",\"className\":\"mt-10\"}],\", each corresponding to a branch. The above operations are repeated for each hidden vector \",[\"$\",\"$L13\",null,{\"latex\":\"h_m\",\"className\":\"mt-10\"}],\" with the condition that the memory embedding tables \",[\"$\",\"$L13\",null,{\"latex\":\"E_{n,k}\",\"className\":\"mt-10\"}],\" and the matrix \",[\"$\",\"$L13\",null,{\"latex\":\"B\",\"className\":\"mt-10\"}],\" are shared across all \",[\"$\",\"$L13\",null,{\"latex\":\"M\",\"className\":\"mt-10\"}],\" branches, whereas the matrices \",[\"$\",\"$L13\",null,{\"latex\":\"\\\\{A_m\\\\}_{m=1}^M\",\"className\":\"mt-10\"}],\" used for computing \",[\"$\",\"$L13\",null,{\"latex\":\"\\\\lambda\",\"className\":\"mt-10\"}],\" is distinct across branches. The architecture also uses \",[\"$\",\"span\",null,{\"className\":\"italic\",\"children\":\"manifold constrained hyper connections\"}],\", which we will discuss in a separate blog.\"]}]\na:[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"$\",\"$L14\",null,{\"src\":{\"src\":\"/_next/static/media/engram.f99d378f.png\",\"width\":1734,\"height\":1032,\"blurWidth\":8,\"blurHeight\":5,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAfUlEQVR42i2OWw6CMAAEuf/1iCKSqn0CRdMWaPo5VmS/NpvJZhpqUgzE8KGUwvL2jNNECJF932l+wOIn/GjIOaOtYXgIpDaklP6An2dG546Hl1Bc2o5r29cuT8BptLgdD1IqjLIMd4HRlmaNG/apUHXwbkZ0ffVJrGmrDpkvh8yVpJdLsacAAAAASUVORK5CYII=\"},\"caption\":\"Engram architecture\"}]}]\n"])</script><script>self.__next_f.push([1,"b:[\"$\",\"div\",\"2\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Scaling Laws for Memory\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"In an engram model the number of inactive parameters (per token) is divided between the inactive engram parameters and the inactive MoE expert parameters. Define the ratio \",[\"$\",\"$L13\",null,{\"latex\":\"\\\\rho=\\\\frac{\\\\text{\\\\# inactive MoE parameters}}{\\\\text{\\\\# inactive parameters }}\",\"displayMode\":true,\"className\":\"mt-10\"}],\" In experiments two compute regimes were explored, with  \",[\"$\",\"$L13\",null,{\"latex\":\"\\\\approx 10\\\\times\",\"className\":\"mt-10\"}],\" as many total parameters as activated parameters per token. First \",[\"$\",\"$L13\",null,{\"latex\":\"2\\\\times 10^{20}\",\"className\":\"mt-10\"}],\" floating point operations, with constant total parameter \",[\"$\",\"$L13\",null,{\"latex\":\"5.7\",\"className\":\"mt-10\"}],\" billion parameters, and constant of \",[\"$\",\"$L13\",null,{\"latex\":\"568\",\"className\":\"mt-10\"}],\" million activated parameters per token. The pure MoE model corresponding the case \",[\"$\",\"$L13\",null,{\"latex\":\"\\\\rho=1\",\"className\":\"mt-10\"}],\" consists of \",[\"$\",\"$L13\",null,{\"latex\":\"106\",\"className\":\"mt-10\"}],\" experts. The second compute regime has a fixed compute \",[\"$\",\"$L13\",null,{\"latex\":\"6\\\\times 10^{20}\",\"className\":\"mt-10\"}],\" FLOPs, with fixed \",[\"$\",\"$L13\",null,{\"latex\":\"(\\\\text{\\\\#total params}, \\\\text{\\\\#active params}) = (9.9B, 993M)\",\"displayMode\":true,\"className\":\"mt-10\"}],\" and in the pure MoE case \",[\"$\",\"$L13\",null,{\"latex\":\"\\\\rho=1\",\"className\":\"mt-10\"}],\" there are \",[\"$\",\"$L13\",null,{\"latex\":\"99\",\"className\":\"mt-10\"}],\" experts.\"]}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"$\",\"$L14\",null,{\"src\":{\"src\":\"/_next/static/media/scaling_law.c5070287.png\",\"width\":1028,\"height\":740,\"blurWidth\":8,\"blurHeight\":6,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAYAAAD+Bd/7AAAAfElEQVR42j2OSxLDIAxDuf9dKU1i/MEY1dCZLDxeSPOeiuoAi8LnRES8J2YgU5RODFWDuWNkyWOeT8x4RFBq/YBZMec/UB8Q31TBLYzS2gXJgoqd0lrrKDaVUlP2hu91J4UhuWUrKc9soI8sRCyIZtA7WmuoteJ5KAl6lD+Pu7swdmBgQAAAAABJRU5ErkJggg==\"},\"caption\":\"Scaling law for engram: does adding memory improve model intelligence?\"}]}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"Another set of experiments scaled up the number \",[\"$\",\"$L13\",null,{\"latex\":\"M\",\"className\":\"mt-10\"}],\" of memory slots over two order magnitudes from \",[\"$\",\"$L13\",null,{\"latex\":\"2.58\\\\times 10^5\",\"className\":\"mt-10\"}],\"  to \",[\"$\",\"$L13\",null,{\"latex\":\"10^7\",\"className\":\"mt-10\"}],\", with a \",[\"$\",\"$L13\",null,{\"latex\":\"3B\",\"className\":\"mt-10\"}],\" model with \",[\"$\",\"$L13\",null,{\"latex\":\"568M\",\"className\":\"mt-10\"}],\" activated parameters trained on \",[\"$\",\"$L13\",null,{\"latex\":\"100B\",\"className\":\"mt-10\"}],\" tokens. The result follows a power law closely.\"]}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"$\",\"$L14\",null,{\"src\":{\"src\":\"/_next/static/media/infinite_memory.f5bf1af4.png\",\"width\":806,\"height\":628,\"blurWidth\":8,\"blurHeight\":6,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAYAAAD+Bd/7AAAAcklEQVR42lWOWw5DIQhE3f9ejSYiL/VOwaRt7geBwGFmiohgzglmxjnnVe6OMsaAqv0Pz/Ob11ootVaIKFQMag6Nr7U39leh937JHUsPgCZjhq3EUc1QMsMYhLRKMKUtQGa5cMlPipBp1VoDEd3Q2RP6AKxou1N1u1yyAAAAAElFTkSuQmCC\"},\"caption\":\"Scalaing memory to practical infinity\"}]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"c:[\"$\",\"$L15\",null,{}]\nd:[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/8292805af7f359b4.js\",\"async\":true,\"nonce\":\"$undefined\"}]\ne:[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/9b725c66512530e6.js\",\"async\":true,\"nonce\":\"$undefined\"}]\nf:[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/5ad9eb95768fc0a4.js\",\"async\":true,\"nonce\":\"$undefined\"}]\n10:[\"$\",\"$L16\",null,{\"children\":[\"$\",\"$17\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@18\"}]}]\n11:[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L19\",null,{\"children\":\"$L1a\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$L1b\",null,{\"children\":[\"$\",\"$17\",null,{\"name\":\"Next.Metadata\",\"children\":\"$L1c\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}]\n"])</script><script>self.__next_f.push([1,"1a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"1d:I[87718,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"IconMark\"]\n18:null\n1c:[[\"$\",\"title\",\"0\",{\"children\":\"Claire Zhao Blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Claire Zhao's Blog on AI and Math\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/favicon.ico?favicon.e10ff5c7.ico\",\"sizes\":\"1176x1032\",\"type\":\"image/x-icon\"}],[\"$\",\"$L1d\",\"3\",{}]]\n"])</script></body></html>