<!DOCTYPE html><!--0CqBaPu4ZjMxMPWU9ZJ5h--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/chunks/bbc7fa474c9f0ed8.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/4057cc9dbcc744c0.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/8775542cebd82754.js"/><script src="/_next/static/chunks/a931c44be7038752.js" async=""></script><script src="/_next/static/chunks/748df87cf5306c08.js" async=""></script><script src="/_next/static/chunks/turbopack-32b09ac787c4e8b4.js" async=""></script><script src="/_next/static/chunks/988ea292de4c4c73.js" async=""></script><script src="/_next/static/chunks/7a959126392b4956.js" async=""></script><script src="/_next/static/chunks/2c133698af3170c9.js" async=""></script><script src="/_next/static/chunks/9b725c66512530e6.js" async=""></script><script src="/_next/static/chunks/5ad9eb95768fc0a4.js" async=""></script><meta name="next-size-adjust" content=""/><title>Claire Zhao Blog</title><meta name="description" content="Claire Zhao&#x27;s Blog on AI and Math"/><link rel="icon" href="/favicon.ico?favicon.e10ff5c7.ico" sizes="1176x1032" type="image/x-icon"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased"><div hidden=""><!--$--><!--/$--></div><div class="flex flex-col justify-center min-h-screen"><div class="flex flex-row justify-center pt-15"><div class="flex flex-row w-full md:max-w-5/10 relative"><div class="flex flex-row md:basis-1/3 "><a href="/"><div class="text-2xl font-bold pl-5 md:pl-0 pt-5 md:pt-0">Claire Zhao</div></a></div><div class="md:hidden grow flex justify-end items-center pr-10"><button><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="size-5"><path d="M8 2H13.5C13.7761 2 14 2.22386 14 2.5V12.5C14 12.7761 13.7761 13 13.5 13H8V2ZM7 2H1.5C1.22386 2 1 2.22386 1 2.5V12.5C1 12.7761 1.22386 13 1.5 13H7V2ZM0 2.5C0 1.67157 0.671573 1 1.5 1H13.5C14.3284 1 15 1.67157 15 2.5V12.5C15 13.3284 14.3284 14 13.5 14H1.5C0.671573 14 0 13.3284 0 12.5V2.5Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg></button></div><nav class="hidden md:flex basis-2/3 flex-row justify-end gap-x-8"><a href="/ai"><div class="flex items-center gap-2 font-medium hover:underline">Machine Learning <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9 4L9 11L4.5 7.5L9 4Z" fill="currentColor"></path></svg></div></a><a href="/math"><div class="flex items-center gap-2 font-medium hover:underline">Mathematics </div></a></nav></div></div><div class="grow flex justify-center w-full pt-10 md:pt-15 px-3"><div class="flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl"><div class="text-3xl font-bold flex justify-center">6D Parallelism for Distributed Training</div><div class="text-sm flex justify-center pt-5"></div><div class=""><div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Matrix multiplication is a fundamental operator in neural networks, and in the previous <a class="
    underline decoration-sky-500
    underline-offset-4
    " target="_blank" rel="noopener noreferrer" href="/ai/distributed-training">blog</a> we have fully elucidated the differentiation of the loss through such an operator in full generality, and we have applied this to the attention operator, a compound operator consisting of a series of matrix mulitiplications. Let us assemble a clear view of the process of training a neural network. In particular let us examine the data being stored and their persistence.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Neural network weights is the first pieces of data that needs to be stored for the duration of training, it is used during forward pass to compute activation, and it is used during backwards pass to compute activation gradients. The second piece of data which require storage are the activation of each layers, since these are used to compute the  gradient of weights in the layer to which the activations are the inputs during forward pass, as we have already seen. Activation can be released whenever the weight gradients they are associated with are computed. The third piece of data that is stored are activation gradients. They are used to compute gradients during backpropagation through the next layer (or operation), and can be released after computing activation and weight gradients in the next layer (or operation). The fourth piece of data are weight gradients, with a lifetime from produced until the corresponding weights are updated. The fifth type of data are associated with the optimization algorithm which updates the weight using the gradients. In the Adam optimizer, we need to keep track of the exponential moving average of the weight and the square of each weight. These moving averages are ususally called optimizer states, and persist throughout training.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">The effective management of the persistence and distribution (across GPUS and CPUs) is a core systems optimization problem.</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Distributed Data Parallel</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">We replicate a set of model weights identically across <span class="mt-10"></span> GPUs. Each GPU holds a distinct collection of training data. Each GPU passes forward with the data to compute the loss, and performs backward pass to compute the gradient. The gradient is <span class="font-semibold">all reduced</span> over <span class="mt-10"></span> parallel agents, and scaled by <span class="mt-10"></span> to obtain the average gradient. Each local parallel agent now holds identical copies of the global gradient. This gradient produces identical optimizer states across <span class="mt-10"></span> parallel agents. The optimizer have identical gradients, identical optimizer states, produces identical updated model weights across <span class="mt-10"></span> GPUs. The procedure is repeated.</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Fully Sharded Data Parallel</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Suppose a neural network consists of <span class="mt-10"></span> layers <span class="mt-10"></span> whereby each layer is a function with suitable domain and range. The neural network evaluated on <span class="mt-10"></span> leading to loss <span class="mt-10"></span>, is defined by <span class="mt-10"></span> Let us introduce the notation that <span class="mt-10"></span> is a function parameterized by weight <span class="mt-10"></span> and takes as input the previous layer&#x27;s activation <span class="mt-10"></span>.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Given dataset <span class="mt-10"></span>, weight matrices <span class="mt-10"></span> and <span class="mt-10"></span> parallel agents (GPU, TPU etc) we partition the dataset into <span class="mt-10"></span> pariwise disjoint subsets <span class="mt-10"></span> and partition each weight matrix into N slices, for example <span class="mt-10"></span> To the <span class="mt-10"></span>-th parallel agent we send <span class="mt-10"></span></div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">To describe one step in training, each of the parallel agents <span class="mt-10"></span> selects a training data <span class="mt-10"></span> and puts <span class="mt-10"></span>. For <span class="mt-10"></span> suppose the activation <span class="mt-10"></span> has been computed and the forward pass is ready to advance through layer <span class="mt-10"></span> on parallel agent <span class="mt-10"></span> we call <span class="mt-10"></span> and compute <span class="mt-10"></span> and immediately free the memory on device <span class="mt-10"></span> of the weight shards <span class="mt-10"></span> and store the layer-<span class="mt-10"></span> activation for weight gradient calculation through <span class="mt-10"></span> during backward pass <span class="mt-10"></span> Let us turn to backwards pass. In our notation <span class="mt-10"></span> computes the loss function and this layer has no weights, so as backwards we get <span class="mt-10"></span>just as reminder <span class="mt-10"></span> is the output of layer <span class="mt-10"></span>, which is the input to layer <span class="mt-10"></span>. We will treat <span class="mt-10"></span> as a variable, whereas the actual activation has a superscript <span class="mt-10"></span> to identify the device <span class="mt-10"></span>. For each <span class="mt-10"></span> supposed that device <span class="mt-10"></span> computed its local activation gradient <span class="mt-10"></span> have computed. Then the local weight gradient can be computed as a function <span class="mt-10"></span> of the map <span class="mt-10"></span>, the activation gradient, and the local activation <span class="mt-10"></span> Since weights are kept in shards across the parallel agents, we need to partition the weight gradient too <span class="mt-10"></span> we will use <span class="font-semibold">ReduceScatter</span> to provide each parallel agent with the average gradient corresponding to the weight shard it owns, namely it receives <span class="mt-10"></span> scaling by <span class="mt-10"></span> this is the average gradient for its local weight shard <span class="mt-10"></span>. The local Adam optimizer stores exponential moving averages of <span class="mt-10"></span> and the component-wise square of <span class="mt-10"></span>. Thus optimizer states are sharded as well. Finally each parallel agent <span class="mt-10"></span> needs to compute its local activation gradient as a function <span class="mt-10"></span> of the function <span class="mt-10"></span> (that we will differentiate via the chain rule) and the weight <span class="mt-10"></span> of layer <span class="mt-10"></span> together with <span class="mt-10"></span> namely <span class="mt-10"></span> Computing this allows  backpropagation to advance from layer <span class="mt-10"></span> to layer <span class="mt-10"></span>. To do this we need to AllGather the weight <span class="mt-10"></span> on each of the parallel agents  <span class="mt-10"></span> After computing the activaiton gradient <span class="mt-10"></span></div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Tensor Parallel</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Attention and MLP are two fundamental operators in a transformer, and their underlying mathematical operation is matrix multiplication. There are two fundamental ways to write matrix product with block matrices, <span class="mt-10"></span> <span class="mt-10"></span> giving rise to two ways in which a GEMM can be performed across GPUs: the first way requires separate block matrix multiplications followed by AllReduce, and the second way requires AllGather. Tensor parallelism is the application of these simple ideas to the matrix multiplications that occur in a transformer.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Let us take the MLP layer as the first example. Recall that in modern architectures the feedforward is computed as  <span class="mt-10"></span> where the Swish linear unit (SiLU) activation function <span class="mt-10"></span> is applied componentwise to <span class="mt-10"></span>. Let there be <span class="mt-10"></span> parallel agents, we can write <span class="mt-10"></span> so that each device <span class="mt-10"></span> computes <span class="mt-10"></span> then we can either AllGather <span class="mt-10"></span> and partition along the column direction and perform matrix multiplication of the first type above, then all reduce, or we can first partition <span class="mt-10"></span> along its column direction <span class="mt-10"></span> We then use <span class="font-semibold">AllToAll</span> to send <span class="mt-10"></span> from <span class="mt-10"></span> to <span class="mt-10"></span> so that on device <span class="mt-10"></span> we get <span class="mt-10"></span> which satisfies <span class="mt-10"></span> Now each device <span class="mt-10"></span> computes <span class="mt-10"></span> followed by AllReduce to obtain <span class="mt-10"></span></div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Sequence Parallel</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">The root mean square normalization with <span class="mt-10"></span> is a mapping <span class="mt-10"></span> <span class="mt-10"></span> Where <span class="mt-10"></span> is the componentwise product. The <span class="font-semibold">RMS normalization</span> layer of a LLM is a positionwise operator, meaning that for a sequence of <span class="mt-10"></span> token embeddings <span class="mt-10"></span> the RMSNorm layer with trainable weight <span class="mt-10"></span> transforms this sequence into <span class="mt-10"></span>. This positionwise property enables RMSNorm to be computed on separate GPUs then subsequently AllGathered, and is an effective method for managing long context. In particular, in a pre-norm transformer architecture, sequence parallel then be followed by tensor parallel we described above.</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Context Parallel</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">TBD</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Pipeline Parallel</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">TBD</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Expert Parallel</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">TBD</div></div></div></div></div></div><div class="flex justify-center my-5 md:my-10">Claire Zhao Â© 2026</div></div><!--$--><!--/$--><script src="/_next/static/chunks/8775542cebd82754.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[48523,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"default\"]\n3:I[82281,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"default\"]\n4:I[1565,[\"/_next/static/chunks/2c133698af3170c9.js\",\"/_next/static/chunks/9b725c66512530e6.js\",\"/_next/static/chunks/5ad9eb95768fc0a4.js\"],\"default\"]\n5:I[6366,[\"/_next/static/chunks/2c133698af3170c9.js\",\"/_next/static/chunks/9b725c66512530e6.js\",\"/_next/static/chunks/5ad9eb95768fc0a4.js\"],\"default\"]\n15:I[75067,[],\"default\"]\n:HL[\"/_next/static/chunks/bbc7fa474c9f0ed8.css\",\"style\"]\n:HL[\"/_next/static/chunks/4057cc9dbcc744c0.css\",\"style\"]\n:HL[\"/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n6:T501,Neural network weights is the first pieces of data that needs to be stored for the duration of training, it is used during forward pass to compute activation, and it is used during backwards pass to compute activation gradients. The second piece of data which require storage are the activation of each layers, since these are used to compute the  gradient of weights in the layer to which the activations are the inputs during forward pass, as we have already seen. Activation can be released whenever the weight gradients they are associated with are computed. The third piece of data that is stored are activation gradients. They are used to compute gradients during backpropagation through the next layer (or operation), and can be released after computing activation and weight gradients in the next layer (or operation). The fourth piece of data are weight gradients, with a lifetime from produced until the corresponding weights are updated. The fifth type of data are associated with the optimization algorithm which updates the weight using the gradients. In the Adam optimizer, we need to keep track of the exponential moving average of the weight and the square of each weight. These moving averages are ususally called optimizer states, and persist throughout training."])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"0CqBaPu4ZjMxMPWU9ZJ5h\",\"c\":[\"\",\"ai\",\"distributed-training\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"ai\",{\"children\":[\"distributed-training\",{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/bbc7fa474c9f0ed8.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/4057cc9dbcc744c0.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col justify-center min-h-screen\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-row justify-center pt-15\",\"children\":[\"$\",\"$L4\",null,{}]}],[\"$\",\"div\",null,{\"className\":\"grow flex justify-center w-full pt-10 md:pt-15 px-3\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-3xl font-bold flex justify-center\",\"children\":\"6D Parallelism for Distributed Training\"}],[\"$\",\"div\",null,{\"className\":\"text-sm flex justify-center pt-5\",\"children\":null}],[\"$\",\"div\",null,{\"className\":\"\",\"children\":[[\"$\",\"div\",\"0\",{\"children\":[\"$undefined\",[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"Matrix multiplication is a fundamental operator in neural networks, and in the previous \",[\"$\",\"$L5\",null,{\"text\":\"blog\",\"url\":\"/ai/distributed-training\"}],\" we have fully elucidated the differentiation of the loss through such an operator in full generality, and we have applied this to the attention operator, a compound operator consisting of a series of matrix mulitiplications. Let us assemble a clear view of the process of training a neural network. In particular let us examine the data being stored and their persistence.\"]}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":\"$6\"}],\"$L7\"]}]]}],\"$L8\",\"$L9\",\"$La\",\"$Lb\",\"$Lc\",\"$Ld\",\"$Le\"]}]]}]}],\"$Lf\"]}],[\"$L10\",\"$L11\",\"$L12\"],\"$L13\"]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],\"$L14\",false]],\"m\":\"$undefined\",\"G\":[\"$15\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"16:I[96045,[\"/_next/static/chunks/2c133698af3170c9.js\",\"/_next/static/chunks/9b725c66512530e6.js\",\"/_next/static/chunks/5ad9eb95768fc0a4.js\"],\"default\"]\n34:I[56691,[\"/_next/static/chunks/2c133698af3170c9.js\",\"/_next/static/chunks/9b725c66512530e6.js\",\"/_next/static/chunks/5ad9eb95768fc0a4.js\"],\"default\"]\n35:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"OutletBoundary\"]\n36:\"$Sreact.suspense\"\n38:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"ViewportBoundary\"]\n3a:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"MetadataBoundary\"]\n7:[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":\"The effective management of the persistence and distribution (across GPUS and CPUs) is a core systems optimization problem.\"}]\n8:[\"$\",\"div\",\"1\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Distributed Data Parallel\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"We replicate a set of model weights identically across \",[\"$\",\"$L16\",null,{\"latex\":\"N\",\"className\":\"mt-10\"}],\" GPUs. Each GPU holds a distinct collection of training data. Each GPU passes forward with the data to compute the loss, and performs backward pass to compute the gradient. The gradient is \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"all reduced\"}],\" over \",[\"$\",\"$L16\",null,{\"latex\":\"N\",\"className\":\"mt-10\"}],\" parallel agents, and scaled by \",[\"$\",\"$L16\",null,{\"latex\":\"N^{-1}\",\"className\":\"mt-10\"}],\" to obtain the average gradient. Each local parallel agent now holds identical copies of the global gradient. This gradient produces identical optimizer states across \",[\"$\",\"$L16\",null,{\"latex\":\"N\",\"className\":\"mt-10\"}],\" parallel agents. The optimizer have identical gradients, identical optimizer states, produces identical updated model weights across \",[\"$\",\"$L16\",null,{\"latex\":\"N\",\"className\":\"mt-10\"}],\" GPUs. The procedure is repeated.\"]}]}]]}]\n"])</script><script>self.__next_f.push([1,"9:[\"$\",\"div\",\"2\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Fully Sharded Data Parallel\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"Suppose a neural network consists of \",[\"$\",\"$L16\",null,{\"latex\":\"K\",\"className\":\"mt-10\"}],\" layers \",[\"$\",\"$L16\",null,{\"latex\":\"\\\\{L_i\\\\}_{i=1}^K\",\"className\":\"mt-10\"}],\" whereby each layer is a function with suitable domain and range. The neural network evaluated on \",[\"$\",\"$L16\",null,{\"latex\":\"x\",\"className\":\"mt-10\"}],\" leading to loss \",[\"$\",\"$L16\",null,{\"latex\":\"\\\\ell(x)\",\"className\":\"mt-10\"}],\", is defined by \",[\"$\",\"$L16\",null,{\"latex\":\"\\\\ell (x) = \\\\left(L_K\\\\circ \\\\dots \\\\circ L_1 \\\\right)(x)\",\"displayMode\":true,\"className\":\"mt-10\"}],\" Let us introduce the notation that \",[\"$\",\"$L16\",null,{\"latex\":\"L_i\",\"className\":\"mt-10\"}],\" is a function parameterized by weight \",[\"$\",\"$L16\",null,{\"latex\":\"W_i\",\"className\":\"mt-10\"}],\" and takes as input the previous layer's activation \",[\"$\",\"$L16\",null,{\"latex\":\"y_{i-1} = \\\\left(L_{i-1}\\\\circ \\\\dots \\\\circ L_1 \\\\right)(x)\",\"className\":\"mt-10\"}],\".\"]}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"Given dataset \",[\"$\",\"$L16\",null,{\"latex\":\"\\\\mathcal{D}\",\"className\":\"mt-10\"}],\", weight matrices \",[\"$\",\"$L16\",null,{\"latex\":\"\\\\{W_i\\\\}_{i=1}^K\",\"className\":\"mt-10\"}],\" and \",[\"$\",\"$L16\",null,{\"latex\":\"N\",\"className\":\"mt-10\"}],\" parallel agents (GPU, TPU etc) we partition the dataset into \",[\"$\",\"$L16\",null,{\"latex\":\"N\",\"className\":\"mt-10\"}],\" pariwise disjoint subsets \",[\"$\",\"$L16\",null,{\"latex\":\"\\\\mathcal{D} = \\\\mathcal{D}^{(1)}\\\\,\\\\cup \\\\dots \\\\cup \\\\mathcal{D}^{(N)}\",\"className\":\"mt-10\"}],\" and partition each weight matrix into N slices, for example \",[\"$\",\"$L16\",null,{\"latex\":\"W_i = [W_i^{(1)}| \\\\dots | W_i^{(N)}]\",\"displayMode\":true,\"className\":\"mt-10\"}],\" To the \",[\"$\",\"$L16\",null,{\"latex\":\"j\",\"className\":\"mt-10\"}],\"-th parallel agent we send \",[\"$\",\"$L16\",null,{\"latex\":\"\\\\left(\\\\mathcal{D}^{(j)}, \\\\, W_1^{(j)}, \\\\dots, W_K^{(j)}\\\\right)\",\"className\":\"mt-10\"}]]}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"To describe one step in training, each of the parallel agents \",[\"$\",\"$L16\",null,{\"latex\":\"1\\\\leq j \\\\leq N\",\"className\":\"mt-10\"}],\" selects a training data \",[\"$\",\"$L16\",null,{\"latex\":\"x^{(j)}\\\\in \\\\mathcal{D}^{(j)}\",\"className\":\"mt-10\"}],\" and puts \",[\"$\",\"$L16\",null,{\"latex\":\"y_0^{(j)}=x^{(j)}\",\"className\":\"mt-10\"}],\". For \",[\"$\",\"$L16\",null,{\"latex\":\"1\\\\leq i \\\\leq K\",\"className\":\"mt-10\"}],\" suppose the activation \",[\"$\",\"$L16\",null,{\"latex\":\"y_{i-1}^{(j)}\",\"className\":\"mt-10\"}],\" has been computed and the forward pass is ready to advance through layer \",[\"$\",\"$L16\",null,{\"latex\":\"L_i\",\"className\":\"mt-10\"}],\" on parallel agent \",[\"$\",\"$L16\",null,{\"latex\":\"j\",\"className\":\"mt-10\"}],\" we call \",[\"$\",\"$L16\",null,{\"latex\":\"W_i = \\\\textbf{AllGather}\\\\left\\\\{W_i^{(j)}: 1\\\\leq j \\\\leq N\\\\right\\\\}\",\"displayMode\":true,\"className\":\"mt-10\"}],\" and compute \",[\"$\",\"$L16\",null,{\"latex\":\"y^{(j)}_i=L_i(y^{(j)}_{i-1})\",\"className\":\"mt-10\"}],\" and immediately free the memory on device \",[\"$\",\"$L16\",null,{\"latex\":\"j\",\"className\":\"mt-10\"}],\" of the weight shards \",[\"$\",\"$L16\",null,{\"latex\":\"\\\\textbf{delete}\\\\left\\\\{W_i^{(k)}\\\\in W_i: j\\\\neq k\\\\right\\\\}\\\\quad (\\\\text{device}\\\\,j)\",\"displayMode\":true,\"className\":\"mt-10\"}],\" and store the layer-\",[\"$\",\"$L16\",null,{\"latex\":\"i\",\"className\":\"mt-10\"}],\" activation for weight gradient calculation through \",[\"$\",\"$L16\",null,{\"latex\":\"L_i\",\"className\":\"mt-10\"}],\" during backward pass \",[\"$\",\"$L16\",null,{\"latex\":\"\\\\textbf{store}\\\\,\\\\, y_i^{(j)}\\\\quad(\\\\text{device } j)\",\"displayMode\":true,\"className\":\"mt-10\"}],\" Let us turn to backwards pass. In our notation \",[\"$\",\"$L16\",null,{\"latex\":\"L_K\",\"className\":\"mt-10\"}],\" computes the loss function and this layer has no weights, so as backwards we get \",[\"$\",\"$L16\",null,{\"latex\":\"\\\\frac{\\\\partial \\\\ell}{\\\\partial y_{K-1}}\",\"displayMode\":true,\"className\":\"mt-10\"}],\"just as reminder \",[\"$\",\"$L16\",null,{\"latex\":\"y_{K-1}\",\"className\":\"mt-10\"}],\" is the output of layer \",[\"$\",\"$L16\",null,{\"latex\":\"K-1\",\"className\":\"mt-10\"}],\", which is the input to layer \",\"$L17\",\". We will treat \",\"$L18\",\" as a variable, whereas the actual activation has a superscript \",\"$L19\",\" to identify the device \",\"$L1a\",\". For each \",\"$L1b\",\" supposed that device \",\"$L1c\",\" computed its local activation gradient \",\"$L1d\",\" have computed. Then the local weight gradient can be computed as a function \",\"$L1e\",\" of the map \",\"$L1f\",\", the activation gradient, and the local activation \",\"$L20\",\" Since weights are kept in shards across the parallel agents, we need to partition the weight gradient too \",\"$L21\",\" we will use \",\"$L22\",\" to provide each parallel agent with the average gradient corresponding to the weight shard it owns, namely it receives \",\"$L23\",\" scaling by \",\"$L24\",\" this is the average gradient for its local weight shard \",\"$L25\",\". The local Adam optimizer stores exponential moving averages of \",\"$L26\",\" and the component-wise square of \",\"$L27\",\". Thus optimizer states are sharded as well. Finally each parallel agent \",\"$L28\",\" needs to compute its local activation gradient as a function \",\"$L29\",\" of the function \",\"$L2a\",\" (that we will differentiate via the chain rule) and the weight \",\"$L2b\",\" of layer \",\"$L2c\",\" together with \",\"$L2d\",\" namely \",\"$L2e\",\" Computing this allows  backpropagation to advance from layer \",\"$L2f\",\" to layer \",\"$L30\",\". To do this we need to AllGather the weight \",\"$L31\",\" on each of the parallel agents  \",\"$L32\",\" After computing the activaiton gradient \",\"$L33\"]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"a:[\"$\",\"div\",\"3\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Tensor Parallel\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"Attention and MLP are two fundamental operators in a transformer, and their underlying mathematical operation is matrix multiplication. There are two fundamental ways to write matrix product with block matrices, \",[\"$\",\"$L16\",null,{\"latex\":\"AB=\\\\begin{bmatrix}A_1 \\\\quad A_2 \\\\end{bmatrix}\\\\begin{bmatrix}B_1 \\\\\\\\ B_2 \\\\end{bmatrix}=[A_1B_1+A_2B_2]\",\"displayMode\":true,\"className\":\"mt-10\"}],\" \",[\"$\",\"$L16\",null,{\"latex\":\" AB=A \\\\begin{bmatrix} B_1\\\\quad B_2 \\\\end{bmatrix} = \\\\begin{bmatrix}AB_1\\\\quad AB_2  \\\\end{bmatrix}\",\"displayMode\":true,\"className\":\"mt-10\"}],\" giving rise to two ways in which a GEMM can be performed across GPUs: the first way requires separate block matrix multiplications followed by AllReduce, and the second way requires AllGather. Tensor parallelism is the application of these simple ideas to the matrix multiplications that occur in a transformer.\"]}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"Let us take the MLP layer as the first example. Recall that in modern architectures the feedforward is computed as  \",[\"$\",\"$L16\",null,{\"latex\":\"\\\\text{SwiGLU}(X, W_1, W_2, W_3) = W_3\\\\left(\\\\text{SiLU}(W_1X)\\\\,\\\\odot\\\\, W_2 X\\\\right)\",\"displayMode\":true,\"className\":\"mt-10\"}],\" where the Swish linear unit (SiLU) activation function \",[\"$\",\"$L16\",null,{\"latex\":\"\\\\text{SiLU}(a) = \\\\frac{a}{1+e^{-a}}\\\\quad (a\\\\in\\\\mathbb{R})\",\"displayMode\":true,\"className\":\"mt-10\"}],\" is applied componentwise to \",[\"$\",\"$L16\",null,{\"latex\":\"X\",\"className\":\"mt-10\"}],\". Let there be \",[\"$\",\"$L16\",null,{\"latex\":\"N\",\"className\":\"mt-10\"}],\" parallel agents, we can write \",[\"$\",\"$L16\",null,{\"latex\":\"W_i = [W_i^{(1)}\\\\, \\\\mid\\\\, \\\\dots \\\\mid\\\\, W_i^{(N)}]\\\\quad (1\\\\leq i\\\\leq 3)\",\"displayMode\":true,\"className\":\"mt-10\"}],\" so that each device \",[\"$\",\"$L16\",null,{\"latex\":\"1\\\\leq j \\\\leq N\",\"className\":\"mt-10\"}],\" computes \",[\"$\",\"$L16\",null,{\"latex\":\"Y^{(j)}=\\\\text{SiLU}(W_1^{(j)}X)\\\\,\\\\odot\\\\, W_2^{(j)} X\",\"displayMode\":true,\"className\":\"mt-10\"}],\" then we can either AllGather \",[\"$\",\"$L16\",null,{\"latex\":\"Y^{(j)}\",\"className\":\"mt-10\"}],\" and partition along the column direction and perform matrix multiplication of the first type above, then all reduce, or we can first partition \",[\"$\",\"$L16\",null,{\"latex\":\"Y^{(j)}\",\"className\":\"mt-10\"}],\" along its column direction \",[\"$\",\"$L16\",null,{\"latex\":\"Y^{(j)} =\\\\begin{bmatrix}Y^{{(1j)}} \\\\\\\\ \\\\\\\\ \\\\vdots \\\\\\\\ \\\\\\\\ Y^{{(Nj)}} \\\\end{bmatrix}\",\"displayMode\":true,\"className\":\"mt-10\"}],\" We then use \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"AllToAll\"}],\" to send \",[\"$\",\"$L16\",null,{\"latex\":\"Y^{(ij)}\",\"className\":\"mt-10\"}],\" from \",[\"$\",\"$L16\",null,{\"latex\":\"j\",\"className\":\"mt-10\"}],\" to \",[\"$\",\"$L16\",null,{\"latex\":\"1\\\\leq i \\\\leq N\",\"className\":\"mt-10\"}],\" so that on device \",[\"$\",\"$L16\",null,{\"latex\":\"j\",\"className\":\"mt-10\"}],\" we get \",[\"$\",\"$L16\",null,{\"latex\":\"\\\\widehat{Y}^{(j)} = [Y^{(j1)}\\\\dots\\\\, Y^{(jN)}]\",\"displayMode\":true,\"className\":\"mt-10\"}],\" which satisfies \",[\"$\",\"$L16\",null,{\"latex\":\"\\\\text{SiLU}(W_1X)\\\\,\\\\odot\\\\, W_2 X = \\\\begin{bmatrix}\\\\widehat{Y}^{(1)} \\\\\\\\ \\\\\\\\ \\\\vdots \\\\\\\\ \\\\\\\\ \\\\widehat{Y}^{(N)}\\\\end{bmatrix}\",\"displayMode\":true,\"className\":\"mt-10\"}],\" Now each device \",[\"$\",\"$L16\",null,{\"latex\":\"j\",\"className\":\"mt-10\"}],\" computes \",[\"$\",\"$L16\",null,{\"latex\":\"W_3^{(j)} \\\\widehat{Y}^{(j)}\",\"className\":\"mt-10\"}],\" followed by AllReduce to obtain \",[\"$\",\"$L16\",null,{\"latex\":\"W_3\\\\left(\\\\text{SiLU}(W_1X)\\\\,\\\\odot\\\\, W_2 X\\\\right)=\\\\textbf{AllReduce}\\\\left\\\\{W_3^{(j)} \\\\widehat{Y}^{(j)}: 1\\\\leq j \\\\leq N\\\\right\\\\}\",\"displayMode\":true,\"className\":\"mt-10\"}]]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"b:[\"$\",\"div\",\"4\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Sequence Parallel\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"The root mean square normalization with \",[\"$\",\"$L16\",null,{\"latex\":\"\\\\varepsilon \u003e 0\",\"className\":\"mt-10\"}],\" is a mapping \",[\"$\",\"$L16\",null,{\"latex\":\"\\\\rho:\\\\mathbb{R}^d\\\\times \\\\mathbb{R}^d\\\\rightarrow \\\\mathbb{R}^d\",\"className\":\"mt-10\"}],\" \",[\"$\",\"$L16\",null,{\"latex\":\"\\\\rho(x, w) = \\\\frac{x\\\\,\\\\odot\\\\,w}{\\\\left(\\\\frac{x^\\\\top x}{d} + \\\\varepsilon\\\\right)^{1/2}}\",\"displayMode\":true,\"className\":\"mt-10\"}],\" Where \",[\"$\",\"$L16\",null,{\"latex\":\"(x\\\\,\\\\odot\\\\,w)_i=x_iw_i\",\"className\":\"mt-10\"}],\" is the componentwise product. The \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"RMS normalization\"}],\" layer of a LLM is a positionwise operator, meaning that for a sequence of \",[\"$\",\"$L16\",null,{\"latex\":\"L\",\"className\":\"mt-10\"}],\" token embeddings \",[\"$\",\"$L16\",null,{\"latex\":\"\\\\{t_1,\\\\dots, t_L\\\\}\\\\subset \\\\mathbb{R}^d\",\"className\":\"mt-10\"}],\" the RMSNorm layer with trainable weight \",[\"$\",\"$L16\",null,{\"latex\":\"w\",\"className\":\"mt-10\"}],\" transforms this sequence into \",[\"$\",\"$L16\",null,{\"latex\":\"\\\\{\\\\rho(t_1, w), \\\\dots, \\\\rho(t_L, w)\\\\}\",\"className\":\"mt-10\"}],\". This positionwise property enables RMSNorm to be computed on separate GPUs then subsequently AllGathered, and is an effective method for managing long context. In particular, in a pre-norm transformer architecture, sequence parallel then be followed by tensor parallel we described above.\"]}]}]]}]\n"])</script><script>self.__next_f.push([1,"c:[\"$\",\"div\",\"5\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Context Parallel\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":\"TBD\"}]}]]}]\nd:[\"$\",\"div\",\"6\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Pipeline Parallel\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":\"TBD\"}]}]]}]\ne:[\"$\",\"div\",\"7\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Expert Parallel\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":\"TBD\"}]}]]}]\nf:[\"$\",\"$L34\",null,{}]\n10:[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/2c133698af3170c9.js\",\"async\":true,\"nonce\":\"$undefined\"}]\n11:[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/9b725c66512530e6.js\",\"async\":true,\"nonce\":\"$undefined\"}]\n12:[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/5ad9eb95768fc0a4.js\",\"async\":true,\"nonce\":\"$undefined\"}]\n13:[\"$\",\"$L35\",null,{\"children\":[\"$\",\"$36\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@37\"}]}]\n14:[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L38\",null,{\"children\":\"$L39\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$L3a\",null,{\"children\":[\"$\",\"$36\",null,{\"name\":\"Next.Metadata\",\"children\":\"$L3b\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}]\n"])</script><script>self.__next_f.push([1,"17:[\"$\",\"$L16\",null,{\"latex\":\"K\",\"className\":\"mt-10\"}]\n18:[\"$\",\"$L16\",null,{\"latex\":\"y_{K-1}\",\"className\":\"mt-10\"}]\n19:[\"$\",\"$L16\",null,{\"latex\":\"y_i^{(j)}\",\"className\":\"mt-10\"}]\n1a:[\"$\",\"$L16\",null,{\"latex\":\"j\",\"className\":\"mt-10\"}]\n1b:[\"$\",\"$L16\",null,{\"latex\":\"1\\\\leq i \\\\leq K-1\",\"className\":\"mt-10\"}]\n1c:[\"$\",\"$L16\",null,{\"latex\":\"j\",\"className\":\"mt-10\"}]\n1d:[\"$\",\"$L16\",null,{\"latex\":\"\\\\frac{\\\\partial \\\\ell}{\\\\partial y_i}\",\"displayMode\":true,\"className\":\"mt-10\"}]\n1e:[\"$\",\"$L16\",null,{\"latex\":\"f\",\"className\":\"mt-10\"}]\n1f:[\"$\",\"$L16\",null,{\"latex\":\"L_i\",\"className\":\"mt-10\"}]\n20:[\"$\",\"$L16\",null,{\"latex\":\"\\\\frac{\\\\partial \\\\ell}{\\\\partial W_i} = f\\\\left(y_i^{(j)},\\\\, \\\\frac{\\\\partial \\\\ell}{\\\\partial y_i}, L_i \\\\right) \\\\quad (\\\\text{weight gradient})\",\"displayMode\":true,\"className\":\"mt-10\"}]\n21:[\"$\",\"$L16\",null,{\"latex\":\"\\\\frac{\\\\partial \\\\ell}{\\\\partial W_i} = \\\\left[\\\\frac{\\\\partial \\\\ell}{\\\\partial W_i^{(1)}}\\\\,\\\\middle|\\\\quad\\\\dots\\\\quad \\\\middle|\\\\, \\\\frac{\\\\partial \\\\ell}{\\\\partial W_i^{(N)}} \\\\right]\",\"displayMode\":true,\"className\":\"mt-10\"}]\n22:[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"ReduceScatter\"}]\n23:[\"$\",\"$L16\",null,{\"latex\":\"\\\\sum_{m=1}^N \\\\frac{\\\\partial \\\\ell}{\\\\partial W_i^{(j)}}\\\\quad (\\\\text{device}\\\\, j)\",\"displayMode\":true,\"className\":\"mt-10\"}]\n24:[\"$\",\"$L16\",null,{\"latex\":\"N^{-1}\",\"className\":\"mt-10\"}]\n25:[\"$\",\"$L16\",null,{\"latex\":\"W_i^{(j)}\",\"className\":\"mt-10\"}]\n26:[\"$\",\"$L16\",null,{\"latex\":\"W_i^{(j)}\",\"className\":\"mt-10\"}]\n27:[\"$\",\"$L16\",null,{\"latex\":\"W_i^{(j)}\",\"className\":\"mt-10\"}]\n28:[\"$\",\"$L16\",null,{\"latex\":\"j\",\"className\":\"mt-10\"}]\n29:[\"$\",\"$L16\",null,{\"latex\":\"g\",\"className\":\"mt-10\"}]\n2a:[\"$\",\"$L16\",null,{\"latex\":\"L_i\",\"className\":\"mt-10\"}]\n2b:[\"$\",\"$L16\",null,{\"latex\":\"W_i\",\"className\":\"mt-10\"}]\n2c:[\"$\",\"$L16\",null,{\"latex\":\"i\",\"className\":\"mt-10\"}]\n2d:[\"$\",\"$L16\",null,{\"latex\":\"\\\\partial\\\\ell / \\\\partial y_i\",\"className\":\"mt-10\"}]\n2e:[\"$\",\"$L16\",null,{\"latex\":\"\\\\frac{\\\\partial \\\\ell}{\\\\partial y_{i-1}} = g\\\\left(\\\\frac{\\\\partial\\\\ell}{\\\\partial y_i}, \\\\, W_i,\\\\, L_i\\\\right)\\\\quad (\\\\text{activation gradient})\",\"displayMode\":true,\"className\":\"mt-10\"}]\n2f:[\"$\",\"$L16\",null,{\"latex\":\"i\",\"className\":\"mt-10\"}]\n30:[\"$\",\"$L16\",null,{\"latex\":\"i-1\",\"className\":\"mt-10\"}]\n31:[\"$\",\"$L16\",null,{\"latex\":\"W_i\",\"className\":\"mt-10\"}]\n32:[\"$\",\"$L16\",null,{\"latex\":\"W_i = \\\\textbf{AllGather}\\\\left\\\\{W_i^{(j)}: 1\\\\leq j \\\\leq N\\\\right\\\\}\",\"displayMode\":true,\"className\":\"mt-10\"}]\n33:[\"$\",\"$L16\",null,{\"latex\":\"\\\\textbf{delete}\\\\left\\\\{W_i^{(k)}\\\\in W_i: j\\\\neq k\\\\right\\\\}\\\\quad (\\\\text{device}\\\\,j)\",\"displayMode\":true,\"className\":\"mt-10\"}]\n"])</script><script>self.__next_f.push([1,"39:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"3c:I[87718,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"IconMark\"]\n37:null\n3b:[[\"$\",\"title\",\"0\",{\"children\":\"Claire Zhao Blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Claire Zhao's Blog on AI and Math\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/favicon.ico?favicon.e10ff5c7.ico\",\"sizes\":\"1176x1032\",\"type\":\"image/x-icon\"}],[\"$\",\"$L3c\",\"3\",{}]]\n"])</script></body></html>