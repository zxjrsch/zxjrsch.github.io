1:"$Sreact.fragment"
2:I[48523,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"default"]
3:I[82281,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"default"]
4:I[1565,["/_next/static/chunks/e02326bdb730da03.js","/_next/static/chunks/bc166ea53d390db7.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
5:I[96045,["/_next/static/chunks/e02326bdb730da03.js","/_next/static/chunks/bc166ea53d390db7.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
11:I[75067,[],"default"]
:HL["/_next/static/chunks/34db3e12e547f437.css","style"]
:HL["/_next/static/chunks/4057cc9dbcc744c0.css","style"]
:HL["/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
0:{"P":null,"b":"GWFjHkMuZfgY9jTcVJppH","c":["","ai","blackwell-gemm-intro"],"q":"","i":false,"f":[[["",{"children":["ai",{"children":["blackwell-gemm-intro",{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/34db3e12e547f437.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/chunks/4057cc9dbcc744c0.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased","children":["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[["$","div",null,{"className":"flex flex-col justify-center min-h-screen","children":[["$","div",null,{"className":"flex flex-row justify-center pt-15","children":["$","$L4",null,{}]}],["$","div",null,{"className":"grow flex justify-center w-full pt-10 md:pt-15 px-3","children":["$","div",null,{"className":"flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl","children":[["$","div",null,{"className":"text-3xl font-bold flex justify-center","children":"Introduction to Blackwell GEMM"}],["$","div",null,{"className":"text-sm flex justify-center pt-5","children":null}],["$","div",null,{"className":"","children":[["$","div","0",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Specifying the Problem"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["To compute the matrix product ",["$","$L5",null,{"latex":"C=AB","className":"mt-10"}]," we first need to fix the ",["$","span",null,{"className":"font-semibold","children":"IO data type"}]," for each of the matrices. We then need to specify the ",["$","span",null,{"className":"font-semibold","children":"accumulator data type"}],", this means setting the datatype for summation (math terminology) or reduction (numerics lingo) both of which refer to the operation ",["$","$L5",null,{"latex":"c_{ij}=\\sum_k{a_{ik}b_{kj}}","displayMode":true,"className":"mt-10"}]," where ",["$","$L5",null,{"latex":"a_{ik}, b_{kj}, c_{ij}","className":"mt-10"}]," are entries, or blocked matrices in ",["$","$L5",null,{"latex":"A, B, C","className":"mt-10"}],". A common accumulator dtype is FP32 to avoid overflow in the summation process. When the accumulator dtype differs from ",["$","$L5",null,{"latex":"C","className":"mt-10"}],"'s IO type then conversion is required. To produce the block matrix data we use the tensor memory accelerator to load some number of blocks to shared memory, this number is determined by the size of the smem and is sometimes called the number of ","$L6",". The block matrix product is accumulated on tensor memory, and there are some number of ","$L7",", which is one in our example."]}],"$L8"]}]]}],"$L9","$La"]}]]}]}],"$Lb"]}],["$Lc","$Ld","$Le"],"$Lf"]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],"$L10",false]],"m":"$undefined","G":["$11",[]],"S":true}
15:I[56691,["/_next/static/chunks/e02326bdb730da03.js","/_next/static/chunks/bc166ea53d390db7.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
16:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"OutletBoundary"]
17:"$Sreact.suspense"
19:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"ViewportBoundary"]
1b:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"MetadataBoundary"]
6:["$","span",null,{"className":"font-semibold","children":"TMA pipeline stages"}]
7:["$","span",null,{"className":"font-semibold","children":"accumulation stages"}]
8:["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Having specified the data and IO, we turn to compute specifications. We provide the shape for the matrix multiplication instruction for 5th generation TensorCore and specify the block matrix size (aka tiler size in cutlass world) that either one or two CTAs will process at once (the number of CTAs is called the ",["$","span",null,{"className":"font-semibold","children":"issue granularity"}]," of the instruction). Further we decide the number of threads in each CTA: to ensure full SM usage we use at least the number of threads in a warp group (128)."]}]
9:["$","div","1",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Host Code"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"We now describe the host code, which runs on the CPU, and is used to lanuch the GEMM kernel on the GPU."}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"The following specifies the matrix multiply and accumulate instruction to use"}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"$L12"}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["To specify shared memory layout for the matrix ",["$","$L5",null,{"latex":"A","className":"mt-10"}],". This layout has at least ",["$","$L5",null,{"latex":"3","className":"mt-10"}]," modes, where the first mode is the shape of the block matrix for a single MMA instruction. The next two modes describe how many times the MMA is repeated traversing the row and columns of ",["$","$L5",null,{"latex":"A","className":"mt-10"}],". This layout is ",["$","span",null,{"className":"font-semibold","children":"swizzled"}]," to avoid smem bank conflict."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"$L13"}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Tensor memory accelerator to load ",["$","$L5",null,{"latex":"A","className":"mt-10"}]," from global to shared memory state space is configured as follows ","$L14"]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"Then the kernel is lanuched, specifying the thread block and grid dimensions as appropriate."}]]}]]}]
a:["$","div","2",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"GEMM Stages"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"Before delving into device code let us give a conceptual overview of what happens during the GEMM computation."}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The ",["$","span",null,{"className":"font-semibold","children":"prologue"}]," is what happens before the matrix multiply instruction occurs. The most important function is to load the data via TMA. This is done by performing the necessary indexing: block, thread, warp ID, which is useful for locating which block matrix the MMA will calculate, which data it should load, the TMA and MMA tensor view. The relevant shared and tensor memory needs to be allocated. Setting up pipelines PipelineTmaUmma for consumer-producer between data loading and MMA, PipelineUmmaAsync for signally accumulation completion."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The ",["$","span",null,{"className":"font-semibold","children":"mainloop"}]," iteratively fetches data, computes MMA and accumulates across the reduction dimension."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The ",["$","span",null,{"className":"font-semibold","children":"epilogue"}]," loads data from tensor memory to register, fuse operation on ",["$","$L5",null,{"latex":"C","className":"mt-10"}]," matrix and performs necessary data conversion. This stages deallocates tensor memory and stores results back to global memory."]}]]}]]}]
b:["$","$L15",null,{}]
c:["$","script","script-0",{"src":"/_next/static/chunks/e02326bdb730da03.js","async":true,"nonce":"$undefined"}]
d:["$","script","script-1",{"src":"/_next/static/chunks/bc166ea53d390db7.js","async":true,"nonce":"$undefined"}]
e:["$","script","script-2",{"src":"/_next/static/chunks/5ad9eb95768fc0a4.js","async":true,"nonce":"$undefined"}]
f:["$","$L16",null,{"children":["$","$17",null,{"name":"Next.MetadataOutlet","children":"$@18"}]}]
10:["$","$1","h",{"children":[null,["$","$L19",null,{"children":"$L1a"}],["$","div",null,{"hidden":true,"children":["$","$L1b",null,{"children":["$","$17",null,{"name":"Next.Metadata","children":"$L1c"}]}]}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]
1a:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
1d:I[87718,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"IconMark"]
18:null
1c:[["$","title","0",{"children":"Claire Zhao Blog"}],["$","meta","1",{"name":"description","content":"Claire Zhao's Blog on AI and Math"}],["$","link","2",{"rel":"icon","href":"/favicon.ico?favicon.e10ff5c7.ico","sizes":"1176x1032","type":"image/x-icon"}],["$","$L1d","3",{}]]
1e:I[66832,["/_next/static/chunks/e02326bdb730da03.js","/_next/static/chunks/bc166ea53d390db7.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
12:["$","$L1e",null,{"code":"op = tcgen05.MmaF16BF16Op(\n    io_dtype,\n    acc_dtype,\n    mma_inst_shape_mnk,\n    tcgen05.CtaGroup.ONE,   # issue granularity\n    tcgen05.OperandSource.SMEM,\n    tcgen05.OperandMajorMode.K, # matrix A reduction dimension\n    tcgen05.OperandMajorMode.K, # matrix B reduction dimension\n)\ntiled_mma = cute.make_tiled_mma(op)\n","tokens":[[{"content":"op = tcgen05.","offset":0,"color":"#EEF0F9","fontStyle":0},{"content":"MmaF16BF16Op","offset":13,"color":"#00DAEF","fontStyle":0},{"content":"(","offset":25,"color":"#EEF0F9","fontStyle":0}],[{"content":"    io_dtype,","offset":27,"color":"#EEF0F9","fontStyle":0}],[{"content":"    acc_dtype,","offset":41,"color":"#EEF0F9","fontStyle":0}],[{"content":"    mma_inst_shape_mnk,","offset":56,"color":"#EEF0F9","fontStyle":0}],[{"content":"    tcgen05.CtaGroup.","offset":80,"color":"#EEF0F9","fontStyle":0},{"content":"ONE","offset":101,"color":"#FFD493","fontStyle":0},{"content":",   ","offset":104,"color":"#EEF0F9","fontStyle":0},{"content":"# issue granularity","offset":108,"color":"#EEF0F98F","fontStyle":1}],[{"content":"    tcgen05.OperandSource.","offset":128,"color":"#EEF0F9","fontStyle":0},{"content":"SMEM","offset":154,"color":"#FFD493","fontStyle":0},{"content":",","offset":158,"color":"#EEF0F9","fontStyle":0}],[{"content":"    tcgen05.OperandMajorMode.K, ","offset":160,"color":"#EEF0F9","fontStyle":0},{"content":"# matrix A reduction dimension","offset":192,"color":"#EEF0F98F","fontStyle":1}],[{"content":"    tcgen05.OperandMajorMode.K, ","offset":223,"color":"#EEF0F9","fontStyle":0},{"content":"# matrix B reduction dimension","offset":255,"color":"#EEF0F98F","fontStyle":1}],[{"content":")","offset":286,"color":"#EEF0F9","fontStyle":0}],[{"content":"tiled_mma = cute.","offset":288,"color":"#EEF0F9","fontStyle":0},{"content":"make_tiled_mma","offset":305,"color":"#00DAEF","fontStyle":0},{"content":"(op)","offset":319,"color":"#EEF0F9","fontStyle":0}],[]]}]
13:["$","$L1e",null,{"code":"a_smem_layout = cutlass.utils.blackwell_helpers.make_smem_layout_a(\n    tiled_mma,\n    mma_tiler_mnk,\n    a.element_type,\n    ab_stages,\n)\na_smem_layout_one_stage = cute.select(a_smem_layout, mode=[0, 1, 2])\n","tokens":[[{"content":"a_smem_layout = cutlass.utils.blackwell_helpers.","offset":0,"color":"#EEF0F9","fontStyle":0},{"content":"make_smem_layout_a","offset":48,"color":"#00DAEF","fontStyle":0},{"content":"(","offset":66,"color":"#EEF0F9","fontStyle":0}],[{"content":"    tiled_mma,","offset":68,"color":"#EEF0F9","fontStyle":0}],[{"content":"    mma_tiler_mnk,","offset":83,"color":"#EEF0F9","fontStyle":0}],[{"content":"    a.element_type,","offset":102,"color":"#EEF0F9","fontStyle":0}],[{"content":"    ab_stages,","offset":122,"color":"#EEF0F9","fontStyle":0}],[{"content":")","offset":137,"color":"#EEF0F9","fontStyle":0}],[{"content":"a_smem_layout_one_stage = cute.","offset":139,"color":"#EEF0F9","fontStyle":0},{"content":"select","offset":170,"color":"#00DAEF","fontStyle":0},{"content":"(a_smem_layout, ","offset":176,"color":"#EEF0F9","fontStyle":0},{"content":"mode","offset":192,"color":"#4BF3C8","fontStyle":1},{"content":"=[","offset":196,"color":"#EEF0F9","fontStyle":0},{"content":"0","offset":198,"color":"#FFD493","fontStyle":0},{"content":", ","offset":199,"color":"#EEF0F9","fontStyle":0},{"content":"1","offset":201,"color":"#FFD493","fontStyle":0},{"content":", ","offset":202,"color":"#EEF0F9","fontStyle":0},{"content":"2","offset":204,"color":"#FFD493","fontStyle":0},{"content":"])","offset":205,"color":"#EEF0F9","fontStyle":0}],[]]}]
14:["$","$L1e",null,{"code":"op = cute.nvgpu.cpasync.CopyBulkTensorTileG2SOp(tcgen05.CtaGroup.ONE)\na_tma_atom, a_tma_tensor = cute.nvgpu.make_tiled_tma_atom_A(\n    op,\n    a,  # the matrix A\n    a_smem_layout_one_stage,\n    mma_tiler_mnk,\n    tiled_mma,\n)\n","tokens":[[{"content":"op = cute.nvgpu.cpasync.","offset":0,"color":"#EEF0F9","fontStyle":0},{"content":"CopyBulkTensorTileG2SOp","offset":24,"color":"#00DAEF","fontStyle":0},{"content":"(tcgen05.CtaGroup.","offset":47,"color":"#EEF0F9","fontStyle":0},{"content":"ONE","offset":65,"color":"#FFD493","fontStyle":0},{"content":")","offset":68,"color":"#EEF0F9","fontStyle":0}],[{"content":"a_tma_atom, a_tma_tensor = cute.nvgpu.","offset":70,"color":"#EEF0F9","fontStyle":0},{"content":"make_tiled_tma_atom_A","offset":108,"color":"#00DAEF","fontStyle":0},{"content":"(","offset":129,"color":"#EEF0F9","fontStyle":0}],[{"content":"    op,","offset":131,"color":"#EEF0F9","fontStyle":0}],[{"content":"    a,  ","offset":139,"color":"#EEF0F9","fontStyle":0},{"content":"# the matrix A","offset":147,"color":"#EEF0F98F","fontStyle":1}],[{"content":"    a_smem_layout_one_stage,","offset":162,"color":"#EEF0F9","fontStyle":0}],[{"content":"    mma_tiler_mnk,","offset":191,"color":"#EEF0F9","fontStyle":0}],[{"content":"    tiled_mma,","offset":210,"color":"#EEF0F9","fontStyle":0}],[{"content":")","offset":225,"color":"#EEF0F9","fontStyle":0}],[]]}]
