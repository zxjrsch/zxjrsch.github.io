1:"$Sreact.fragment"
2:I[1565,["/_next/static/chunks/8292805af7f359b4.js","/_next/static/chunks/9b725c66512530e6.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
3:I[96045,["/_next/static/chunks/8292805af7f359b4.js","/_next/static/chunks/9b725c66512530e6.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
17:I[24237,["/_next/static/chunks/8292805af7f359b4.js","/_next/static/chunks/9b725c66512530e6.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
18:I[56691,["/_next/static/chunks/8292805af7f359b4.js","/_next/static/chunks/9b725c66512530e6.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
19:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"OutletBoundary"]
1a:"$Sreact.suspense"
0:{"buildId":"WAHbF-IuoHa9ll8cBu51j","rsc":["$","$1","c",{"children":[["$","div",null,{"className":"flex flex-col justify-center min-h-screen","children":[["$","div",null,{"className":"flex flex-row justify-center pt-15","children":["$","$L2",null,{}]}],["$","div",null,{"className":"grow flex justify-center w-full pt-10 md:pt-15 px-3","children":["$","div",null,{"className":"flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl","children":[["$","div",null,{"className":"text-3xl font-bold flex justify-center","children":"Engram and LLM Memory"}],["$","div",null,{"className":"text-sm flex justify-center pt-5","children":"Feb 23"}],["$","div",null,{"className":"","children":[["$","div","0",{"children":["$undefined",["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"Engram is a newly proposed memory module for foundation models. The basic idea is to replace on-the-fly knowledge recomputation during forward pass by efficient knowledge lookup. In some sense engram is a new paradigm of neural network sparsity in addition to mixture of experts."}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"A central problem explored by the engram proposal, which leads to a new scaling law is this: under constant model parameter count, how should capacility (whatever this may mean, reasoning capacity, knowledge capacity etc.) be divided amongst engram memory and MoE experts?"}]]}]]}],["$","div","1",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Architectural Description"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The engram conditional memory module performs the two operations of ",["$","span",null,{"className":"font-semibold","children":"retrieval and fusion"}]," for every token position in a token sequence."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"Roughly speaking, the retrieval operator associates sequence local context with static memory entries through a deterministic hash function. Let us enter into a precise description of the steps."}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Each word token is associated with an integer called its token ID, this is just recalling standard byte pair encoding (BPE) tokenization. Now we want to further merge sematically similar (normalized textual equivalence, lowercasing) tokens to achieve compression. With ",["$","$L3",null,{"latex":"V","className":"mt-10"}]," being the BPE vocabulary and ",["$","$L3",null,{"latex":"W","className":"mt-10"}]," being the compressed vocabular, the tokenizer compression is simply a map ",["$","$L3",null,{"latex":"\\mathcal{P}: V\\rightarrow W","className":"mt-10"}],". Every token is first re-tokenized ",["$","$L3",null,{"latex":"y_i=\\mathcal{P}(x_i)","className":"mt-10"}]," ",["$","$L3",null,{"latex":"x_1, \\dots, x_L \\mapsto y_1 , \\dots, y_L","displayMode":true,"className":"mt-10"}]," For each ",["$","$L3",null,{"latex":"1\\leq i \\leq L","className":"mt-10"}]," and ",["$","$L3",null,{"latex":"1<n \\leq N","className":"mt-10"}]," consider the ",["$","$L3",null,{"latex":"n","className":"mt-10"}],"-grams ",["$","$L3",null,{"latex":"y_{i, n}=y_{i-(n-1)},\\dots,y_{i-1}, y_i\\in W^n","displayMode":true,"className":"mt-10"}]," which we ",["$","span",null,{"className":"font-semibold","children":"hash"}]," some ",["$","$L3",null,{"latex":"K","className":"mt-10"}]," times via the mappings ",["$","$L3",null,{"latex":"\\{h_{n, k}: W^n\\rightarrow \\mathbb{N}: 1\\leq k \\leq K\\}","className":"mt-10"}]," ","$L4"," The use of multiple hash maps is to prevent hash collision. In implementation, each ","$L5"," is a multiplicative XOR hash. The ","$L6"," is an index to a memory lookup table ","$L7"," from which we retrieve embedding vectors ","$L8"," Such vectors are then ","$L9"," (denoted by ","$La",") to give a ","$Lb"," ","$Lc"," associated with token ","$Ld"," ","$Le"]}],"$Lf","$L10","$L11"]}]]}]]}]]}]}],"$L12"]}],["$L13","$L14","$L15"],"$L16"]}],"loading":null,"isPartial":false}
4:["$","$L3",null,{"latex":"z_{i, n, k} = h_{n,k}(y_{i,n})","displayMode":true,"className":"mt-10"}]
5:["$","$L3",null,{"latex":"h_k","className":"mt-10"}]
6:["$","$L3",null,{"latex":"z_{i, n, k}","className":"mt-10"}]
7:["$","$L3",null,{"latex":"E_{n,k}","className":"mt-10"}]
8:["$","$L3",null,{"latex":"e_{i,n,k}=E_{n,k}(z_{i,n,k})","displayMode":true,"className":"mt-10"}]
9:["$","span",null,{"className":"italic","children":"concatenated"}]
a:["$","$L3",null,{"latex":"\\oplus","className":"mt-10"}]
b:["$","span",null,{"className":"font-semibold","children":"memory embedding vector"}]
c:["$","$L3",null,{"latex":"e_i","className":"mt-10"}]
d:["$","$L3",null,{"latex":"i","className":"mt-10"}]
e:["$","$L3",null,{"latex":"e_{i}=\\bigoplus_{n, k} e_{i,n,k}","displayMode":true,"className":"mt-10"}]
f:["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["This is the retrieval stage, the other fusion stage invovles so called ",["$","span",null,{"className":"font-semibold","children":"context aware gating"}],". The operation performed is as follows. Analogous to the QKV matrices, we project the memory embedding ",["$","$L3",null,{"latex":"e_i","className":"mt-10"}]," for each token position ",["$","$L3",null,{"latex":"i","className":"mt-10"}]," (we will now omit the index and write ",["$","$L3",null,{"latex":"e=e_i","className":"mt-10"}]," and likewise ",["$","$L3",null,{"latex":"h=h_i","className":"mt-10"}]," is the hidden embedding for token ",["$","$L3",null,{"latex":"i","className":"mt-10"}]," while agreeing that the operation is performed per token) through linear layers ",["$","$L3",null,{"latex":"A","className":"mt-10"}]," and ",["$","$L3",null,{"latex":"B","className":"mt-10"}]," and get ",["$","$L3",null,{"latex":"a = Ae,\\quad b = Be","displayMode":true,"className":"mt-10"}]," We then  compute ",["$","$L3",null,{"latex":"\\lambda = \\sigma\\left(\\frac{\\text{RMS}(h)^\\top\\, \\text{RMS}(a)}{\\sqrt{d}}\\right)","displayMode":true,"className":"mt-10"}]," where ",["$","$L3",null,{"latex":"\\textbf{RMS}","className":"mt-10"}]," is the usual root mean square normalizatio layer, ",["$","$L3",null,{"latex":"d","className":"mt-10"}]," is the dimension of the vectors ",["$","$L3",null,{"latex":"h","className":"mt-10"}]," and ",["$","$L3",null,{"latex":"a","className":"mt-10"}],", and ",["$","$L3",null,{"latex":"\\sigma","className":"mt-10"}]," is the sigmoid. The ",["$","span",null,{"className":"font-semibold","children":"gating"}]," is simply the scalar vector product ",["$","$L3",null,{"latex":"c = \\lambda b","displayMode":true,"className":"mt-10"}]," Let ",["$","$L3",null,{"latex":"C","className":"mt-10"}]," be the matrix whose rows are the token gated values ",["$","$L3",null,{"latex":"c=c_i","className":"mt-10"}],", the following operation involves 1D convolution with kernel size ",["$","$L3",null,{"latex":"\\omega=4","className":"mt-10"}]," and dilation ",["$","$L3",null,{"latex":"\\delta = N","className":"mt-10"}]," ",["$","$L3",null,{"latex":"D = C + \\varphi(C)","displayMode":true,"className":"mt-10"}]," where ",["$","$L3",null,{"latex":"\\varphi=\\text{SiLU}\\circ\\text{Conv1D}_{\\omega, \\delta}\\circ\\text{RMS}","className":"mt-10"}],". This is followed by residual connection, namesly, if we denote the matrix of hidden embeddings ",["$","$L3",null,{"latex":"H","className":"mt-10"}]," (its ",["$","$L3",null,{"latex":"i","className":"mt-10"}],"-th row is ",["$","$L3",null,{"latex":"h_i","className":"mt-10"}],") that is inputted to the engram layer, then output of the engram layer is ",["$","$L3",null,{"latex":"H+D","className":"mt-10"}],"."]}]
10:["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The engram paper uses ",["$","$L3",null,{"latex":"M=4","className":"mt-10"}]," residual streams, each called a ",["$","span",null,{"className":"font-semibold","children":"branch"}],". At each engram layer, and for each token, there are multiple hidden vectors ",["$","$L3",null,{"latex":"\\{h_m\\}_{m=1}^M","className":"mt-10"}],", each corresponding to a branch. The above operations are repeated for each hidden vector ",["$","$L3",null,{"latex":"h_m","className":"mt-10"}]," with the condition that the memory embedding tables ",["$","$L3",null,{"latex":"E_{n,k}","className":"mt-10"}]," and the matrix ",["$","$L3",null,{"latex":"B","className":"mt-10"}]," are shared across all ",["$","$L3",null,{"latex":"M","className":"mt-10"}]," branches, whereas the matrices ",["$","$L3",null,{"latex":"\\{A_m\\}_{m=1}^M","className":"mt-10"}]," used for computing ",["$","$L3",null,{"latex":"\\lambda","className":"mt-10"}]," is distinct across branches. The architecture also uses ",["$","span",null,{"className":"italic","children":"manifold constrained hyper connections"}],", which we will discuss in a separate blog."]}]
11:["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["$","$L17",null,{"src":{"src":"/_next/static/media/engram.f99d378f.png","width":1734,"height":1032,"blurWidth":8,"blurHeight":5,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAfUlEQVR42i2OWw6CMAAEuf/1iCKSqn0CRdMWaPo5VmS/NpvJZhpqUgzE8KGUwvL2jNNECJF932l+wOIn/GjIOaOtYXgIpDaklP6An2dG546Hl1Bc2o5r29cuT8BptLgdD1IqjLIMd4HRlmaNG/apUHXwbkZ0ffVJrGmrDpkvh8yVpJdLsacAAAAASUVORK5CYII="},"caption":"Engram architecture"}]}]
12:["$","$L18",null,{}]
13:["$","script","script-0",{"src":"/_next/static/chunks/8292805af7f359b4.js","async":true}]
14:["$","script","script-1",{"src":"/_next/static/chunks/9b725c66512530e6.js","async":true}]
15:["$","script","script-2",{"src":"/_next/static/chunks/5ad9eb95768fc0a4.js","async":true}]
16:["$","$L19",null,{"children":["$","$1a",null,{"name":"Next.MetadataOutlet","children":"$@1b"}]}]
1b:null
