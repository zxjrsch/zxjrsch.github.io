<!DOCTYPE html><!--Knmg63qJ7iDNp2d01mhTL--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/chunks/0203b6e18d279108.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/4057cc9dbcc744c0.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/be439e04947a2cba.js"/><script src="/_next/static/chunks/922a6be1c048f7e7.js" async=""></script><script src="/_next/static/chunks/748df87cf5306c08.js" async=""></script><script src="/_next/static/chunks/turbopack-0af7db035ef23dbc.js" async=""></script><script src="/_next/static/chunks/988ea292de4c4c73.js" async=""></script><script src="/_next/static/chunks/7a959126392b4956.js" async=""></script><script src="/_next/static/chunks/e5d291186d0cbb54.js" async=""></script><script src="/_next/static/chunks/bc166ea53d390db7.js" async=""></script><meta name="next-size-adjust" content=""/><title>Claire Zhao Blog</title><meta name="description" content="Claire Zhao&#x27;s Blog on AI and Math"/><link rel="icon" href="/favicon.ico?favicon.e10ff5c7.ico" sizes="1176x1032" type="image/x-icon"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased"><div hidden=""><!--$--><!--/$--></div><div class="flex flex-col justify-center min-h-screen"><div class="flex flex-row justify-center pt-15"><div class="flex flex-row w-full md:max-w-5/10 relative"><div class="flex flex-row md:basis-1/3 "><a href="/"><div class="text-2xl font-bold pl-5 md:pl-0 pt-5 md:pt-0">Claire Zhao</div></a></div><div class="md:hidden grow flex justify-end items-center pr-10"><button><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="size-5"><path d="M8 2H13.5C13.7761 2 14 2.22386 14 2.5V12.5C14 12.7761 13.7761 13 13.5 13H8V2ZM7 2H1.5C1.22386 2 1 2.22386 1 2.5V12.5C1 12.7761 1.22386 13 1.5 13H7V2ZM0 2.5C0 1.67157 0.671573 1 1.5 1H13.5C14.3284 1 15 1.67157 15 2.5V12.5C15 13.3284 14.3284 14 13.5 14H1.5C0.671573 14 0 13.3284 0 12.5V2.5Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg></button></div><nav class="hidden md:flex basis-2/3 flex-row justify-end gap-x-8"><a href="/ai"><div class="flex items-center gap-2 font-medium hover:underline">Machine Learning <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9 4L9 11L4.5 7.5L9 4Z" fill="currentColor"></path></svg></div></a><a href="/math"><div class="flex items-center gap-2 font-medium hover:underline">Mathematics </div></a></nav></div></div><div class="grow flex justify-center w-full pt-10 md:pt-15 px-3"><div class="flex flex-col lg:min-w-1/2 lg:max-w-1/2"><div class="text-3xl font-bold flex justify-center">GPU Kernel Level Optimization for Efficient MoE Training</div><div class="text-sm flex justify-center pt-5">Jan 20</div><div class=""><div><div class=" pt-20 font-bold text-2xl" id="section-2">Variable Length Grouped General Matrix Multiply</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Recall that the definition of the general matrix multiply (GEMM) is the operation <span class="mt-10"></span> on matrices <span class="mt-10"></span> and scalers <span class="mt-10"></span>. We shall be concerned with the special case <span class="mt-10"></span> where <span class="mt-10"></span> has dimensions <span class="mt-10"></span> and <span class="mt-10"></span> has dimensions <span class="mt-10"></span>, and we say will denote the problem size by <span class="mt-10"></span>.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">This special case is relevant in the mixture of experts layers where we have a set of such multiplications for the different tokens routed to the various experts. Within an MoE layer, these multiplications have possibly with different shapes due to the tokens routed to each expert. For example if <span class="mt-10"></span> is the token dimension, we shall refer to the problem as variable length M grouped GEMM.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">The up-projection of a given expert requires tokens routed to it to be gathered, while the down-projection has all tokens already contiguously packaged.</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Forward and Backward MoE Kernels</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Let us turn to the kernel design in the SonicMoE paper.  A total of <span class="mt-10"></span> kernels are associated with the MoE layer. The forward computation has up-projection, down-projection, and expert aggregation kernels. Recall that in general, backwards gradients are computed with respective to the activations as well as for the weights. In the case of MoE, the backwards activation gradients are computed with respect to down-projection, up-projection and the MoE input. Two gradients with respect to up and down projection weight matrices are also computed. Moreover a top-k routing kernel is given in SonicMoE.</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Kernel (Epilogue) Fusion</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">The first strategy is a kernel fusion of the input gather (during token routing) with input loading from global memory (HBM) to shared memory (SMEM). During this phase the token indices are first gathered, then the activations at those indicies are obtained via the cp.async PTX instruction. The authors of SonicMoE note that on Blackwell architecture, the 2-CTA GEMM requires the leader CTA to wait for gather to be complete in both CTAs, which leads to the following pipeline structure of the two CTAs: 1 warp to fetch token indices, 4 warps to gather, then 1 warp to relay the signal and perform MMA.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">The second fusion occurs during the epilogue associated with the MoE layer. Specifically, the SwiGLU is fused with forward up-projection, while the backward of SwiGLU is fused with the down-projection gradient computation.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Thirdly fusion involves the <span class="mt-10"></span> and <span class="mt-10"></span> associated with  the scores <span class="mt-10"></span> and down-project <span class="mt-10"></span>, respectively.</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Computation and Async IO Overlap</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">A useful strategy for achieving high tensor core throughput with intense epilogue is called ping-pong scheduling where warp groups that collectively issues WGMMA overlap IO and GEMM at a given moment and exchange roles at a future time. Such a strategy was used in Flash Attention 3, and is used in SonicMoE&#x27;s down projection activation gradient computation, and the down project forward epilogue.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">The paper also leverages async TMA for data movement between SMEM and GMEM in a subset of the <span class="mt-10"></span> kernels, for example to store forward down project, among other places.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">On Blackwell, the ping-poing strategy is used together with the TMEM and UMMA for better pipelining.</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Top-K Sorting Kernel</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">The SonicMoE paper also provides a top-K kernel using bitonic sort, and has optional softmax fusion.</div></div></div></div></div></div><div class="flex justify-center my-5 md:my-10">Claire Zhao Â© 2026</div></div><!--$--><!--/$--><script src="/_next/static/chunks/be439e04947a2cba.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[48523,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"default\"]\n3:I[82281,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"default\"]\n4:I[1565,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/e5d291186d0cbb54.js\",\"/_next/static/chunks/bc166ea53d390db7.js\"],\"default\"]\n5:I[96045,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/e5d291186d0cbb54.js\",\"/_next/static/chunks/bc166ea53d390db7.js\"],\"default\"]\n12:I[75067,[],\"default\"]\n:HL[\"/_next/static/chunks/0203b6e18d279108.css\",\"style\"]\n:HL[\"/_next/static/chunks/4057cc9dbcc744c0.css\",\"style\"]\n:HL[\"/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"Knmg63qJ7iDNp2d01mhTL\",\"c\":[\"\",\"ai\",\"moe-kernel\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"ai\",{\"children\":[\"moe-kernel\",{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/0203b6e18d279108.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/4057cc9dbcc744c0.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col justify-center min-h-screen\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-row justify-center pt-15\",\"children\":[\"$\",\"$L4\",null,{}]}],[\"$\",\"div\",null,{\"className\":\"grow flex justify-center w-full pt-10 md:pt-15 px-3\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col lg:min-w-1/2 lg:max-w-1/2\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-3xl font-bold flex justify-center\",\"children\":\"GPU Kernel Level Optimization for Efficient MoE Training\"}],[\"$\",\"div\",null,{\"className\":\"text-sm flex justify-center pt-5\",\"children\":\"Jan 20\"}],[\"$\",\"div\",null,{\"className\":\"\",\"children\":[[\"$\",\"div\",\"0\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Variable Length Grouped General Matrix Multiply\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"Recall that the definition of the general matrix multiply (GEMM) is the operation \",[\"$\",\"$L5\",null,{\"latex\":\"\\\\alpha AB + \\\\beta Z\",\"displayMode\":true,\"className\":\"mt-10\"}],\" on matrices \",[\"$\",\"$L5\",null,{\"latex\":\"A, B, Z\",\"className\":\"mt-10\"}],\" and scalers \",[\"$\",\"$L5\",null,{\"latex\":\"\\\\alpha, \\\\beta\",\"className\":\"mt-10\"}],\". We shall be concerned with the special case \",[\"$\",\"$L5\",null,{\"latex\":\"C = AB\",\"displayMode\":true,\"className\":\"mt-10\"}],\" where \",[\"$\",\"$L5\",null,{\"latex\":\"A\",\"className\":\"mt-10\"}],\" has dimensions \",[\"$\",\"$L5\",null,{\"latex\":\"M\\\\times K\",\"className\":\"mt-10\"}],\" and \",[\"$\",\"$L5\",null,{\"latex\":\"B\",\"className\":\"mt-10\"}],\" has dimensions \",[\"$\",\"$L5\",null,{\"latex\":\"K\\\\times N\",\"className\":\"mt-10\"}],\", and we say will denote the problem size by \",[\"$\",\"$L5\",null,{\"latex\":\"(M, N, K)\",\"className\":\"mt-10\"}],\".\"]}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"This special case is relevant in the mixture of experts layers where we have a set of such multiplications for the different tokens routed to the various experts. Within an MoE layer, these multiplications have possibly with different shapes due to the tokens routed to each expert. For example if \",\"$L6\",\" is the token dimension, we shall refer to the problem as variable length M grouped GEMM.\"]}],\"$L7\"]}]]}],\"$L8\",\"$L9\",\"$La\",\"$Lb\"]}]]}]}],\"$Lc\"]}],[\"$Ld\",\"$Le\",\"$Lf\"],\"$L10\"]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],\"$L11\",false]],\"m\":\"$undefined\",\"G\":[\"$12\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"13:I[56691,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/e5d291186d0cbb54.js\",\"/_next/static/chunks/bc166ea53d390db7.js\"],\"default\"]\n14:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"OutletBoundary\"]\n15:\"$Sreact.suspense\"\n17:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"ViewportBoundary\"]\n19:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"MetadataBoundary\"]\n6:[\"$\",\"$L5\",null,{\"latex\":\"M\",\"className\":\"mt-10\"}]\n7:[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":\"The up-projection of a given expert requires tokens routed to it to be gathered, while the down-projection has all tokens already contiguously packaged.\"}]\n8:[\"$\",\"div\",\"1\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Forward and Backward MoE Kernels\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"Let us turn to the kernel design in the SonicMoE paper.  A total of \",[\"$\",\"$L5\",null,{\"latex\":\"8\",\"className\":\"mt-10\"}],\" kernels are associated with the MoE layer. The forward computation has up-projection, down-projection, and expert aggregation kernels. Recall that in general, backwards gradients are computed with respective to the activations as well as for the weights. In the case of MoE, the backwards activation gradients are computed with respect to down-projection, up-projection and the MoE input. Two gradients with respect to up and down projection weight matrices are also computed. Moreover a top-k routing kernel is given in SonicMoE.\"]}]}]]}]\n"])</script><script>self.__next_f.push([1,"9:[\"$\",\"div\",\"2\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Kernel (Epilogue) Fusion\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":\"The first strategy is a kernel fusion of the input gather (during token routing) with input loading from global memory (HBM) to shared memory (SMEM). During this phase the token indices are first gathered, then the activations at those indicies are obtained via the cp.async PTX instruction. The authors of SonicMoE note that on Blackwell architecture, the 2-CTA GEMM requires the leader CTA to wait for gather to be complete in both CTAs, which leads to the following pipeline structure of the two CTAs: 1 warp to fetch token indices, 4 warps to gather, then 1 warp to relay the signal and perform MMA.\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":\"The second fusion occurs during the epilogue associated with the MoE layer. Specifically, the SwiGLU is fused with forward up-projection, while the backward of SwiGLU is fused with the down-projection gradient computation.\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"Thirdly fusion involves the \",[\"$\",\"$L5\",null,{\"latex\":\"dS\",\"className\":\"mt-10\"}],\" and \",[\"$\",\"$L5\",null,{\"latex\":\"dH\",\"className\":\"mt-10\"}],\" associated with  the scores \",[\"$\",\"$L5\",null,{\"latex\":\"S\",\"className\":\"mt-10\"}],\" and down-project \",[\"$\",\"$L5\",null,{\"latex\":\"H\",\"className\":\"mt-10\"}],\", respectively.\"]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"a:[\"$\",\"div\",\"3\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Computation and Async IO Overlap\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":\"A useful strategy for achieving high tensor core throughput with intense epilogue is called ping-pong scheduling where warp groups that collectively issues WGMMA overlap IO and GEMM at a given moment and exchange roles at a future time. Such a strategy was used in Flash Attention 3, and is used in SonicMoE's down projection activation gradient computation, and the down project forward epilogue.\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"The paper also leverages async TMA for data movement between SMEM and GMEM in a subset of the \",[\"$\",\"$L5\",null,{\"latex\":\"8\",\"className\":\"mt-10\"}],\" kernels, for example to store forward down project, among other places.\"]}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":\"On Blackwell, the ping-poing strategy is used together with the TMEM and UMMA for better pipelining.\"}]]}]]}]\nb:[\"$\",\"div\",\"4\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Top-K Sorting Kernel\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":\"The SonicMoE paper also provides a top-K kernel using bitonic sort, and has optional softmax fusion.\"}]}]]}]\nc:[\"$\",\"$L13\",null,{}]\nd:[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/988ea292de4c4c73.js\",\"async\":true,\"nonce\":\"$undefined\"}]\ne:[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/e5d291186d0cbb54.js\",\"async\":true,\"nonce\":\"$undefined\"}]\nf:[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/bc166ea53d390db7.js\",\"async\":true,\"nonce\":\"$undefined\"}]\n10:[\"$\",\"$L14\",null,{\"children\":[\"$\",\"$15\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@16\"}]}]\n11:[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L17\",null,{\"children\":\"$L18\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$L19\",null,{\"children\":[\"$\",\"$15\",null,{\"name\":\"Next.Metadata\",\"children\":\"$L1a\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}]\n"])</script><script>self.__next_f.push([1,"18:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"1b:I[87718,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"IconMark\"]\n16:null\n1a:[[\"$\",\"title\",\"0\",{\"children\":\"Claire Zhao Blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Claire Zhao's Blog on AI and Math\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/favicon.ico?favicon.e10ff5c7.ico\",\"sizes\":\"1176x1032\",\"type\":\"image/x-icon\"}],[\"$\",\"$L1b\",\"3\",{}]]\n"])</script></body></html>