<!DOCTYPE html><!--_PDM1eQFKJJmTk1GhsL6I--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/chunks/1fcb9d377ba4db76.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/4057cc9dbcc744c0.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/be439e04947a2cba.js"/><script src="/_next/static/chunks/922a6be1c048f7e7.js" async=""></script><script src="/_next/static/chunks/748df87cf5306c08.js" async=""></script><script src="/_next/static/chunks/turbopack-0af7db035ef23dbc.js" async=""></script><script src="/_next/static/chunks/988ea292de4c4c73.js" async=""></script><script src="/_next/static/chunks/7a959126392b4956.js" async=""></script><script src="/_next/static/chunks/e5d291186d0cbb54.js" async=""></script><script src="/_next/static/chunks/bc166ea53d390db7.js" async=""></script><meta name="next-size-adjust" content=""/><title>Claire Zhao Blog</title><meta name="description" content="Claire Zhao&#x27;s Blog on AI and Math"/><link rel="icon" href="/favicon.ico?favicon.e10ff5c7.ico" sizes="1176x1032" type="image/x-icon"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased"><div hidden=""><!--$--><!--/$--></div><div class="flex flex-col justify-center min-h-screen"><div class="flex flex-row justify-center pt-15"><div class="flex flex-row w-full md:max-w-5/10 relative"><div class="flex flex-row md:basis-1/3 "><a href="/"><div class="text-2xl font-bold pl-5 md:pl-0 pt-5 md:pt-0">Claire Zhao</div></a></div><div class="md:hidden grow flex justify-end items-center pr-10"><button><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="size-5"><path d="M8 2H13.5C13.7761 2 14 2.22386 14 2.5V12.5C14 12.7761 13.7761 13 13.5 13H8V2ZM7 2H1.5C1.22386 2 1 2.22386 1 2.5V12.5C1 12.7761 1.22386 13 1.5 13H7V2ZM0 2.5C0 1.67157 0.671573 1 1.5 1H13.5C14.3284 1 15 1.67157 15 2.5V12.5C15 13.3284 14.3284 14 13.5 14H1.5C0.671573 14 0 13.3284 0 12.5V2.5Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg></button></div><nav class="hidden md:flex basis-2/3 flex-row justify-end gap-x-8"><a href="/ai"><div class="flex items-center gap-2 font-medium hover:underline">Machine Learning <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9 4L9 11L4.5 7.5L9 4Z" fill="currentColor"></path></svg></div></a><a href="/math"><div class="flex items-center gap-2 font-medium hover:underline">Mathematics </div></a></nav></div></div><div class="grow flex justify-center w-full pt-10 md:pt-15 px-3"><div class="flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl"><div class="text-3xl font-bold flex justify-center">Programming Blackwell Tensor Core with Cutlass and CuTe DSL</div><div class="text-sm flex justify-center pt-5"></div><div class=""><div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">The CuTe <span class="italic">domain specific language</span> (DSL) provides a Python interface that enables efficient matrix muliplication GPU kernels implementation leveraging the tile based abstraction. The DSL offers Blackwell specific support in the <span class="font-semibold">cutlass.cute.nvgpu.tcgen05</span> module. Since our discussions about this module will largely be hardware aware, let us preface the discussion by an excusrion of Blackwell tensor core programming at the more fundamental <span class="italic">parallel thread excution</span> (PTX) virtual instruction set level.</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Tensor Memory</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">A cooperative thread array (CTA) is a set of threads that excute a kernel concurrently or in parallel. A <span class="font-semibold">warp</span> of a CTA is a maximum subset of the set of threads in the CTA that can execute the same instruction simultaneously. In common hardware, a warp is a <span class="mt-10"></span> thread subset of the CTA. Threads in a CTA can be organized into a block of 1D, 2D, or 3D block of dimensions <span class="mt-10"></span> and each thread indexed by a tuple <span class="mt-10"></span>. If <span class="mt-10"></span> then the CTA is a 1D block and so forth. One can enumerate the threads in an <span class="mt-10"></span> CTA by <span class="mt-10"></span>We can enumerate all threads in a CTA and every four consecutive warps where the begining thread has thread ID <span class="mt-10"></span> is called a is called a <span class="font-semibold">warp group</span>. The warps in a warp group are associated with a <span class="font-semibold">warp rank</span> from <span class="mt-10"></span>.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">For each cooperative thread array (CTA), the structure of tensor memory is a <span class="mt-10"></span> matrix of <span class="mt-10"></span> bit cells. Each of the <span class="mt-10"></span> rows in tensor memory is also referred to as a <span class="font-semibold">lane</span>, which is <span class="mt-10"></span> KB in size. Thus the lanes are in bijective correspondence with the threads in a warp group. In fact the lanes are divided between the warps, so that warp rank <span class="mt-10"></span> can only access lanes <span class="mt-10"></span> using <span class="font-semibold">ld</span> load, <span class="font-semibold">st</span> store,     <span class="font-semibold">cp</span> copy tcgen05 instructions which are issued on inputs of lane <span class="mt-10"></span> size. Each warp can access all columns within is accessible lane. Tensor memory allocation and deallocation use the <span class="font-semibold">alloc</span> and <span class="font-semibold">dealloc</span> instructions and are allocated in units of <span class="mt-10"></span> columns, and the number of allocated columns must be a power of <span class="mt-10"></span>. Thus when a column is allocated, each of its <span class="mt-10"></span> lanes are allocated.</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Matrix Multiplication Instructions</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify"><span class="font-semibold">Matrix multiply and accumulate</span> (MMA) are excuted by <span class="font-semibold">mma</span> instruction which depend on a number launch configurations, such as the data type of input pair of matrices and their shapes, the data type of accumlation or output matrix. Support for warp-specialization, whether one or two CTAs cooperate, and whether the matmul is dense or sparse. For instance 2CTA FP16 dense MMA can have largest shape <span class="mt-10"></span> while NVFP4 dense MMA has largest shape <span class="mt-10"></span></div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">MMA configuration are stored on register as a 32 bit <span class="font-semibold">instruction descriptor</span>. For example bits 13 to 16 describe whether the input matrices needs to be <span class="font-semibold">transposed</span> or <span class="font-semibold">negated</span>. Other data store includes sparsity, input output data type.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">MAA supports <span class="font-semibold">block scaled</span> matrix multiplication for data types such as <span class="font-semibold">mxf4</span> and <span class="font-semibold">nvf4</span>.</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Granularity and Synchronization</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">TensorCore Gen 5 Instructions such as <span class="font-semibold">mma</span>, <span class="font-semibold">cp</span> can be issued by a single thread within a CTA or CTA-Pair. Instructions such as <span class="font-semibold">alloc</span> and <span class="font-semibold">dealloc</span> can be issued from a warp of a pair of warps one in each CTA-pair. Each warp can access a quarter of the tensor memory via <span class="font-semibold">ld</span> and <span class="font-semibold">st</span> so a warp group is required for complete tensor memory access. These characterize the instruction issuing granularity.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Instructions can be synchronous (alloc/dealloc, fence) or asynchronous (cp, mma, ld, st). Special pairs of asyncrhonous instructions, called <span class="font-semibold">pipelined instructions</span> are guarenteed to excute in the order they are issued, an example is <span class="font-semibold">tcgen05.copy.cta_group::N</span> and <span class="font-semibold">tcgen05.mma.cta_group::N</span> for the same N.</div></div></div></div></div></div><div class="flex justify-center my-5 md:my-10">Claire Zhao Â© 2026</div></div><!--$--><!--/$--><script src="/_next/static/chunks/be439e04947a2cba.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[48523,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"default\"]\n3:I[82281,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"default\"]\n4:I[1565,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/e5d291186d0cbb54.js\",\"/_next/static/chunks/bc166ea53d390db7.js\"],\"default\"]\n19:I[75067,[],\"default\"]\n:HL[\"/_next/static/chunks/1fcb9d377ba4db76.css\",\"style\"]\n:HL[\"/_next/static/chunks/4057cc9dbcc744c0.css\",\"style\"]\n:HL[\"/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"-PDM1eQFKJJmTk1GhsL6I\",\"c\":[\"\",\"ai\",\"cute-dsl\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"ai\",{\"children\":[\"cute-dsl\",{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/1fcb9d377ba4db76.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/4057cc9dbcc744c0.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col justify-center min-h-screen\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-row justify-center pt-15\",\"children\":[\"$\",\"$L4\",null,{}]}],[\"$\",\"div\",null,{\"className\":\"grow flex justify-center w-full pt-10 md:pt-15 px-3\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-3xl font-bold flex justify-center\",\"children\":\"Programming Blackwell Tensor Core with Cutlass and CuTe DSL\"}],[\"$\",\"div\",null,{\"className\":\"text-sm flex justify-center pt-5\",\"children\":null}],[\"$\",\"div\",null,{\"className\":\"\",\"children\":[[\"$\",\"div\",\"0\",{\"children\":[\"$undefined\",[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"The CuTe \",[\"$\",\"span\",null,{\"className\":\"italic\",\"children\":\"domain specific language\"}],\" (DSL) provides a Python interface that enables efficient matrix muliplication GPU kernels implementation leveraging the tile based abstraction. The DSL offers Blackwell specific support in the \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"cutlass.cute.nvgpu.tcgen05\"}],\" module. Since our discussions about this module will largely be hardware aware, let us preface the discussion by an excusrion of Blackwell tensor core programming at the more fundamental \",[\"$\",\"span\",null,{\"className\":\"italic\",\"children\":\"parallel thread excution\"}],\" (PTX) virtual instruction set level.\"]}]}]]}],[\"$\",\"div\",\"1\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Tensor Memory\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"A cooperative thread array (CTA) is a set of threads that excute a kernel concurrently or in parallel. A \",\"$L5\",\" of a CTA is a maximum subset of the set of threads in the CTA that can execute the same instruction simultaneously. In common hardware, a warp is a \",\"$L6\",\" thread subset of the CTA. Threads in a CTA can be organized into a block of 1D, 2D, or 3D block of dimensions \",\"$L7\",\" and each thread indexed by a tuple \",\"$L8\",\". If \",\"$L9\",\" then the CTA is a 1D block and so forth. One can enumerate the threads in an \",\"$La\",\" CTA by \",\"$Lb\",\"We can enumerate all threads in a CTA and every four consecutive warps where the begining thread has thread ID \",\"$Lc\",\" is called a is called a \",\"$Ld\",\". The warps in a warp group are associated with a \",\"$Le\",\" from \",\"$Lf\",\".\"]}],\"$L10\"]}]]}],\"$L11\",\"$L12\"]}]]}]}],\"$L13\"]}],[\"$L14\",\"$L15\",\"$L16\"],\"$L17\"]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],\"$L18\",false]],\"m\":\"$undefined\",\"G\":[\"$19\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"1a:I[96045,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/e5d291186d0cbb54.js\",\"/_next/static/chunks/bc166ea53d390db7.js\"],\"default\"]\n1b:I[56691,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/e5d291186d0cbb54.js\",\"/_next/static/chunks/bc166ea53d390db7.js\"],\"default\"]\n1c:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"OutletBoundary\"]\n1d:\"$Sreact.suspense\"\n1f:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"ViewportBoundary\"]\n21:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"MetadataBoundary\"]\n5:[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"warp\"}]\n6:[\"$\",\"$L1a\",null,{\"latex\":\"32\",\"className\":\"mt-10\"}]\n7:[\"$\",\"$L1a\",null,{\"latex\":\"X\\\\times Y\\\\times Z\",\"className\":\"mt-10\"}]\n8:[\"$\",\"$L1a\",null,{\"latex\":\"(i, j, k)\",\"className\":\"mt-10\"}]\n9:[\"$\",\"$L1a\",null,{\"latex\":\"Y=Z=1\",\"className\":\"mt-10\"}]\na:[\"$\",\"$L1a\",null,{\"latex\":\"X\\\\times Y\\\\times Z\",\"className\":\"mt-10\"}]\nb:[\"$\",\"$L1a\",null,{\"latex\":\"(i,j,k)\\\\mapsto i + jX + kXY\",\"displayMode\":true,\"className\":\"mt-10\"}]\nc:[\"$\",\"$L1a\",null,{\"latex\":\"I\\\\equiv 0 \\\\mod 128\",\"className\":\"mt-10\"}]\nd:[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"warp group\"}]\ne:[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"warp rank\"}]\nf:[\"$\",\"$L1a\",null,{\"latex\":\"0\\\\sim 3\",\"className\":\"mt-10\"}]\n"])</script><script>self.__next_f.push([1,"10:[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"For each cooperative thread array (CTA), the structure of tensor memory is a \",[\"$\",\"$L1a\",null,{\"latex\":\"128 \\\\times 512\",\"className\":\"mt-10\"}],\" matrix of \",[\"$\",\"$L1a\",null,{\"latex\":\"32\",\"className\":\"mt-10\"}],\" bit cells. Each of the \",[\"$\",\"$L1a\",null,{\"latex\":\"128\",\"className\":\"mt-10\"}],\" rows in tensor memory is also referred to as a \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"lane\"}],\", which is \",[\"$\",\"$L1a\",null,{\"latex\":\"2\",\"className\":\"mt-10\"}],\" KB in size. Thus the lanes are in bijective correspondence with the threads in a warp group. In fact the lanes are divided between the warps, so that warp rank \",[\"$\",\"$L1a\",null,{\"latex\":\"0\",\"className\":\"mt-10\"}],\" can only access lanes \",[\"$\",\"$L1a\",null,{\"latex\":\"0-31\",\"className\":\"mt-10\"}],\" using \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"ld\"}],\" load, \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"st\"}],\" store,     \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"cp\"}],\" copy tcgen05 instructions which are issued on inputs of lane \",[\"$\",\"$L1a\",null,{\"latex\":\"\\\\times\",\"className\":\"mt-10\"}],\" size. Each warp can access all columns within is accessible lane. Tensor memory allocation and deallocation use the \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"alloc\"}],\" and \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"dealloc\"}],\" instructions and are allocated in units of \",[\"$\",\"$L1a\",null,{\"latex\":\"32\",\"className\":\"mt-10\"}],\" columns, and the number of allocated columns must be a power of \",[\"$\",\"$L1a\",null,{\"latex\":\"2\",\"className\":\"mt-10\"}],\". Thus when a column is allocated, each of its \",[\"$\",\"$L1a\",null,{\"latex\":\"128\",\"className\":\"mt-10\"}],\" lanes are allocated.\"]}]\n"])</script><script>self.__next_f.push([1,"11:[\"$\",\"div\",\"2\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Matrix Multiplication Instructions\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Matrix multiply and accumulate\"}],\" (MMA) are excuted by \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"mma\"}],\" instruction which depend on a number launch configurations, such as the data type of input pair of matrices and their shapes, the data type of accumlation or output matrix. Support for warp-specialization, whether one or two CTAs cooperate, and whether the matmul is dense or sparse. For instance 2CTA FP16 dense MMA can have largest shape \",[\"$\",\"$L1a\",null,{\"latex\":\"M\\\\times N\\\\times K=256 \\\\times 256 \\\\times 16\",\"className\":\"mt-10\"}],\" while NVFP4 dense MMA has largest shape \",[\"$\",\"$L1a\",null,{\"latex\":\"M\\\\times N\\\\times K=256\\\\times 256 \\\\times 64 \",\"className\":\"mt-10\"}]]}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"MMA configuration are stored on register as a 32 bit \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"instruction descriptor\"}],\". For example bits 13 to 16 describe whether the input matrices needs to be \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"transposed\"}],\" or \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"negated\"}],\". Other data store includes sparsity, input output data type.\"]}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"MAA supports \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"block scaled\"}],\" matrix multiplication for data types such as \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"mxf4\"}],\" and \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"nvf4\"}],\".\"]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"12:[\"$\",\"div\",\"3\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Granularity and Synchronization\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"TensorCore Gen 5 Instructions such as \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"mma\"}],\", \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"cp\"}],\" can be issued by a single thread within a CTA or CTA-Pair. Instructions such as \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"alloc\"}],\" and \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"dealloc\"}],\" can be issued from a warp of a pair of warps one in each CTA-pair. Each warp can access a quarter of the tensor memory via \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"ld\"}],\" and \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"st\"}],\" so a warp group is required for complete tensor memory access. These characterize the instruction issuing granularity.\"]}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"Instructions can be synchronous (alloc/dealloc, fence) or asynchronous (cp, mma, ld, st). Special pairs of asyncrhonous instructions, called \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"pipelined instructions\"}],\" are guarenteed to excute in the order they are issued, an example is \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"tcgen05.copy.cta_group::N\"}],\" and \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"tcgen05.mma.cta_group::N\"}],\" for the same N.\"]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"13:[\"$\",\"$L1b\",null,{}]\n14:[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/988ea292de4c4c73.js\",\"async\":true,\"nonce\":\"$undefined\"}]\n15:[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/e5d291186d0cbb54.js\",\"async\":true,\"nonce\":\"$undefined\"}]\n16:[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/bc166ea53d390db7.js\",\"async\":true,\"nonce\":\"$undefined\"}]\n17:[\"$\",\"$L1c\",null,{\"children\":[\"$\",\"$1d\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@1e\"}]}]\n18:[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L1f\",null,{\"children\":\"$L20\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$L21\",null,{\"children\":[\"$\",\"$1d\",null,{\"name\":\"Next.Metadata\",\"children\":\"$L22\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}]\n"])</script><script>self.__next_f.push([1,"20:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"23:I[87718,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"IconMark\"]\n1e:null\n22:[[\"$\",\"title\",\"0\",{\"children\":\"Claire Zhao Blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Claire Zhao's Blog on AI and Math\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/favicon.ico?favicon.e10ff5c7.ico\",\"sizes\":\"1176x1032\",\"type\":\"image/x-icon\"}],[\"$\",\"$L23\",\"3\",{}]]\n"])</script></body></html>