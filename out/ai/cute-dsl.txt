1:"$Sreact.fragment"
2:I[48523,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"default"]
3:I[82281,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"default"]
4:I[1565,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/e5d291186d0cbb54.js","/_next/static/chunks/bc166ea53d390db7.js"],"default"]
19:I[75067,[],"default"]
:HL["/_next/static/chunks/1fcb9d377ba4db76.css","style"]
:HL["/_next/static/chunks/4057cc9dbcc744c0.css","style"]
:HL["/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
0:{"P":null,"b":"-PDM1eQFKJJmTk1GhsL6I","c":["","ai","cute-dsl"],"q":"","i":false,"f":[[["",{"children":["ai",{"children":["cute-dsl",{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/1fcb9d377ba4db76.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/chunks/4057cc9dbcc744c0.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased","children":["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[["$","div",null,{"className":"flex flex-col justify-center min-h-screen","children":[["$","div",null,{"className":"flex flex-row justify-center pt-15","children":["$","$L4",null,{}]}],["$","div",null,{"className":"grow flex justify-center w-full pt-10 md:pt-15 px-3","children":["$","div",null,{"className":"flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl","children":[["$","div",null,{"className":"text-3xl font-bold flex justify-center","children":"Programming Blackwell Tensor Core with Cutlass and CuTe DSL"}],["$","div",null,{"className":"text-sm flex justify-center pt-5","children":null}],["$","div",null,{"className":"","children":[["$","div","0",{"children":["$undefined",["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The CuTe ",["$","span",null,{"className":"italic","children":"domain specific language"}]," (DSL) provides a Python interface that enables efficient matrix muliplication GPU kernels implementation leveraging the tile based abstraction. The DSL offers Blackwell specific support in the ",["$","span",null,{"className":"font-semibold","children":"cutlass.cute.nvgpu.tcgen05"}]," module. Since our discussions about this module will largely be hardware aware, let us preface the discussion by an excusrion of Blackwell tensor core programming at the more fundamental ",["$","span",null,{"className":"italic","children":"parallel thread excution"}]," (PTX) virtual instruction set level."]}]}]]}],["$","div","1",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Tensor Memory"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["A cooperative thread array (CTA) is a set of threads that excute a kernel concurrently or in parallel. A ","$L5"," of a CTA is a maximum subset of the set of threads in the CTA that can execute the same instruction simultaneously. In common hardware, a warp is a ","$L6"," thread subset of the CTA. Threads in a CTA can be organized into a block of 1D, 2D, or 3D block of dimensions ","$L7"," and each thread indexed by a tuple ","$L8",". If ","$L9"," then the CTA is a 1D block and so forth. One can enumerate the threads in an ","$La"," CTA by ","$Lb","We can enumerate all threads in a CTA and every four consecutive warps where the begining thread has thread ID ","$Lc"," is called a is called a ","$Ld",". The warps in a warp group are associated with a ","$Le"," from ","$Lf","."]}],"$L10"]}]]}],"$L11","$L12"]}]]}]}],"$L13"]}],["$L14","$L15","$L16"],"$L17"]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],"$L18",false]],"m":"$undefined","G":["$19",[]],"S":true}
1a:I[96045,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/e5d291186d0cbb54.js","/_next/static/chunks/bc166ea53d390db7.js"],"default"]
1b:I[56691,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/e5d291186d0cbb54.js","/_next/static/chunks/bc166ea53d390db7.js"],"default"]
1c:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"OutletBoundary"]
1d:"$Sreact.suspense"
1f:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"ViewportBoundary"]
21:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"MetadataBoundary"]
5:["$","span",null,{"className":"font-semibold","children":"warp"}]
6:["$","$L1a",null,{"latex":"32","className":"mt-10"}]
7:["$","$L1a",null,{"latex":"X\\times Y\\times Z","className":"mt-10"}]
8:["$","$L1a",null,{"latex":"(i, j, k)","className":"mt-10"}]
9:["$","$L1a",null,{"latex":"Y=Z=1","className":"mt-10"}]
a:["$","$L1a",null,{"latex":"X\\times Y\\times Z","className":"mt-10"}]
b:["$","$L1a",null,{"latex":"(i,j,k)\\mapsto i + jX + kXY","displayMode":true,"className":"mt-10"}]
c:["$","$L1a",null,{"latex":"I\\equiv 0 \\mod 128","className":"mt-10"}]
d:["$","span",null,{"className":"font-semibold","children":"warp group"}]
e:["$","span",null,{"className":"font-semibold","children":"warp rank"}]
f:["$","$L1a",null,{"latex":"0\\sim 3","className":"mt-10"}]
10:["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["For each cooperative thread array (CTA), the structure of tensor memory is a ",["$","$L1a",null,{"latex":"128 \\times 512","className":"mt-10"}]," matrix of ",["$","$L1a",null,{"latex":"32","className":"mt-10"}]," bit cells. Each of the ",["$","$L1a",null,{"latex":"128","className":"mt-10"}]," rows in tensor memory is also referred to as a ",["$","span",null,{"className":"font-semibold","children":"lane"}],", which is ",["$","$L1a",null,{"latex":"2","className":"mt-10"}]," KB in size. Thus the lanes are in bijective correspondence with the threads in a warp group. In fact the lanes are divided between the warps, so that warp rank ",["$","$L1a",null,{"latex":"0","className":"mt-10"}]," can only access lanes ",["$","$L1a",null,{"latex":"0-31","className":"mt-10"}]," using ",["$","span",null,{"className":"font-semibold","children":"ld"}]," load, ",["$","span",null,{"className":"font-semibold","children":"st"}]," store,     ",["$","span",null,{"className":"font-semibold","children":"cp"}]," copy tcgen05 instructions which are issued on inputs of lane ",["$","$L1a",null,{"latex":"\\times","className":"mt-10"}]," size. Each warp can access all columns within is accessible lane. Tensor memory allocation and deallocation use the ",["$","span",null,{"className":"font-semibold","children":"alloc"}]," and ",["$","span",null,{"className":"font-semibold","children":"dealloc"}]," instructions and are allocated in units of ",["$","$L1a",null,{"latex":"32","className":"mt-10"}]," columns, and the number of allocated columns must be a power of ",["$","$L1a",null,{"latex":"2","className":"mt-10"}],". Thus when a column is allocated, each of its ",["$","$L1a",null,{"latex":"128","className":"mt-10"}]," lanes are allocated."]}]
11:["$","div","2",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Matrix Multiplication Instructions"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":[["$","span",null,{"className":"font-semibold","children":"Matrix multiply and accumulate"}]," (MMA) are excuted by ",["$","span",null,{"className":"font-semibold","children":"mma"}]," instruction which depend on a number launch configurations, such as the data type of input pair of matrices and their shapes, the data type of accumlation or output matrix. Support for warp-specialization, whether one or two CTAs cooperate, and whether the matmul is dense or sparse. For instance 2CTA FP16 dense MMA can have largest shape ",["$","$L1a",null,{"latex":"M\\times N\\times K=256 \\times 256 \\times 16","className":"mt-10"}]," while NVFP4 dense MMA has largest shape ",["$","$L1a",null,{"latex":"M\\times N\\times K=256\\times 256 \\times 64 ","className":"mt-10"}]]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["MMA configuration are stored on register as a 32 bit ",["$","span",null,{"className":"font-semibold","children":"instruction descriptor"}],". For example bits 13 to 16 describe whether the input matrices needs to be ",["$","span",null,{"className":"font-semibold","children":"transposed"}]," or ",["$","span",null,{"className":"font-semibold","children":"negated"}],". Other data store includes sparsity, input output data type."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["MAA supports ",["$","span",null,{"className":"font-semibold","children":"block scaled"}]," matrix multiplication for data types such as ",["$","span",null,{"className":"font-semibold","children":"mxf4"}]," and ",["$","span",null,{"className":"font-semibold","children":"nvf4"}],"."]}]]}]]}]
12:["$","div","3",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Granularity and Synchronization"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["TensorCore Gen 5 Instructions such as ",["$","span",null,{"className":"font-semibold","children":"mma"}],", ",["$","span",null,{"className":"font-semibold","children":"cp"}]," can be issued by a single thread within a CTA or CTA-Pair. Instructions such as ",["$","span",null,{"className":"font-semibold","children":"alloc"}]," and ",["$","span",null,{"className":"font-semibold","children":"dealloc"}]," can be issued from a warp of a pair of warps one in each CTA-pair. Each warp can access a quarter of the tensor memory via ",["$","span",null,{"className":"font-semibold","children":"ld"}]," and ",["$","span",null,{"className":"font-semibold","children":"st"}]," so a warp group is required for complete tensor memory access. These characterize the instruction issuing granularity."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Instructions can be synchronous (alloc/dealloc, fence) or asynchronous (cp, mma, ld, st). Special pairs of asyncrhonous instructions, called ",["$","span",null,{"className":"font-semibold","children":"pipelined instructions"}]," are guarenteed to excute in the order they are issued, an example is ",["$","span",null,{"className":"font-semibold","children":"tcgen05.copy.cta_group::N"}]," and ",["$","span",null,{"className":"font-semibold","children":"tcgen05.mma.cta_group::N"}]," for the same N."]}]]}]]}]
13:["$","$L1b",null,{}]
14:["$","script","script-0",{"src":"/_next/static/chunks/988ea292de4c4c73.js","async":true,"nonce":"$undefined"}]
15:["$","script","script-1",{"src":"/_next/static/chunks/e5d291186d0cbb54.js","async":true,"nonce":"$undefined"}]
16:["$","script","script-2",{"src":"/_next/static/chunks/bc166ea53d390db7.js","async":true,"nonce":"$undefined"}]
17:["$","$L1c",null,{"children":["$","$1d",null,{"name":"Next.MetadataOutlet","children":"$@1e"}]}]
18:["$","$1","h",{"children":[null,["$","$L1f",null,{"children":"$L20"}],["$","div",null,{"hidden":true,"children":["$","$L21",null,{"children":["$","$1d",null,{"name":"Next.Metadata","children":"$L22"}]}]}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]
20:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
23:I[87718,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"IconMark"]
1e:null
22:[["$","title","0",{"children":"Claire Zhao Blog"}],["$","meta","1",{"name":"description","content":"Claire Zhao's Blog on AI and Math"}],["$","link","2",{"rel":"icon","href":"/favicon.ico?favicon.e10ff5c7.ico","sizes":"1176x1032","type":"image/x-icon"}],["$","$L23","3",{}]]
