1:"$Sreact.fragment"
2:I[1565,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/e5d291186d0cbb54.js","/_next/static/chunks/bc166ea53d390db7.js"],"default"]
4:I[96045,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/e5d291186d0cbb54.js","/_next/static/chunks/bc166ea53d390db7.js"],"default"]
2c:I[56691,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/e5d291186d0cbb54.js","/_next/static/chunks/bc166ea53d390db7.js"],"default"]
2d:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"OutletBoundary"]
2e:"$Sreact.suspense"
3:T524,Neural network weights is the fist pieces of data that needs to be stored for the duration of training, it is used during forward pass to compute activation, and it is used during backwards to compute activation gradients. The second piece of data which require storage are the activation of each layers, since these are used to compute the weight gradient of weights in the layer to which the activations are the inputs during forward pass, as we have already seen. Activation can be released whenever the weight gradients they are associated with are computed. The third piece of data that is stored are activation gradients. They are used to compute gradients during backpropagation through the next layer (or operation), and can be released after computing activation and weight gradients in the next layer (or operation). The fourth piece of data are weight gradients, with a lifetime from produced until the corresponding weights are updated. The fifth type of data are associated with the optimization algorithm which updates the weight using the gradients. In default optimizers that implements the Adam algorithm, we need to keep track of the exponential moving average of the weight and the square of each weight. These moving averages are ususally called optimizer states, and persist throughout training.0:{"buildId":"C9ZA5QLb86CfEdUgr5wD1","rsc":["$","$1","c",{"children":[["$","div",null,{"className":"flex flex-col justify-center min-h-screen","children":[["$","div",null,{"className":"flex flex-row justify-center pt-15","children":["$","$L2",null,{}]}],["$","div",null,{"className":"grow flex justify-center w-full pt-10 md:pt-15 px-3","children":["$","div",null,{"className":"flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl","children":[["$","div",null,{"className":"text-3xl font-bold flex justify-center","children":"6D Parallelism for Distributed Training"}],["$","div",null,{"className":"text-sm flex justify-center pt-5","children":null}],["$","div",null,{"className":"","children":[["$","div","0",{"children":["$undefined",["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"Matrix multiplication is a fundamental operator in neural networks, and in the above we have fully elucidated the differentiation of the loss through such an operator in full generality, and we have applied this to the attention operator, a compound operator consisting of a series of matrix mulitiplications. Let us assemble a clear view of the process of training a neural network. In particular let us examine the data being stored and their persistence."}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"$3"}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"The effective management of the persistence and distribution (across GPUS and CPUs) is a core systems optimization problem."}]]}]]}],["$","div","1",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Distributed Data Parallel"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["We replicate a set of model weights identically across ",["$","$L4",null,{"latex":"N","className":"mt-10"}]," GPUs. Each GPU holds a distinct collection of training data. Each GPU passes forward with the data to compute the loss, and performs backward pass to compute the gradient. The gradient is ","$L5"," over ","$L6"," parallel agents, and scaled by ","$L7"," to obtain the average gradient. Each local parallel agent now holds identical copies of the global gradient. This gradient produces identical optimizer states across ","$L8"," parallel agents. The optimizer have identical gradients, identical optimizer states, produces identical updated model weights across ","$L9"," GPUs. The procedure is repeated."]}]}]]}],"$La"]}]]}]}],"$Lb"]}],["$Lc","$Ld","$Le"],"$Lf"]}],"loading":null,"isPartial":false}
5:["$","span",null,{"className":"font-semibold","children":"all reduced"}]
6:["$","$L4",null,{"latex":"N","className":"mt-10"}]
7:["$","$L4",null,{"latex":"N^{-1}","className":"mt-10"}]
8:["$","$L4",null,{"latex":"N","className":"mt-10"}]
9:["$","$L4",null,{"latex":"N","className":"mt-10"}]
a:["$","div","2",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Fully Sharded Data Parallel"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Suppose a neural network consists of ",["$","$L4",null,{"latex":"K","className":"mt-10"}]," layers ",["$","$L4",null,{"latex":"\\{L_i\\}_{i=1}^K","className":"mt-10"}]," whereby each layer is a function with suitable domain and range. The neural network evaluated on ",["$","$L4",null,{"latex":"x","className":"mt-10"}]," leading to loss ",["$","$L4",null,{"latex":"\\ell(x)","className":"mt-10"}],", is defined by ",["$","$L4",null,{"latex":"\\ell (x) = \\left(L_K\\circ \\dots \\circ L_1 \\right)(x)","displayMode":true,"className":"mt-10"}]," Let us introduce the notation that ",["$","$L4",null,{"latex":"L_i","className":"mt-10"}]," is a function parameterized by weight ",["$","$L4",null,{"latex":"W_i","className":"mt-10"}]," and takes as input the previous layer's activation ",["$","$L4",null,{"latex":"y_{i-1} = \\left(L_{i-1}\\circ \\dots \\circ L_1 \\right)(x)","className":"mt-10"}],"."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Given dataset ",["$","$L4",null,{"latex":"\\mathcal{D}","className":"mt-10"}],", weight matrices ",["$","$L4",null,{"latex":"\\{W_i\\}_{i=1}^K","className":"mt-10"}]," and ",["$","$L4",null,{"latex":"N","className":"mt-10"}]," parallel agents (GPU, TPU etc) we partition the dataset into ",["$","$L4",null,{"latex":"N","className":"mt-10"}]," pariwise disjoint subsets ",["$","$L4",null,{"latex":"\\mathcal{D} = \\mathcal{D}^{(1)}\\,\\cup \\dots \\cup \\mathcal{D}^{(N)}","className":"mt-10"}]," and partition each weight matrix into N slices, for example ",["$","$L4",null,{"latex":"W_i = [W_i^{(1)}| \\dots | W_i^{(N)}]","displayMode":true,"className":"mt-10"}]," To the ",["$","$L4",null,{"latex":"j","className":"mt-10"}],"-th parallel agent we send ",["$","$L4",null,{"latex":"\\left(\\mathcal{D}^{(j)}, \\, W_1^{(j)}, \\dots, W_K^{(j)}\\right)","className":"mt-10"}]]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["To describe one step in training, each of the parallel agents ",["$","$L4",null,{"latex":"1\\leq j \\leq N","className":"mt-10"}]," selects a training data ",["$","$L4",null,{"latex":"x^{(j)}\\in \\mathcal{D}^{(j)}","className":"mt-10"}]," and puts ",["$","$L4",null,{"latex":"y_0^{(j)}=x^{(j)}","className":"mt-10"}],". For ",["$","$L4",null,{"latex":"1\\leq i \\leq K","className":"mt-10"}]," suppose the activation ",["$","$L4",null,{"latex":"y_{i-1}^{(j)}","className":"mt-10"}]," has been computed and the forward pass is ready to advance through layer ",["$","$L4",null,{"latex":"L_i","className":"mt-10"}]," on parallel agent ",["$","$L4",null,{"latex":"j","className":"mt-10"}]," we call ",["$","$L4",null,{"latex":"W_i = \\textbf{AllGather}\\left\\{W_i^{(j)}: 1\\leq j \\leq N\\right\\}","displayMode":true,"className":"mt-10"}]," and compute ",["$","$L4",null,{"latex":"y^{(j)}_i=L_i(y^{(j)}_{i-1})","className":"mt-10"}]," and immediately free the memory on device ",["$","$L4",null,{"latex":"j","className":"mt-10"}]," of the weight shards ",["$","$L4",null,{"latex":"\\textbf{delete}\\left\\{W_i^{(k)}\\in W_i: j\\neq k\\right\\}\\quad (\\text{device}\\,j)","displayMode":true,"className":"mt-10"}]," and store the layer-",["$","$L4",null,{"latex":"i","className":"mt-10"}]," activation for weight gradient calculation through ",["$","$L4",null,{"latex":"L_i","className":"mt-10"}]," during backward pass ",["$","$L4",null,{"latex":"\\textbf{store}\\,\\, y_i^{(j)}\\quad(\\text{device } j)","displayMode":true,"className":"mt-10"}]," Let us turn to backwards pass. In our notation ",["$","$L4",null,{"latex":"L_K","className":"mt-10"}]," computes the loss function and this layer has no weights, so as backwards we get ",["$","$L4",null,{"latex":"\\frac{\\partial \\ell}{\\partial y_{K-1}}","displayMode":true,"className":"mt-10"}],"just as reminder ",["$","$L4",null,{"latex":"y_{K-1}","className":"mt-10"}]," is the output of layer ",["$","$L4",null,{"latex":"K-1","className":"mt-10"}],", which is the input to layer ","$L10",". We will treat ","$L11"," as a variable, whereas the actual activation has a superscript ","$L12"," to identify the device ","$L13",". For each ","$L14"," supposed that device ","$L15"," computed its local activation gradient ","$L16"," have computed. Then the local weight gradient can be computed as a function ","$L17"," of the map ","$L18",", the activation gradient, and the local activation ","$L19"," Since weights are kept in shards across the parallel agents, we need to partition the weight gradient too ","$L1a"," we will use ","$L1b"," to provide each parallel agent with the average gradient corresponding to the weight shard it owns, namely it receives ","$L1c"," scaling by ","$L1d"," this is the average gradient for its local weight shard ","$L1e",". The local Adam optimizer stores exponential moving averages of ","$L1f"," and the component-wise square of ","$L20",". Thus optimizer states are sharded as well. Finally each parallel agent ","$L21"," needs to compute its local activation gradient as a function ","$L22"," of the function ","$L23"," (that we will differentiate via the chain rule) and the weight ","$L24"," of layer ","$L25"," together with ","$L26"," namely ","$L27"," Computing this allows  backpropagation to advance from layer ","$L28"," to layer ","$L29",". To do this we need to AllGather the weight ","$L2a"," on each of the parallel agents  ","$L2b"]}]]}]]}]
b:["$","$L2c",null,{}]
c:["$","script","script-0",{"src":"/_next/static/chunks/988ea292de4c4c73.js","async":true}]
d:["$","script","script-1",{"src":"/_next/static/chunks/e5d291186d0cbb54.js","async":true}]
e:["$","script","script-2",{"src":"/_next/static/chunks/bc166ea53d390db7.js","async":true}]
f:["$","$L2d",null,{"children":["$","$2e",null,{"name":"Next.MetadataOutlet","children":"$@2f"}]}]
10:["$","$L4",null,{"latex":"K","className":"mt-10"}]
11:["$","$L4",null,{"latex":"y_{K-1}","className":"mt-10"}]
12:["$","$L4",null,{"latex":"y_i^{(j)}","className":"mt-10"}]
13:["$","$L4",null,{"latex":"j","className":"mt-10"}]
14:["$","$L4",null,{"latex":"1\\leq i \\leq K-1","className":"mt-10"}]
15:["$","$L4",null,{"latex":"j","className":"mt-10"}]
16:["$","$L4",null,{"latex":"\\frac{\\partial \\ell}{\\partial y_i}","displayMode":true,"className":"mt-10"}]
17:["$","$L4",null,{"latex":"f","className":"mt-10"}]
18:["$","$L4",null,{"latex":"L_i","className":"mt-10"}]
19:["$","$L4",null,{"latex":"\\frac{\\partial \\ell}{\\partial W_i} = f\\left(y_i^{(j)},\\, \\frac{\\partial \\ell}{\\partial y_i}, L_i \\right) \\quad (\\text{weight gradient})","displayMode":true,"className":"mt-10"}]
1a:["$","$L4",null,{"latex":"\\frac{\\partial \\ell}{\\partial W_i} = \\left[\\frac{\\partial \\ell}{\\partial W_i^{(1)}}\\,\\middle|\\quad\\dots\\quad \\middle|\\, \\frac{\\partial \\ell}{\\partial W_i^{(N)}} \\right]","displayMode":true,"className":"mt-10"}]
1b:["$","span",null,{"className":"font-semibold","children":"ReduceScatter"}]
1c:["$","$L4",null,{"latex":"\\sum_{m=1}^N \\frac{\\partial \\ell}{\\partial W_i^{(j)}}\\quad (\\text{device}\\, j)","displayMode":true,"className":"mt-10"}]
1d:["$","$L4",null,{"latex":"N^{-1}","className":"mt-10"}]
1e:["$","$L4",null,{"latex":"W_i^{(j)}","className":"mt-10"}]
1f:["$","$L4",null,{"latex":"W_i^{(j)}","className":"mt-10"}]
20:["$","$L4",null,{"latex":"W_i^{(j)}","className":"mt-10"}]
21:["$","$L4",null,{"latex":"j","className":"mt-10"}]
22:["$","$L4",null,{"latex":"g","className":"mt-10"}]
23:["$","$L4",null,{"latex":"L_i","className":"mt-10"}]
24:["$","$L4",null,{"latex":"W_i","className":"mt-10"}]
25:["$","$L4",null,{"latex":"i","className":"mt-10"}]
26:["$","$L4",null,{"latex":"\\partial\\ell / \\partial y_i","className":"mt-10"}]
27:["$","$L4",null,{"latex":"\\frac{\\partial \\ell}{\\partial y_{i-1}} = g\\left(\\frac{\\partial\\ell}{\\partial y_i}, \\, W_i,\\, L_i\\right)\\quad (\\text{activation gradient})","displayMode":true,"className":"mt-10"}]
28:["$","$L4",null,{"latex":"i","className":"mt-10"}]
29:["$","$L4",null,{"latex":"i-1","className":"mt-10"}]
2a:["$","$L4",null,{"latex":"W_i","className":"mt-10"}]
2b:["$","$L4",null,{"latex":"W_i = \\textbf{AllGather}\\left\\{W_i^{(j)}: 1\\leq j \\leq N\\right\\}","displayMode":true,"className":"mt-10"}]
2f:null
