1:"$Sreact.fragment"
2:I[1565,["/_next/static/chunks/3020cc836eb789f8.js","/_next/static/chunks/e5d291186d0cbb54.js","/_next/static/chunks/bc166ea53d390db7.js"],"default"]
3:I[6366,["/_next/static/chunks/3020cc836eb789f8.js","/_next/static/chunks/e5d291186d0cbb54.js","/_next/static/chunks/bc166ea53d390db7.js"],"default"]
5:I[96045,["/_next/static/chunks/3020cc836eb789f8.js","/_next/static/chunks/e5d291186d0cbb54.js","/_next/static/chunks/bc166ea53d390db7.js"],"default"]
33:I[56691,["/_next/static/chunks/3020cc836eb789f8.js","/_next/static/chunks/e5d291186d0cbb54.js","/_next/static/chunks/bc166ea53d390db7.js"],"default"]
34:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"OutletBoundary"]
35:"$Sreact.suspense"
4:T501,Neural network weights is the first pieces of data that needs to be stored for the duration of training, it is used during forward pass to compute activation, and it is used during backwards pass to compute activation gradients. The second piece of data which require storage are the activation of each layers, since these are used to compute the  gradient of weights in the layer to which the activations are the inputs during forward pass, as we have already seen. Activation can be released whenever the weight gradients they are associated with are computed. The third piece of data that is stored are activation gradients. They are used to compute gradients during backpropagation through the next layer (or operation), and can be released after computing activation and weight gradients in the next layer (or operation). The fourth piece of data are weight gradients, with a lifetime from produced until the corresponding weights are updated. The fifth type of data are associated with the optimization algorithm which updates the weight using the gradients. In the Adam optimizer, we need to keep track of the exponential moving average of the weight and the square of each weight. These moving averages are ususally called optimizer states, and persist throughout training.0:{"buildId":"-PDM1eQFKJJmTk1GhsL6I","rsc":["$","$1","c",{"children":[["$","div",null,{"className":"flex flex-col justify-center min-h-screen","children":[["$","div",null,{"className":"flex flex-row justify-center pt-15","children":["$","$L2",null,{}]}],["$","div",null,{"className":"grow flex justify-center w-full pt-10 md:pt-15 px-3","children":["$","div",null,{"className":"flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl","children":[["$","div",null,{"className":"text-3xl font-bold flex justify-center","children":"6D Parallelism for Distributed Training"}],["$","div",null,{"className":"text-sm flex justify-center pt-5","children":null}],["$","div",null,{"className":"","children":[["$","div","0",{"children":["$undefined",["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Matrix multiplication is a fundamental operator in neural networks, and in the previous ",["$","$L3",null,{"text":"blog","url":"/ai/distributed-training"}]," we have fully elucidated the differentiation of the loss through such an operator in full generality, and we have applied this to the attention operator, a compound operator consisting of a series of matrix mulitiplications. Let us assemble a clear view of the process of training a neural network. In particular let us examine the data being stored and their persistence."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"$4"}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"The effective management of the persistence and distribution (across GPUS and CPUs) is a core systems optimization problem."}]]}]]}],["$","div","1",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Distributed Data Parallel"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["We replicate a set of model weights identically across ",["$","$L5",null,{"latex":"N","className":"mt-10"}]," GPUs. Each GPU holds a distinct collection of training data. Each GPU passes forward with the data to compute the loss, and performs backward pass to compute the gradient. The gradient is ","$L6"," over ","$L7"," parallel agents, and scaled by ","$L8"," to obtain the average gradient. Each local parallel agent now holds identical copies of the global gradient. This gradient produces identical optimizer states across ","$L9"," parallel agents. The optimizer have identical gradients, identical optimizer states, produces identical updated model weights across ","$La"," GPUs. The procedure is repeated."]}]}]]}],"$Lb","$Lc","$Ld","$Le","$Lf","$L10"]}]]}]}],"$L11"]}],["$L12","$L13","$L14"],"$L15"]}],"loading":null,"isPartial":false}
6:["$","span",null,{"className":"font-semibold","children":"all reduced"}]
7:["$","$L5",null,{"latex":"N","className":"mt-10"}]
8:["$","$L5",null,{"latex":"N^{-1}","className":"mt-10"}]
9:["$","$L5",null,{"latex":"N","className":"mt-10"}]
a:["$","$L5",null,{"latex":"N","className":"mt-10"}]
b:["$","div","2",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Fully Sharded Data Parallel"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Suppose a neural network consists of ",["$","$L5",null,{"latex":"K","className":"mt-10"}]," layers ",["$","$L5",null,{"latex":"\\{L_i\\}_{i=1}^K","className":"mt-10"}]," whereby each layer is a function with suitable domain and range. The neural network evaluated on ",["$","$L5",null,{"latex":"x","className":"mt-10"}]," leading to loss ",["$","$L5",null,{"latex":"\\ell(x)","className":"mt-10"}],", is defined by ",["$","$L5",null,{"latex":"\\ell (x) = \\left(L_K\\circ \\dots \\circ L_1 \\right)(x)","displayMode":true,"className":"mt-10"}]," Let us introduce the notation that ",["$","$L5",null,{"latex":"L_i","className":"mt-10"}]," is a function parameterized by weight ",["$","$L5",null,{"latex":"W_i","className":"mt-10"}]," and takes as input the previous layer's activation ",["$","$L5",null,{"latex":"y_{i-1} = \\left(L_{i-1}\\circ \\dots \\circ L_1 \\right)(x)","className":"mt-10"}],"."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Given dataset ",["$","$L5",null,{"latex":"\\mathcal{D}","className":"mt-10"}],", weight matrices ",["$","$L5",null,{"latex":"\\{W_i\\}_{i=1}^K","className":"mt-10"}]," and ",["$","$L5",null,{"latex":"N","className":"mt-10"}]," parallel agents (GPU, TPU etc) we partition the dataset into ",["$","$L5",null,{"latex":"N","className":"mt-10"}]," pariwise disjoint subsets ",["$","$L5",null,{"latex":"\\mathcal{D} = \\mathcal{D}^{(1)}\\,\\cup \\dots \\cup \\mathcal{D}^{(N)}","className":"mt-10"}]," and partition each weight matrix into N slices, for example ",["$","$L5",null,{"latex":"W_i = [W_i^{(1)}| \\dots | W_i^{(N)}]","displayMode":true,"className":"mt-10"}]," To the ",["$","$L5",null,{"latex":"j","className":"mt-10"}],"-th parallel agent we send ",["$","$L5",null,{"latex":"\\left(\\mathcal{D}^{(j)}, \\, W_1^{(j)}, \\dots, W_K^{(j)}\\right)","className":"mt-10"}]]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["To describe one step in training, each of the parallel agents ",["$","$L5",null,{"latex":"1\\leq j \\leq N","className":"mt-10"}]," selects a training data ",["$","$L5",null,{"latex":"x^{(j)}\\in \\mathcal{D}^{(j)}","className":"mt-10"}]," and puts ",["$","$L5",null,{"latex":"y_0^{(j)}=x^{(j)}","className":"mt-10"}],". For ",["$","$L5",null,{"latex":"1\\leq i \\leq K","className":"mt-10"}]," suppose the activation ",["$","$L5",null,{"latex":"y_{i-1}^{(j)}","className":"mt-10"}]," has been computed and the forward pass is ready to advance through layer ",["$","$L5",null,{"latex":"L_i","className":"mt-10"}]," on parallel agent ",["$","$L5",null,{"latex":"j","className":"mt-10"}]," we call ",["$","$L5",null,{"latex":"W_i = \\textbf{AllGather}\\left\\{W_i^{(j)}: 1\\leq j \\leq N\\right\\}","displayMode":true,"className":"mt-10"}]," and compute ",["$","$L5",null,{"latex":"y^{(j)}_i=L_i(y^{(j)}_{i-1})","className":"mt-10"}]," and immediately free the memory on device ",["$","$L5",null,{"latex":"j","className":"mt-10"}]," of the weight shards ",["$","$L5",null,{"latex":"\\textbf{delete}\\left\\{W_i^{(k)}\\in W_i: j\\neq k\\right\\}\\quad (\\text{device}\\,j)","displayMode":true,"className":"mt-10"}]," and store the layer-",["$","$L5",null,{"latex":"i","className":"mt-10"}]," activation for weight gradient calculation through ",["$","$L5",null,{"latex":"L_i","className":"mt-10"}]," during backward pass ",["$","$L5",null,{"latex":"\\textbf{store}\\,\\, y_i^{(j)}\\quad(\\text{device } j)","displayMode":true,"className":"mt-10"}]," Let us turn to backwards pass. In our notation ",["$","$L5",null,{"latex":"L_K","className":"mt-10"}]," computes the loss function and this layer has no weights, so as backwards we get ",["$","$L5",null,{"latex":"\\frac{\\partial \\ell}{\\partial y_{K-1}}","displayMode":true,"className":"mt-10"}],"just as reminder ",["$","$L5",null,{"latex":"y_{K-1}","className":"mt-10"}]," is the output of layer ",["$","$L5",null,{"latex":"K-1","className":"mt-10"}],", which is the input to layer ","$L16",". We will treat ","$L17"," as a variable, whereas the actual activation has a superscript ","$L18"," to identify the device ","$L19",". For each ","$L1a"," supposed that device ","$L1b"," computed its local activation gradient ","$L1c"," have computed. Then the local weight gradient can be computed as a function ","$L1d"," of the map ","$L1e",", the activation gradient, and the local activation ","$L1f"," Since weights are kept in shards across the parallel agents, we need to partition the weight gradient too ","$L20"," we will use ","$L21"," to provide each parallel agent with the average gradient corresponding to the weight shard it owns, namely it receives ","$L22"," scaling by ","$L23"," this is the average gradient for its local weight shard ","$L24",". The local Adam optimizer stores exponential moving averages of ","$L25"," and the component-wise square of ","$L26",". Thus optimizer states are sharded as well. Finally each parallel agent ","$L27"," needs to compute its local activation gradient as a function ","$L28"," of the function ","$L29"," (that we will differentiate via the chain rule) and the weight ","$L2a"," of layer ","$L2b"," together with ","$L2c"," namely ","$L2d"," Computing this allows  backpropagation to advance from layer ","$L2e"," to layer ","$L2f",". To do this we need to AllGather the weight ","$L30"," on each of the parallel agents  ","$L31"," After computing the activaiton gradient ","$L32"]}]]}]]}]
c:["$","div","3",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Tensor Parallel"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Attention and MLP are two fundamental operators in a transformer, and their underlying mathematical operation is matrix multiplication. There are two fundamental ways to write matrix product with block matrices, ",["$","$L5",null,{"latex":"AB=\\begin{bmatrix}A_1 \\quad A_2 \\end{bmatrix}\\begin{bmatrix}B_1 \\\\ B_2 \\end{bmatrix}=[A_1B_1+A_2B_2]","displayMode":true,"className":"mt-10"}]," ",["$","$L5",null,{"latex":" AB=A \\begin{bmatrix} B_1\\quad B_2 \\end{bmatrix} = \\begin{bmatrix}AB_1\\quad AB_2  \\end{bmatrix}","displayMode":true,"className":"mt-10"}]," giving rise to two ways in which a GEMM can be performed across GPUs: the first way requires separate block matrix multiplications followed by AllReduce, and the second way requires AllGather. Tensor parallelism is the application of these simple ideas to the matrix multiplications that occur in a transformer."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Let us take the MLP layer as the first example. Recall that in modern architectures the feedforward is computed as  ",["$","$L5",null,{"latex":"\\text{SwiGLU}(X, W_1, W_2, W_3) = W_3\\left(\\text{SiLU}(W_1X)\\,\\odot\\, W_2 X\\right)","displayMode":true,"className":"mt-10"}]," where the Swish linear unit (SiLU) activation function ",["$","$L5",null,{"latex":"\\text{SiLU}(a) = \\frac{a}{1+e^{-a}}\\quad (a\\in\\mathbb{R})","displayMode":true,"className":"mt-10"}]," is applied componentwise to ",["$","$L5",null,{"latex":"X","className":"mt-10"}],". Let there be ",["$","$L5",null,{"latex":"N","className":"mt-10"}]," parallel agents, we can write ",["$","$L5",null,{"latex":"W_i = [W_i^{(1)}\\, \\mid\\, \\dots \\mid\\, W_i^{(N)}]\\quad (1\\leq i\\leq 3)","displayMode":true,"className":"mt-10"}]," so that each device ",["$","$L5",null,{"latex":"1\\leq j \\leq N","className":"mt-10"}]," computes ",["$","$L5",null,{"latex":"Y^{(j)}=\\text{SiLU}(W_1^{(j)}X)\\,\\odot\\, W_2^{(j)} X","displayMode":true,"className":"mt-10"}]," then we can either AllGather ",["$","$L5",null,{"latex":"Y^{(j)}","className":"mt-10"}]," and partition along the column direction and perform matrix multiplication of the first type above, then all reduce, or we can first partition ",["$","$L5",null,{"latex":"Y^{(j)}","className":"mt-10"}]," along its column direction ",["$","$L5",null,{"latex":"Y^{(j)} =\\begin{bmatrix}Y^{{(1j)}} \\\\ \\\\ \\vdots \\\\ \\\\ Y^{{(Nj)}} \\end{bmatrix}","displayMode":true,"className":"mt-10"}]," We then use ",["$","span",null,{"className":"font-semibold","children":"AllToAll"}]," to send ",["$","$L5",null,{"latex":"Y^{(ij)}","className":"mt-10"}]," from ",["$","$L5",null,{"latex":"j","className":"mt-10"}]," to ",["$","$L5",null,{"latex":"1\\leq i \\leq N","className":"mt-10"}]," so that on device ",["$","$L5",null,{"latex":"j","className":"mt-10"}]," we get ",["$","$L5",null,{"latex":"\\widehat{Y}^{(j)} = [Y^{(j1)}\\dots\\, Y^{(jN)}]","displayMode":true,"className":"mt-10"}]," which satisfies ",["$","$L5",null,{"latex":"\\text{SiLU}(W_1X)\\,\\odot\\, W_2 X = \\begin{bmatrix}\\widehat{Y}^{(1)} \\\\ \\\\ \\vdots \\\\ \\\\ \\widehat{Y}^{(N)}\\end{bmatrix}","displayMode":true,"className":"mt-10"}]," Now each device ",["$","$L5",null,{"latex":"j","className":"mt-10"}]," computes ",["$","$L5",null,{"latex":"W_3^{(j)} \\widehat{Y}^{(j)}","className":"mt-10"}]," followed by AllReduce to obtain ",["$","$L5",null,{"latex":"W_3\\left(\\text{SiLU}(W_1X)\\,\\odot\\, W_2 X\\right)=\\textbf{AllReduce}\\left\\{W_3^{(j)} \\widehat{Y}^{(j)}: 1\\leq j \\leq N\\right\\}","displayMode":true,"className":"mt-10"}]]}]]}]]}]
d:["$","div","4",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Sequence Parallel"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The root mean square normalization with ",["$","$L5",null,{"latex":"\\varepsilon > 0","className":"mt-10"}]," is a mapping ",["$","$L5",null,{"latex":"\\rho:\\mathbb{R}^d\\times \\mathbb{R}^d\\rightarrow \\mathbb{R}^d","className":"mt-10"}]," ",["$","$L5",null,{"latex":"\\rho(x, w) = \\frac{x\\,\\odot\\,w}{\\left(\\frac{x^\\top x}{d} + \\varepsilon\\right)^{1/2}}","displayMode":true,"className":"mt-10"}]," Where ",["$","$L5",null,{"latex":"(x\\,\\odot\\,w)_i=x_iw_i","className":"mt-10"}]," is the componentwise product. The ",["$","span",null,{"className":"font-semibold","children":"RMS normalization"}]," layer of a LLM is a positionwise operator, meaning that for a sequence of ",["$","$L5",null,{"latex":"L","className":"mt-10"}]," token embeddings ",["$","$L5",null,{"latex":"\\{t_1,\\dots, t_L\\}\\subset \\mathbb{R}^d","className":"mt-10"}]," the RMSNorm layer with trainable weight ",["$","$L5",null,{"latex":"w","className":"mt-10"}]," transforms this sequence into ",["$","$L5",null,{"latex":"\\{\\rho(t_1, w), \\dots, \\rho(t_L, w)\\}","className":"mt-10"}],". This positionwise property enables RMSNorm to be computed on separate GPUs then subsequently AllGathered, and is an effective method for managing long context. In particular, in a pre-norm transformer architecture, sequence parallel then be followed by tensor parallel we described above."]}]}]]}]
e:["$","div","5",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Context Parallel"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"TBD"}]}]]}]
f:["$","div","6",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Pipeline Parallel"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"TBD"}]}]]}]
10:["$","div","7",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Expert Parallel"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"TBD"}]}]]}]
11:["$","$L33",null,{}]
12:["$","script","script-0",{"src":"/_next/static/chunks/3020cc836eb789f8.js","async":true}]
13:["$","script","script-1",{"src":"/_next/static/chunks/e5d291186d0cbb54.js","async":true}]
14:["$","script","script-2",{"src":"/_next/static/chunks/bc166ea53d390db7.js","async":true}]
15:["$","$L34",null,{"children":["$","$35",null,{"name":"Next.MetadataOutlet","children":"$@36"}]}]
16:["$","$L5",null,{"latex":"K","className":"mt-10"}]
17:["$","$L5",null,{"latex":"y_{K-1}","className":"mt-10"}]
18:["$","$L5",null,{"latex":"y_i^{(j)}","className":"mt-10"}]
19:["$","$L5",null,{"latex":"j","className":"mt-10"}]
1a:["$","$L5",null,{"latex":"1\\leq i \\leq K-1","className":"mt-10"}]
1b:["$","$L5",null,{"latex":"j","className":"mt-10"}]
1c:["$","$L5",null,{"latex":"\\frac{\\partial \\ell}{\\partial y_i}","displayMode":true,"className":"mt-10"}]
1d:["$","$L5",null,{"latex":"f","className":"mt-10"}]
1e:["$","$L5",null,{"latex":"L_i","className":"mt-10"}]
1f:["$","$L5",null,{"latex":"\\frac{\\partial \\ell}{\\partial W_i} = f\\left(y_i^{(j)},\\, \\frac{\\partial \\ell}{\\partial y_i}, L_i \\right) \\quad (\\text{weight gradient})","displayMode":true,"className":"mt-10"}]
20:["$","$L5",null,{"latex":"\\frac{\\partial \\ell}{\\partial W_i} = \\left[\\frac{\\partial \\ell}{\\partial W_i^{(1)}}\\,\\middle|\\quad\\dots\\quad \\middle|\\, \\frac{\\partial \\ell}{\\partial W_i^{(N)}} \\right]","displayMode":true,"className":"mt-10"}]
21:["$","span",null,{"className":"font-semibold","children":"ReduceScatter"}]
22:["$","$L5",null,{"latex":"\\sum_{m=1}^N \\frac{\\partial \\ell}{\\partial W_i^{(j)}}\\quad (\\text{device}\\, j)","displayMode":true,"className":"mt-10"}]
23:["$","$L5",null,{"latex":"N^{-1}","className":"mt-10"}]
24:["$","$L5",null,{"latex":"W_i^{(j)}","className":"mt-10"}]
25:["$","$L5",null,{"latex":"W_i^{(j)}","className":"mt-10"}]
26:["$","$L5",null,{"latex":"W_i^{(j)}","className":"mt-10"}]
27:["$","$L5",null,{"latex":"j","className":"mt-10"}]
28:["$","$L5",null,{"latex":"g","className":"mt-10"}]
29:["$","$L5",null,{"latex":"L_i","className":"mt-10"}]
2a:["$","$L5",null,{"latex":"W_i","className":"mt-10"}]
2b:["$","$L5",null,{"latex":"i","className":"mt-10"}]
2c:["$","$L5",null,{"latex":"\\partial\\ell / \\partial y_i","className":"mt-10"}]
2d:["$","$L5",null,{"latex":"\\frac{\\partial \\ell}{\\partial y_{i-1}} = g\\left(\\frac{\\partial\\ell}{\\partial y_i}, \\, W_i,\\, L_i\\right)\\quad (\\text{activation gradient})","displayMode":true,"className":"mt-10"}]
2e:["$","$L5",null,{"latex":"i","className":"mt-10"}]
2f:["$","$L5",null,{"latex":"i-1","className":"mt-10"}]
30:["$","$L5",null,{"latex":"W_i","className":"mt-10"}]
31:["$","$L5",null,{"latex":"W_i = \\textbf{AllGather}\\left\\{W_i^{(j)}: 1\\leq j \\leq N\\right\\}","displayMode":true,"className":"mt-10"}]
32:["$","$L5",null,{"latex":"\\textbf{delete}\\left\\{W_i^{(k)}\\in W_i: j\\neq k\\right\\}\\quad (\\text{device}\\,j)","displayMode":true,"className":"mt-10"}]
36:null
