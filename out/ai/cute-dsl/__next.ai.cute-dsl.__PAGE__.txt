1:"$Sreact.fragment"
2:I[1565,["/_next/static/chunks/8850d1d03d80978f.js","/_next/static/chunks/9b725c66512530e6.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
3:I[96045,["/_next/static/chunks/8850d1d03d80978f.js","/_next/static/chunks/9b725c66512530e6.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
11:I[66832,["/_next/static/chunks/8850d1d03d80978f.js","/_next/static/chunks/9b725c66512530e6.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
18:I[56691,["/_next/static/chunks/8850d1d03d80978f.js","/_next/static/chunks/9b725c66512530e6.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
19:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"OutletBoundary"]
1a:"$Sreact.suspense"
0:{"buildId":"0CqBaPu4ZjMxMPWU9ZJ5h","rsc":["$","$1","c",{"children":[["$","div",null,{"className":"flex flex-col justify-center min-h-screen","children":[["$","div",null,{"className":"flex flex-row justify-center pt-15","children":["$","$L2",null,{}]}],["$","div",null,{"className":"grow flex justify-center w-full pt-10 md:pt-15 px-3","children":["$","div",null,{"className":"flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl","children":[["$","div",null,{"className":"text-3xl font-bold flex justify-center","children":"Programming Blackwell Tensor Core with Cutlass and CuTe DSL"}],["$","div",null,{"className":"text-sm flex justify-center pt-5","children":null}],["$","div",null,{"className":"","children":[["$","div","0",{"children":["$undefined",["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The CuTe ",["$","span",null,{"className":"italic","children":"domain specific language"}]," (DSL) provides a Python interface that enables efficient matrix muliplication GPU kernels implementation leveraging the tile based abstraction. To gain a clear understanding of how the DSL provides an interface to highly efficient kernel, we examine hardware mechanisms, memory structures, CUDA programming patterns, and the DSL abstractions. In particular, the DSL offers Blackwell specific support in the ",["$","span",null,{"className":"font-semibold","children":"cutlass.cute.nvgpu.tcgen05"}]," module. Since our discussions about this module will not shy away from hardware, we will also take excusrions of Blackwell tensor core programming at the more fundamental ",["$","span",null,{"className":"italic","children":"parallel thread excution"}]," (PTX) virtual instruction set level."]}]}]]}],["$","div","1",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Fundamentals"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["A ",["$","span",null,{"className":"font-semibold","children":"cooperative thread array"}]," (CTA) is a set of threads that excute a kernel concurrently or in parallel. A ",["$","span",null,{"className":"font-semibold","children":"warp"}]," of a CTA is a maximum subset of the set of threads in the CTA that can execute the same instruction simultaneously. In common hardware, a warp is a ",["$","$L3",null,{"latex":"32","className":"mt-10"}]," thread subset of the CTA. Threads in a CTA can be organized into a block of 1D, 2D, or 3D block of dimensions ",["$","$L3",null,{"latex":"X\\times Y\\times Z","className":"mt-10"}]," and each thread indexed by a tuple ",["$","$L3",null,{"latex":"(i, j, k)","className":"mt-10"}],". If ",["$","$L3",null,{"latex":"Y=Z=1","className":"mt-10"}]," then the CTA is a 1D block and so forth. One can enumerate the threads in an ",["$","$L3",null,{"latex":"X\\times Y\\times Z","className":"mt-10"}]," CTA by ",["$","$L3",null,{"latex":"(i,j,k)\\mapsto i + jX + kXY","displayMode":true,"className":"mt-10"}],"We can enumerate all threads in a CTA and every four consecutive warps where the begining thread has thread ID ",["$","$L3",null,{"latex":"I\\equiv 0 \\mod 128","className":"mt-10"}]," is called a is called a ",["$","span",null,{"className":"font-semibold","children":"warp group"}],". The warps in a warp group are associated with a ",["$","span",null,{"className":"font-semibold","children":"warp rank"}]," from ",["$","$L3",null,{"latex":"0\\sim 3","className":"mt-10"}],"."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The ",["$","span",null,{"className":"font-semibold","children":"occupancy"}]," of a kernel is the ratio of active warps to the maximum warps supported by a streaming multiprocessor (SM)."]}],"$L4","$L5"]}]]}],"$L6","$L7","$L8","$L9","$La","$Lb"]}]]}]}],"$Lc"]}],["$Ld","$Le","$Lf"],"$L10"]}],"loading":null,"isPartial":false}
4:["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Threads in a CTA have access to a memory space called ",["$","span",null,{"className":"font-semibold","children":"shared memory"}]," which is structured into ",["$","$L3",null,{"latex":"32","className":"mt-10"}]," ",["$","span",null,{"className":"font-semibold","children":"banks"}]," to faciliate parallel access by a warp. When a shared memory bank is accessed by multiple threads in the same warp, then access can no longer be parallel as single access, and must be serialized, this situation is often described as a ",["$","span",null,{"className":"italic","children":"bank conflict"}],"."]}]
5:["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Global memory is accessed in units of ",["$","$L3",null,{"latex":"32","className":"mt-10"}]," bytes. Thus to achieve a ratio of bytes used to bytes accessed close to ",["$","$L3",null,{"latex":"1","className":"mt-10"}]," we want to ",["$","span",null,{"className":"font-semibold","children":"coalesce global memeory access"}],"."]}]
6:["$","div","2",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Async Pipeline"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["When solving a problem like matrix multiplication on GPU, the program first loads a small matrix block (or a tile of data as it is often refered to in Cutlass terminology) from global memory to shared memory, then a block matrix multiply is performed on the loaded data. This follows a so called ",["$","span",null,{"className":"font-semibold","children":"producer-consumer"}]," pattern, and an important performance optimization is to leverage async to our advantage when it comes to overlapping load, multiply, and store. The CuTe DSL exposes this pattern in ",["$","span",null,{"className":"italic","children":"PipelineAsync"}],", ",["$","span",null,{"className":"italic","children":"PipelineProducer"}]," and ",["$","span",null,{"className":"italic","children":"PipelineConsumer"}],"."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"The producer uses the following mechanisms to wait for an opportunity to write to data to memory where the consumer can then use (this is blocking), and how ot signal to the consumer that the data is ready to be used."}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["$","$L11",null,{"code":"PipelineProducer.acquire()\n\nPipelineProducer.commit(PipelineProducer.ImmutableProducerHandle)\n# or\nPipelineProducer.ImmutableProducerHandle.commit()\n","tokens":[[{"content":"PipelineProducer.","offset":0,"color":"#EEF0F9","fontStyle":0},{"content":"acquire","offset":17,"color":"#00DAEF","fontStyle":0},{"content":"()","offset":24,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"PipelineProducer.","offset":28,"color":"#EEF0F9","fontStyle":0},{"content":"commit","offset":45,"color":"#00DAEF","fontStyle":0},{"content":"(PipelineProducer.ImmutableProducerHandle)","offset":51,"color":"#EEF0F9","fontStyle":0}],[{"content":"# or","offset":94,"color":"#EEF0F98F","fontStyle":1}],[{"content":"PipelineProducer.ImmutableProducerHandle.","offset":99,"color":"#EEF0F9","fontStyle":0},{"content":"commit","offset":140,"color":"#00DAEF","fontStyle":0},{"content":"()","offset":146,"color":"#EEF0F9","fontStyle":0}],[]]}]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"The consumer must wait (blocking operation) for producer to get data to do its work, and then to release the memory for producers to use. The mechanisms are"}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["$","$L11",null,{"code":"PipelineConsumer.wait()\n\nPipelineConsumerHandle.release(PipelineConsumer.ImmutableConsumerHandle)\n# or\nPipelineConsumer.ImmutableConsumerHandle.release()\n","tokens":[[{"content":"PipelineConsumer.","offset":0,"color":"#EEF0F9","fontStyle":0},{"content":"wait","offset":17,"color":"#00DAEF","fontStyle":0},{"content":"()","offset":21,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"PipelineConsumerHandle.","offset":25,"color":"#EEF0F9","fontStyle":0},{"content":"release","offset":48,"color":"#00DAEF","fontStyle":0},{"content":"(PipelineConsumer.ImmutableConsumerHandle)","offset":55,"color":"#EEF0F9","fontStyle":0}],[{"content":"# or","offset":98,"color":"#EEF0F98F","fontStyle":1}],[{"content":"PipelineConsumer.ImmutableConsumerHandle.","offset":103,"color":"#EEF0F9","fontStyle":0},{"content":"release","offset":144,"color":"#00DAEF","fontStyle":0},{"content":"()","offset":151,"color":"#EEF0F9","fontStyle":0}],[]]}]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"For example, in a kernel one may have"}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"$L12"}],"$L13","$L14","$L15"]}]]}]
7:["$","div","3",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Tensor Memory"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["For each cooperative thread array (CTA), the structure of tensor memory is a ",["$","$L3",null,{"latex":"128 \\times 512","className":"mt-10"}]," matrix of ",["$","$L3",null,{"latex":"32","className":"mt-10"}]," bit cells. Each of the ",["$","$L3",null,{"latex":"128","className":"mt-10"}]," rows in tensor memory is also referred to as a ",["$","span",null,{"className":"font-semibold","children":"lane"}],", which is ",["$","$L3",null,{"latex":"2","className":"mt-10"}]," KB in size. Thus the lanes are in bijective correspondence with the threads in a warp group. In fact the lanes are divided between the warps, so that warp rank ",["$","$L3",null,{"latex":"0","className":"mt-10"}]," can only access lanes ",["$","$L3",null,{"latex":"0-31","className":"mt-10"}]," using ",["$","span",null,{"className":"font-semibold","children":"ld"}]," load, ",["$","span",null,{"className":"font-semibold","children":"st"}]," store,     ",["$","span",null,{"className":"font-semibold","children":"cp"}]," copy tcgen05 instructions which are issued on inputs of lane ",["$","$L3",null,{"latex":"\\times","className":"mt-10"}]," size. Each warp can access all columns within is accessible lane. Tensor memory allocation and deallocation use the ",["$","span",null,{"className":"font-semibold","children":"alloc"}]," and ",["$","span",null,{"className":"font-semibold","children":"dealloc"}]," instructions and are allocated in units of ",["$","$L3",null,{"latex":"32","className":"mt-10"}]," columns, and the number of allocated columns must be a power of ",["$","$L3",null,{"latex":"2","className":"mt-10"}],". Thus when a column is allocated, each of its ",["$","$L3",null,{"latex":"128","className":"mt-10"}]," lanes are allocated."]}]}]]}]
8:["$","div","4",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Tensor Memory Accelerator"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The instruction ",["$","span",null,{"className":"font-semibold","children":"cp.async.bulk.tensor"}]," is used to copy tensor asynchronously between different types of memory locations (ex. from global to shared memory and back). This is facilitated by a hardware unit called the tensor memory accelerator (TMA). CUDA exposes this instruction via ",["$","span",null,{"className":"font-semibold","children":"cuda::memcpy_async"}]," which can be issued by a single thread within a warp and the synchronization mechanism is ",["$","span",null,{"className":"font-semibold","children":"cuda::barrier"}],". The PTX instruction also allows copying from global memory to the shared memory of ",["$","span",null,{"className":"italic","children":"multiple"}]," CTAs in a cluster, this is so called multicasting and is done by invoking the instruction modifier ",["$","span",null,{"className":"font-semibold","children":".multicast::cluster"}],". One can even copy to distributed shared memory."]}]}]]}]
9:["$","div","5",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Matrix Multiplication Instructions"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":[["$","span",null,{"className":"font-semibold","children":"Matrix multiply and accumulate"}]," (MMA) are excuted by ",["$","span",null,{"className":"font-semibold","children":"mma"}]," instruction which depend on a number launch configurations, such as the data type of input pair of matrices and their shapes, the data type of accumlation or output matrix. Support for warp-specialization, whether one or two CTAs cooperate, and whether the matmul is dense or sparse. For instance 2CTA FP16 dense MMA can have largest shape ",["$","$L3",null,{"latex":"M\\times N\\times K=256 \\times 256 \\times 16","className":"mt-10"}]," while NVFP4 dense MMA has largest shape ",["$","$L3",null,{"latex":"M\\times N\\times K=256\\times 256 \\times 64 ","className":"mt-10"}]]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["MMA configuration are stored on register as a 32 bit ",["$","span",null,{"className":"font-semibold","children":"instruction descriptor"}],". For example bits 13 to 16 describe whether the input matrices needs to be ",["$","span",null,{"className":"font-semibold","children":"transposed"}]," or ",["$","span",null,{"className":"font-semibold","children":"negated"}],". Other data store includes sparsity, input output data type."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["MAA supports ",["$","span",null,{"className":"font-semibold","children":"block scaled"}]," matrix multiplication for data types such as ",["$","span",null,{"className":"font-semibold","children":"mxf4"}]," and ",["$","span",null,{"className":"font-semibold","children":"nvf4"}],"."]}]]}]]}]
a:["$","div","6",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Granularity and Synchronization"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["TensorCore Gen 5 Instructions such as ",["$","span",null,{"className":"font-semibold","children":"mma"}],", ",["$","span",null,{"className":"font-semibold","children":"cp"}]," can be issued by a single thread within a CTA or CTA-Pair. Instructions such as ",["$","span",null,{"className":"font-semibold","children":"alloc"}]," and ",["$","span",null,{"className":"font-semibold","children":"dealloc"}]," can be issued from a warp of a pair of warps one in each CTA-pair. Each warp can access a quarter of the tensor memory via ",["$","span",null,{"className":"font-semibold","children":"ld"}]," and ",["$","span",null,{"className":"font-semibold","children":"st"}]," so a warp group is required for complete tensor memory access. These characterize the instruction issuing granularity."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Instructions can be synchronous (alloc/dealloc, fence) or asynchronous (cp, mma, ld, st). Special pairs of asyncrhonous instructions, called ",["$","span",null,{"className":"font-semibold","children":"pipelined instructions"}]," are guarenteed to excute in the order they are issued, an example is ",["$","span",null,{"className":"font-semibold","children":"tcgen05.copy.cta_group::N"}]," and ",["$","span",null,{"className":"font-semibold","children":"tcgen05.mma.cta_group::N"}]," for the same N."]}]]}]]}]
b:["$","div","7",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"CuTe DSL"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["$","$L11",null,{"code":"# useful methods in cutlass.cute.nvgpu\nfrom cutlass.cute.nvgpu import (\n    CopyUniversalOp, MmaUniversalOp,\n    make_tiled_tma_atom_A, make_tiled_tma_atom_B\n)\n\n# warp level operations\nfrom cutlass.cute.nvgpu.warp import (\n    MmaF16BF16Op, MmaMXF4Op, MmaMXF4NVF4Op  # MMA \n    LdMatrix8x8x16bOp, StMatrix8x8x16bOp # load & store (&more for other shapes)\n)\n\n# warp group\nfrom cutlass.cute.nvgpu.warpgroup import (\n    MmaF16BF16Op,\n    make_smem_layout_atom,\n    fence, commit, wait_group\n)\n\n# TMA\nfrom class cutlass.cute.nvgpu.cpasync import (\n    CopyBulkTensorTileG2SOp, CopyBulkTensorTileS2GOp # G2S <-> S2G means global and shared memory\n    CopyBulkTensorTileG2SMulticastOp, # plus multicast copy in reverse direction \n    make_tiled_tma_atom,\n)\n","tokens":[[{"content":"# useful methods in cutlass.cute.nvgpu","offset":0,"color":"#EEF0F98F","fontStyle":1}],[{"content":"from","offset":39,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.cute.nvgpu ","offset":43,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":63,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":69,"color":"#EEF0F9","fontStyle":0}],[{"content":"    CopyUniversalOp, MmaUniversalOp,","offset":72,"color":"#EEF0F9","fontStyle":0}],[{"content":"    make_tiled_tma_atom_A, make_tiled_tma_atom_B","offset":109,"color":"#EEF0F9","fontStyle":0}],[{"content":")","offset":158,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"# warp level operations","offset":161,"color":"#EEF0F98F","fontStyle":1}],[{"content":"from","offset":185,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.cute.nvgpu.warp ","offset":189,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":214,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":220,"color":"#EEF0F9","fontStyle":0}],[{"content":"    MmaF16BF16Op, MmaMXF4Op, MmaMXF4NVF4Op  ","offset":223,"color":"#EEF0F9","fontStyle":0},{"content":"# MMA ","offset":267,"color":"#EEF0F98F","fontStyle":1}],[{"content":"    LdMatrix8x8x16bOp, StMatrix8x8x16bOp ","offset":274,"color":"#EEF0F9","fontStyle":0},{"content":"# load & store (&more for other shapes)","offset":315,"color":"#EEF0F98F","fontStyle":1}],[{"content":")","offset":355,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"# warp group","offset":358,"color":"#EEF0F98F","fontStyle":1}],[{"content":"from","offset":371,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.cute.nvgpu.warpgroup ","offset":375,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":405,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":411,"color":"#EEF0F9","fontStyle":0}],[{"content":"    MmaF16BF16Op,","offset":414,"color":"#EEF0F9","fontStyle":0}],[{"content":"    make_smem_layout_atom,","offset":432,"color":"#EEF0F9","fontStyle":0}],[{"content":"    fence, commit, wait_group","offset":459,"color":"#EEF0F9","fontStyle":0}],[{"content":")","offset":489,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"# TMA","offset":492,"color":"#EEF0F98F","fontStyle":1}],[{"content":"from","offset":498,"color":"#54B9FF","fontStyle":1},{"content":" ","offset":502,"color":"#EEF0F9","fontStyle":0},{"content":"class","offset":503,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.cute.nvgpu.cpasync ","offset":508,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":536,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":542,"color":"#EEF0F9","fontStyle":0}],[{"content":"    CopyBulkTensorTileG2SOp, CopyBulkTensorTileS2GOp ","offset":545,"color":"#EEF0F9","fontStyle":0},{"content":"# G2S <-> S2G means global and shared memory","offset":598,"color":"#EEF0F98F","fontStyle":1}],[{"content":"    CopyBulkTensorTileG2SMulticastOp, ","offset":643,"color":"#EEF0F9","fontStyle":0},{"content":"# plus multicast copy in reverse direction ","offset":681,"color":"#EEF0F98F","fontStyle":1}],[{"content":"    make_tiled_tma_atom,","offset":725,"color":"#EEF0F9","fontStyle":0}],[{"content":")","offset":750,"color":"#EEF0F9","fontStyle":0}],[]]}]}],"$L16","$L17"]}]]}]
c:["$","$L18",null,{}]
d:["$","script","script-0",{"src":"/_next/static/chunks/8850d1d03d80978f.js","async":true}]
e:["$","script","script-1",{"src":"/_next/static/chunks/9b725c66512530e6.js","async":true}]
f:["$","script","script-2",{"src":"/_next/static/chunks/5ad9eb95768fc0a4.js","async":true}]
10:["$","$L19",null,{"children":["$","$1a",null,{"name":"Next.MetadataOutlet","children":"$@1b"}]}]
12:["$","$L11",null,{"code":"# Warp 0\nproducer_group = cutlass.pipeline.CooperativeGroup(\n    cutlass.pipeline.Agent.Thread, 32\n)\n# Warp 1\nconsumer_group = cutlass.pipeline.CooperativeGroup(\n    cutlass.pipeline.Agent.Thread, 32\n)\n\npipeline = cutlass.pipeline.PipelineAsync.create(\n    producer_group=producer_group,\n    consumer_group=consumer_group,\n    ...\n)\n\nproducer, consumer = pipeline.make_participants()\n\n\n# producer warp\nif warp_idx == 0:\n\n    handle = producer.acquire_and_advance()\n\n    # store data ...\n\n    handle.commit()\n    producer.tail()\n\n# consumer warp\nif warp_idx == 1:\n\n    handle = consumer.wait_and_advance()\n\n    # do work with data ...\n\n    handle.release()\n","tokens":[[{"content":"# Warp 0","offset":0,"color":"#EEF0F98F","fontStyle":1}],[{"content":"producer_group = cutlass.pipeline.","offset":9,"color":"#EEF0F9","fontStyle":0},{"content":"CooperativeGroup","offset":43,"color":"#00DAEF","fontStyle":0},{"content":"(","offset":59,"color":"#EEF0F9","fontStyle":0}],[{"content":"    cutlass.pipeline.Agent.Thread, ","offset":61,"color":"#EEF0F9","fontStyle":0},{"content":"32","offset":96,"color":"#FFD493","fontStyle":0}],[{"content":")","offset":99,"color":"#EEF0F9","fontStyle":0}],[{"content":"# Warp 1","offset":101,"color":"#EEF0F98F","fontStyle":1}],[{"content":"consumer_group = cutlass.pipeline.","offset":110,"color":"#EEF0F9","fontStyle":0},{"content":"CooperativeGroup","offset":144,"color":"#00DAEF","fontStyle":0},{"content":"(","offset":160,"color":"#EEF0F9","fontStyle":0}],[{"content":"    cutlass.pipeline.Agent.Thread, ","offset":162,"color":"#EEF0F9","fontStyle":0},{"content":"32","offset":197,"color":"#FFD493","fontStyle":0}],[{"content":")","offset":200,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"pipeline = cutlass.pipeline.PipelineAsync.","offset":203,"color":"#EEF0F9","fontStyle":0},{"content":"create","offset":245,"color":"#00DAEF","fontStyle":0},{"content":"(","offset":251,"color":"#EEF0F9","fontStyle":0}],[{"content":"    ","offset":253,"color":"#EEF0F9","fontStyle":0},{"content":"producer_group","offset":257,"color":"#4BF3C8","fontStyle":1},{"content":"=producer_group,","offset":271,"color":"#EEF0F9","fontStyle":0}],[{"content":"    ","offset":288,"color":"#EEF0F9","fontStyle":0},{"content":"consumer_group","offset":292,"color":"#4BF3C8","fontStyle":1},{"content":"=consumer_group,","offset":306,"color":"#EEF0F9","fontStyle":0}],[{"content":"    ","offset":323,"color":"#EEF0F9","fontStyle":0},{"content":"...","offset":327,"color":"#FFD493","fontStyle":0}],[{"content":")","offset":331,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"producer, consumer = pipeline.","offset":334,"color":"#EEF0F9","fontStyle":0},{"content":"make_participants","offset":364,"color":"#00DAEF","fontStyle":0},{"content":"()","offset":381,"color":"#EEF0F9","fontStyle":0}],[],[],[{"content":"# producer warp","offset":386,"color":"#EEF0F98F","fontStyle":1}],[{"content":"if","offset":402,"color":"#54B9FF","fontStyle":1},{"content":" warp_idx == ","offset":404,"color":"#EEF0F9","fontStyle":0},{"content":"0","offset":417,"color":"#FFD493","fontStyle":0},{"content":":","offset":418,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"    handle = producer.","offset":421,"color":"#EEF0F9","fontStyle":0},{"content":"acquire_and_advance","offset":443,"color":"#00DAEF","fontStyle":0},{"content":"()","offset":462,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"    ","offset":466,"color":"#EEF0F9","fontStyle":0},{"content":"# store data ...","offset":470,"color":"#EEF0F98F","fontStyle":1}],[],[{"content":"    handle.","offset":488,"color":"#EEF0F9","fontStyle":0},{"content":"commit","offset":499,"color":"#00DAEF","fontStyle":0},{"content":"()","offset":505,"color":"#EEF0F9","fontStyle":0}],[{"content":"    producer.","offset":508,"color":"#EEF0F9","fontStyle":0},{"content":"tail","offset":521,"color":"#00DAEF","fontStyle":0},{"content":"()","offset":525,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"# consumer warp","offset":529,"color":"#EEF0F98F","fontStyle":1}],[{"content":"if","offset":545,"color":"#54B9FF","fontStyle":1},{"content":" warp_idx == ","offset":547,"color":"#EEF0F9","fontStyle":0},{"content":"1","offset":560,"color":"#FFD493","fontStyle":0},{"content":":","offset":561,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"    handle = consumer.","offset":564,"color":"#EEF0F9","fontStyle":0},{"content":"wait_and_advance","offset":586,"color":"#00DAEF","fontStyle":0},{"content":"()","offset":602,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"    ","offset":606,"color":"#EEF0F9","fontStyle":0},{"content":"# do work with data ...","offset":610,"color":"#EEF0F98F","fontStyle":1}],[],[{"content":"    handle.","offset":635,"color":"#EEF0F9","fontStyle":0},{"content":"release","offset":646,"color":"#00DAEF","fontStyle":0},{"content":"()","offset":653,"color":"#EEF0F9","fontStyle":0}],[]]}]
13:["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["In the above pattern, the producer waits for data to be used by consumer before doing more work, this is a serialization point which leads to suboptimal performance. Additional parallelization can be introduced with ",["$","span",null,{"className":"font-semibold","children":"multi-stage async pipeline"}]," by juggling multiple buffers each with associated memory barrier. Some relevant APIs are"]}]
14:["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["$","$L11",null,{"code":"# advance producer write index to next buffer\nPipelineProducer.advance()\n# buffer slot index\nPipelineProducer.ImmutableResourceHandle.index\n\n# advance consumer read buffer\nPipelineConsumer.advance()\nPipelineConsumer.ImmutableResourceHandle.index\n","tokens":[[{"content":"# advance producer write index to next buffer","offset":0,"color":"#EEF0F98F","fontStyle":1}],[{"content":"PipelineProducer.","offset":46,"color":"#EEF0F9","fontStyle":0},{"content":"advance","offset":63,"color":"#00DAEF","fontStyle":0},{"content":"()","offset":70,"color":"#EEF0F9","fontStyle":0}],[{"content":"# buffer slot index","offset":73,"color":"#EEF0F98F","fontStyle":1}],[{"content":"PipelineProducer.ImmutableResourceHandle.index","offset":93,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"# advance consumer read buffer","offset":141,"color":"#EEF0F98F","fontStyle":1}],[{"content":"PipelineConsumer.","offset":172,"color":"#EEF0F9","fontStyle":0},{"content":"advance","offset":189,"color":"#00DAEF","fontStyle":0},{"content":"()","offset":196,"color":"#EEF0F9","fontStyle":0}],[{"content":"PipelineConsumer.ImmutableResourceHandle.index","offset":199,"color":"#EEF0F9","fontStyle":0}],[]]}]}]
15:["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The stage specification works as follows ",["$","$L11",null,{"code":"pipeline = cutlass.pipeline.PipelineAsync.create(\n    num_stages=stages, # multi-stage\n    producer_group=producer_group,\n    consumer_group=consumer_group,\n    ...\n)\n","tokens":[[{"content":"pipeline = cutlass.pipeline.PipelineAsync.","offset":0,"color":"#EEF0F9","fontStyle":0},{"content":"create","offset":42,"color":"#00DAEF","fontStyle":0},{"content":"(","offset":48,"color":"#EEF0F9","fontStyle":0}],[{"content":"    ","offset":50,"color":"#EEF0F9","fontStyle":0},{"content":"num_stages","offset":54,"color":"#4BF3C8","fontStyle":1},{"content":"=stages, ","offset":64,"color":"#EEF0F9","fontStyle":0},{"content":"# multi-stage","offset":73,"color":"#EEF0F98F","fontStyle":1}],[{"content":"    ","offset":87,"color":"#EEF0F9","fontStyle":0},{"content":"producer_group","offset":91,"color":"#4BF3C8","fontStyle":1},{"content":"=producer_group,","offset":105,"color":"#EEF0F9","fontStyle":0}],[{"content":"    ","offset":122,"color":"#EEF0F9","fontStyle":0},{"content":"consumer_group","offset":126,"color":"#4BF3C8","fontStyle":1},{"content":"=consumer_group,","offset":140,"color":"#EEF0F9","fontStyle":0}],[{"content":"    ","offset":157,"color":"#EEF0F9","fontStyle":0},{"content":"...","offset":161,"color":"#FFD493","fontStyle":0}],[{"content":")","offset":165,"color":"#EEF0F9","fontStyle":0}],[]]}]]}]
16:["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["$","$L11",null,{"code":"# Pipeline \nfrom cutlass.pipeline import (\n    PipelineProducer, PipelineConsumer,\n    PipelineAsync,   # async producer-consumer pattern\n    PipelineTmaAsync,\n    PipelineTmaUmma,\n    PipelineAsyncUmma,\n    PipelineUmmaAsync,\n)\n\n# useful util\nfrom cutlass.utils import (\n    SmemAllocator,\n    TmemAllocator,\n    HardwareInfo,\n    print_latex,\n    print_latex_tv\n    # also things like cutlass.utils.sm100.make_smem_layout_a\n)\n","tokens":[[{"content":"# Pipeline ","offset":0,"color":"#EEF0F98F","fontStyle":1}],[{"content":"from","offset":12,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.pipeline ","offset":16,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":34,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":40,"color":"#EEF0F9","fontStyle":0}],[{"content":"    PipelineProducer, PipelineConsumer,","offset":43,"color":"#EEF0F9","fontStyle":0}],[{"content":"    PipelineAsync,   ","offset":83,"color":"#EEF0F9","fontStyle":0},{"content":"# async producer-consumer pattern","offset":104,"color":"#EEF0F98F","fontStyle":1}],[{"content":"    PipelineTmaAsync,","offset":138,"color":"#EEF0F9","fontStyle":0}],[{"content":"    PipelineTmaUmma,","offset":160,"color":"#EEF0F9","fontStyle":0}],[{"content":"    PipelineAsyncUmma,","offset":181,"color":"#EEF0F9","fontStyle":0}],[{"content":"    PipelineUmmaAsync,","offset":204,"color":"#EEF0F9","fontStyle":0}],[{"content":")","offset":227,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"# useful util","offset":230,"color":"#EEF0F98F","fontStyle":1}],[{"content":"from","offset":244,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.utils ","offset":248,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":263,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":269,"color":"#EEF0F9","fontStyle":0}],[{"content":"    SmemAllocator,","offset":272,"color":"#EEF0F9","fontStyle":0}],[{"content":"    TmemAllocator,","offset":291,"color":"#EEF0F9","fontStyle":0}],[{"content":"    HardwareInfo,","offset":310,"color":"#EEF0F9","fontStyle":0}],[{"content":"    print_latex,","offset":328,"color":"#EEF0F9","fontStyle":0}],[{"content":"    print_latex_tv","offset":345,"color":"#EEF0F9","fontStyle":0}],[{"content":"    ","offset":364,"color":"#EEF0F9","fontStyle":0},{"content":"# also things like cutlass.utils.sm100.make_smem_layout_a","offset":368,"color":"#EEF0F98F","fontStyle":1}],[{"content":")","offset":426,"color":"#EEF0F9","fontStyle":0}],[]]}]}]
17:["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["$","$L11",null,{"code":"from cutlass.cute import (\n    make_layout,\n    zipped_product,\n    Swizzle,\n    gemm,\n    depth, rank, size, \n    struct, # decorator\n    printf, ceil_div # etc \n)\n\nfrom cutlass.cute.arch import (\n    thread_idx,\n    block_idx, block_dim,\n    warp.idx # etc\n)\n\nfrom cutlass.cute.runtime import (\n    from_dlpack,\n    make_ptr\n)\n","tokens":[[{"content":"from","offset":0,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.cute ","offset":4,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":18,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":24,"color":"#EEF0F9","fontStyle":0}],[{"content":"    make_layout,","offset":27,"color":"#EEF0F9","fontStyle":0}],[{"content":"    zipped_product,","offset":44,"color":"#EEF0F9","fontStyle":0}],[{"content":"    Swizzle,","offset":64,"color":"#EEF0F9","fontStyle":0}],[{"content":"    gemm,","offset":77,"color":"#EEF0F9","fontStyle":0}],[{"content":"    depth, rank, size, ","offset":87,"color":"#EEF0F9","fontStyle":0}],[{"content":"    struct, ","offset":111,"color":"#EEF0F9","fontStyle":0},{"content":"# decorator","offset":123,"color":"#EEF0F98F","fontStyle":1}],[{"content":"    printf, ceil_div ","offset":135,"color":"#EEF0F9","fontStyle":0},{"content":"# etc ","offset":156,"color":"#EEF0F98F","fontStyle":1}],[{"content":")","offset":163,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"from","offset":166,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.cute.arch ","offset":170,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":189,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":195,"color":"#EEF0F9","fontStyle":0}],[{"content":"    thread_idx,","offset":198,"color":"#EEF0F9","fontStyle":0}],[{"content":"    block_idx, block_dim,","offset":214,"color":"#EEF0F9","fontStyle":0}],[{"content":"    warp.idx ","offset":240,"color":"#EEF0F9","fontStyle":0},{"content":"# etc","offset":253,"color":"#EEF0F98F","fontStyle":1}],[{"content":")","offset":259,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"from","offset":262,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.cute.runtime ","offset":266,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":288,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":294,"color":"#EEF0F9","fontStyle":0}],[{"content":"    from_dlpack,","offset":297,"color":"#EEF0F9","fontStyle":0}],[{"content":"    make_ptr","offset":314,"color":"#EEF0F9","fontStyle":0}],[{"content":")","offset":327,"color":"#EEF0F9","fontStyle":0}],[]]}]}]
1b:null
