1:"$Sreact.fragment"
2:I[1565,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/e5d291186d0cbb54.js","/_next/static/chunks/bc166ea53d390db7.js"],"default"]
3:I[96045,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/e5d291186d0cbb54.js","/_next/static/chunks/bc166ea53d390db7.js"],"default"]
16:I[56691,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/e5d291186d0cbb54.js","/_next/static/chunks/bc166ea53d390db7.js"],"default"]
17:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"OutletBoundary"]
18:"$Sreact.suspense"
0:{"buildId":"-PDM1eQFKJJmTk1GhsL6I","rsc":["$","$1","c",{"children":[["$","div",null,{"className":"flex flex-col justify-center min-h-screen","children":[["$","div",null,{"className":"flex flex-row justify-center pt-15","children":["$","$L2",null,{}]}],["$","div",null,{"className":"grow flex justify-center w-full pt-10 md:pt-15 px-3","children":["$","div",null,{"className":"flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl","children":[["$","div",null,{"className":"text-3xl font-bold flex justify-center","children":"Programming Blackwell Tensor Core with Cutlass and CuTe DSL"}],["$","div",null,{"className":"text-sm flex justify-center pt-5","children":null}],["$","div",null,{"className":"","children":[["$","div","0",{"children":["$undefined",["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The CuTe ",["$","span",null,{"className":"italic","children":"domain specific language"}]," (DSL) provides a Python interface that enables efficient matrix muliplication GPU kernels implementation leveraging the tile based abstraction. The DSL offers Blackwell specific support in the ",["$","span",null,{"className":"font-semibold","children":"cutlass.cute.nvgpu.tcgen05"}]," module. Since our discussions about this module will largely be hardware aware, let us preface the discussion by an excusrion of Blackwell tensor core programming at the more fundamental ",["$","span",null,{"className":"italic","children":"parallel thread excution"}]," (PTX) virtual instruction set level."]}]}]]}],["$","div","1",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Tensor Memory"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["A cooperative thread array (CTA) is a set of threads that excute a kernel concurrently or in parallel. A ",["$","span",null,{"className":"font-semibold","children":"warp"}]," of a CTA is a maximum subset of the set of threads in the CTA that can execute the same instruction simultaneously. In common hardware, a warp is a ",["$","$L3",null,{"latex":"32","className":"mt-10"}]," thread subset of the CTA. Threads in a CTA can be organized into a block of 1D, 2D, or 3D block of dimensions ",["$","$L3",null,{"latex":"X\\times Y\\times Z","className":"mt-10"}]," and each thread indexed by a tuple ",["$","$L3",null,{"latex":"(i, j, k)","className":"mt-10"}],". If ",["$","$L3",null,{"latex":"Y=Z=1","className":"mt-10"}]," then the CTA is a 1D block and so forth. One can enumerate the threads in an ",["$","$L3",null,{"latex":"X\\times Y\\times Z","className":"mt-10"}]," CTA by ",["$","$L3",null,{"latex":"(i,j,k)\\mapsto i + jX + kXY","displayMode":true,"className":"mt-10"}],"We can enumerate all threads in a CTA and every four consecutive warps where the begining thread has thread ID ",["$","$L3",null,{"latex":"I\\equiv 0 \\mod 128","className":"mt-10"}]," is called a is called a ",["$","span",null,{"className":"font-semibold","children":"warp group"}],". The warps in a warp group are associated with a ",["$","span",null,{"className":"font-semibold","children":"warp rank"}]," from ",["$","$L3",null,{"latex":"0\\sim 3","className":"mt-10"}],"."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["For each cooperative thread array (CTA), the structure of tensor memory is a ",["$","$L3",null,{"latex":"128 \\times 512","className":"mt-10"}]," matrix of ",["$","$L3",null,{"latex":"32","className":"mt-10"}]," bit cells. Each of the ",["$","$L3",null,{"latex":"128","className":"mt-10"}]," rows in tensor memory is also referred to as a ",["$","span",null,{"className":"font-semibold","children":"lane"}],", which is ",["$","$L3",null,{"latex":"2","className":"mt-10"}]," KB in size. Thus the lanes are in bijective correspondence with the threads in a warp group. In fact the lanes are divided between the warps, so that warp rank ","$L4"," can only access lanes ","$L5"," using ","$L6"," load, ","$L7"," store,     ","$L8"," copy tcgen05 instructions which are issued on inputs of lane ","$L9"," size. Each warp can access all columns within is accessible lane. Tensor memory allocation and deallocation use the ","$La"," and ","$Lb"," instructions and are allocated in units of ","$Lc"," columns, and the number of allocated columns must be a power of ","$Ld",". Thus when a column is allocated, each of its ","$Le"," lanes are allocated."]}]]}]]}],"$Lf","$L10"]}]]}]}],"$L11"]}],["$L12","$L13","$L14"],"$L15"]}],"loading":null,"isPartial":false}
4:["$","$L3",null,{"latex":"0","className":"mt-10"}]
5:["$","$L3",null,{"latex":"0-31","className":"mt-10"}]
6:["$","span",null,{"className":"font-semibold","children":"ld"}]
7:["$","span",null,{"className":"font-semibold","children":"st"}]
8:["$","span",null,{"className":"font-semibold","children":"cp"}]
9:["$","$L3",null,{"latex":"\\times","className":"mt-10"}]
a:["$","span",null,{"className":"font-semibold","children":"alloc"}]
b:["$","span",null,{"className":"font-semibold","children":"dealloc"}]
c:["$","$L3",null,{"latex":"32","className":"mt-10"}]
d:["$","$L3",null,{"latex":"2","className":"mt-10"}]
e:["$","$L3",null,{"latex":"128","className":"mt-10"}]
f:["$","div","2",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Matrix Multiplication Instructions"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":[["$","span",null,{"className":"font-semibold","children":"Matrix multiply and accumulate"}]," (MMA) are excuted by ",["$","span",null,{"className":"font-semibold","children":"mma"}]," instruction which depend on a number launch configurations, such as the data type of input pair of matrices and their shapes, the data type of accumlation or output matrix. Support for warp-specialization, whether one or two CTAs cooperate, and whether the matmul is dense or sparse. For instance 2CTA FP16 dense MMA can have largest shape ",["$","$L3",null,{"latex":"M\\times N\\times K=256 \\times 256 \\times 16","className":"mt-10"}]," while NVFP4 dense MMA has largest shape ",["$","$L3",null,{"latex":"M\\times N\\times K=256\\times 256 \\times 64 ","className":"mt-10"}]]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["MMA configuration are stored on register as a 32 bit ",["$","span",null,{"className":"font-semibold","children":"instruction descriptor"}],". For example bits 13 to 16 describe whether the input matrices needs to be ",["$","span",null,{"className":"font-semibold","children":"transposed"}]," or ",["$","span",null,{"className":"font-semibold","children":"negated"}],". Other data store includes sparsity, input output data type."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["MAA supports ",["$","span",null,{"className":"font-semibold","children":"block scaled"}]," matrix multiplication for data types such as ",["$","span",null,{"className":"font-semibold","children":"mxf4"}]," and ",["$","span",null,{"className":"font-semibold","children":"nvf4"}],"."]}]]}]]}]
10:["$","div","3",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Granularity and Synchronization"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["TensorCore Gen 5 Instructions such as ",["$","span",null,{"className":"font-semibold","children":"mma"}],", ",["$","span",null,{"className":"font-semibold","children":"cp"}]," can be issued by a single thread within a CTA or CTA-Pair. Instructions such as ",["$","span",null,{"className":"font-semibold","children":"alloc"}]," and ",["$","span",null,{"className":"font-semibold","children":"dealloc"}]," can be issued from a warp of a pair of warps one in each CTA-pair. Each warp can access a quarter of the tensor memory via ",["$","span",null,{"className":"font-semibold","children":"ld"}]," and ",["$","span",null,{"className":"font-semibold","children":"st"}]," so a warp group is required for complete tensor memory access. These characterize the instruction issuing granularity."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Instructions can be synchronous (alloc/dealloc, fence) or asynchronous (cp, mma, ld, st). Special pairs of asyncrhonous instructions, called ",["$","span",null,{"className":"font-semibold","children":"pipelined instructions"}]," are guarenteed to excute in the order they are issued, an example is ",["$","span",null,{"className":"font-semibold","children":"tcgen05.copy.cta_group::N"}]," and ",["$","span",null,{"className":"font-semibold","children":"tcgen05.mma.cta_group::N"}]," for the same N."]}]]}]]}]
11:["$","$L16",null,{}]
12:["$","script","script-0",{"src":"/_next/static/chunks/988ea292de4c4c73.js","async":true}]
13:["$","script","script-1",{"src":"/_next/static/chunks/e5d291186d0cbb54.js","async":true}]
14:["$","script","script-2",{"src":"/_next/static/chunks/bc166ea53d390db7.js","async":true}]
15:["$","$L17",null,{"children":["$","$18",null,{"name":"Next.MetadataOutlet","children":"$@19"}]}]
19:null
