1:"$Sreact.fragment"
2:I[48523,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"default"]
3:I[82281,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"default"]
4:I[1565,["/_next/static/chunks/8850d1d03d80978f.js","/_next/static/chunks/9b725c66512530e6.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
5:I[96045,["/_next/static/chunks/8850d1d03d80978f.js","/_next/static/chunks/9b725c66512530e6.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
3b:I[75067,[],"default"]
:HL["/_next/static/chunks/bbc7fa474c9f0ed8.css","style"]
:HL["/_next/static/chunks/4057cc9dbcc744c0.css","style"]
:HL["/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
0:{"P":null,"b":"WAHbF-IuoHa9ll8cBu51j","c":["","ai","blackwell-gemm-intro"],"q":"","i":false,"f":[[["",{"children":["ai",{"children":["blackwell-gemm-intro",{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/bbc7fa474c9f0ed8.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/chunks/4057cc9dbcc744c0.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased","children":["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[["$","div",null,{"className":"flex flex-col justify-center min-h-screen","children":[["$","div",null,{"className":"flex flex-row justify-center pt-15","children":["$","$L4",null,{}]}],["$","div",null,{"className":"grow flex justify-center w-full pt-10 md:pt-15 px-3","children":["$","div",null,{"className":"flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl","children":[["$","div",null,{"className":"text-3xl font-bold flex justify-center","children":"Intro Blackwell GEMM"}],["$","div",null,{"className":"text-sm flex justify-center pt-5","children":null}],["$","div",null,{"className":"","children":[["$","div","0",{"children":["$undefined",["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Let ",["$","$L5",null,{"latex":"A","className":"mt-10"}]," be a ",["$","$L5",null,{"latex":"m\\times k","className":"mt-10"}]," matrix and ",["$","$L5",null,{"latex":"B","className":"mt-10"}]," be a ",["$","$L5",null,{"latex":"k\\times n","className":"mt-10"}]," matrix. We are interested in computing the matrix product ",["$","$L5",null,{"latex":"C=AB","className":"mt-10"}],". Now hardware instruction does not compute arbitrary shaped matrix multiplication at once, so we are really interested in breaking the product into smaller chunks."]}]}]]}],["$","div","1",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Block Matrices"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["We can partition ",["$","$L5",null,{"latex":"A","className":"mt-10"}]," into ",["$","span",null,{"className":"font-semibold","children":"block matrices"}]," along its column direction ",["$","$L5",null,{"latex":"A = \\begin{bmatrix}A_{11} \\\\ A_{21} \\end{bmatrix}","displayMode":true,"className":"mt-10"}]," where ","$L6"," has dimension ","$L7"," and ","$L8"," has dimension ","$L9"," with ","$La",". Likewise we can partition ","$Lb"," along its row direction ","$Lc"," for a block matrix ","$Ld"," with dimension ","$Le"," and a block matrix ","$Lf"," with dimension ","$L10"," such that ","$L11",". Thus in general a matrix can be partitioned in both directions, and as for the product ","$L12"," we can partition each matrix in both direction a ","$L13"," number of times  ","$L14"," so that ","$L15"," is partitioned into a ","$L16"," block matrix ","$L17"," becomes a ","$L18"," block matrix, and ","$L19"," is a ","$L1a"," matrix. The shapes of these matrices are as follows ","$L1b"," such that ","$L1c","   ","$L1d","   ","$L1e"," The matrix product, in terms of block matrices, is ","$L1f"," For folks working in numerical computing, the block matrices are called ","$L20",", and say that ","$L21"," has ","$L22"," ","$L23",". For convenience one usually chooses ","$L24"," ","$L25"," ","$L26"," and says that the ","$L27"," is ","$L28",". When realizing the block matrix multiplication on hardware, each block of ","$L29"," is computed as a summation, which in numerical methods lingo is called a ","$L2a"," where each iteration which functions to ","$L2b"," the ","$L2c"," to a ","$L2d",". Each accumulation step is called a ","$L2e","."]}]}]]}],"$L2f","$L30","$L31","$L32","$L33","$L34"]}]]}]}],"$L35"]}],["$L36","$L37","$L38"],"$L39"]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],"$L3a",false]],"m":"$undefined","G":["$3b",[]],"S":true}
3f:I[56691,["/_next/static/chunks/8850d1d03d80978f.js","/_next/static/chunks/9b725c66512530e6.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
40:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"OutletBoundary"]
41:"$Sreact.suspense"
43:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"ViewportBoundary"]
45:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"MetadataBoundary"]
6:["$","$L5",null,{"latex":"A_{11}","className":"mt-10"}]
7:["$","$L5",null,{"latex":"m_1\\times k","className":"mt-10"}]
8:["$","$L5",null,{"latex":"A_{21}","className":"mt-10"}]
9:["$","$L5",null,{"latex":"m_2\\times k","className":"mt-10"}]
a:["$","$L5",null,{"latex":"m_1+m_2=m","className":"mt-10"}]
b:["$","$L5",null,{"latex":"A","className":"mt-10"}]
c:["$","$L5",null,{"latex":"A = \\begin{bmatrix}A_{11} & A_{12} \\end{bmatrix}","displayMode":true,"className":"mt-10"}]
d:["$","$L5",null,{"latex":"A_{11}","className":"mt-10"}]
e:["$","$L5",null,{"latex":"m\\times k_1","className":"mt-10"}]
f:["$","$L5",null,{"latex":"A_{12}","className":"mt-10"}]
10:["$","$L5",null,{"latex":"m\\times k_2","className":"mt-10"}]
11:["$","$L5",null,{"latex":"k_1+k_2=k","className":"mt-10"}]
12:["$","$L5",null,{"latex":"AB","className":"mt-10"}]
13:["$","span",null,{"className":"italic","children":"suitable"}]
14:["$","$L5",null,{"latex":"AB = \\begin{bmatrix}A_{11} & \\dots & A_{1Q} \\\\ \\vdots & & \\vdots \\\\ A_{P1} & \\dots & A_{PQ} \\end{bmatrix}\\begin{bmatrix}B_{11} & \\dots & B_{1R} \\\\ \\vdots & & \\vdots \\\\ B_{Q1} & \\dots & B_{QR} \\end{bmatrix}","displayMode":true,"className":"mt-10"}]
15:["$","$L5",null,{"latex":"A","className":"mt-10"}]
16:["$","$L5",null,{"latex":"P\\times Q","className":"mt-10"}]
17:["$","$L5",null,{"latex":"B","className":"mt-10"}]
18:["$","$L5",null,{"latex":"Q\\times R","className":"mt-10"}]
19:["$","$L5",null,{"latex":"C","className":"mt-10"}]
1a:["$","$L5",null,{"latex":"P\\times R","className":"mt-10"}]
1b:["$","$L5",null,{"latex":"\\begin{bmatrix}m_{1}\\times k_{1} & \\dots & m_{1}\\times k_{Q} \\\\ \\vdots & & \\vdots \\\\ m_{P}\\times k_{1} & \\dots & m_{P}\\times k_{Q} \\end{bmatrix}\\begin{bmatrix}k_{1}\\times n_{1} & \\dots & k_{1}\\times n_{R} \\\\ \\vdots & & \\vdots \\\\ k_{Q}\\times n_{1} & \\dots & k_{Q}\\times n_{R} \\end{bmatrix} = \\begin{bmatrix}m_{1}\\times n_{1} & \\dots & m_{1}\\times n_{R} \\\\ \\vdots & & \\vdots \\\\ m_{P}\\times n_{1} & \\dots & m_{P}\\times n_{R} \\end{bmatrix}","displayMode":true,"className":"mt-10"}]
1c:["$","$L5",null,{"latex":"m_1+\\dots+ m_P = m","displayMode":true,"className":"mt-10"}]
1d:["$","$L5",null,{"latex":"k_1+\\dots +k_Q = k","displayMode":true,"className":"mt-10"}]
1e:["$","$L5",null,{"latex":"n_1+\\dots +n_R = n","displayMode":true,"className":"mt-10"}]
1f:["$","$L5",null,{"latex":" C = \\begin{bmatrix}\\displaystyle\\sum_{1\\leq j \\leq Q} A_{1j}B_{j1} & \\dots & \\displaystyle\\sum_{1\\leq j \\leq Q}A_{1j}B_{jR} \\\\ \\vdots & & \\vdots \\\\ \\displaystyle\\sum_{1\\leq j \\leq Q}A_{Pj}B_{j1} & \\dots & \\displaystyle\\sum_{1\\leq j \\leq Q}A_{Pj}B_{jR} \\end{bmatrix}","displayMode":true,"className":"mt-10"}]
20:["$","span",null,{"className":"font-semibold","children":"matrix tiles"}]
21:["$","$L5",null,{"latex":"C=AB","className":"mt-10"}]
22:["$","span",null,{"className":"font-semibold","children":"problem size"}]
23:["$","$L5",null,{"latex":"mnk","className":"mt-10"}]
24:["$","$L5",null,{"latex":"m_1=\\dots=m_p = \\mu","displayMode":true,"className":"mt-10"}]
25:["$","$L5",null,{"latex":"n_1=\\dots=n_R = \\nu","displayMode":true,"className":"mt-10"}]
26:["$","$L5",null,{"latex":"k_1=\\dots=k_Q = \\kappa","displayMode":true,"className":"mt-10"}]
27:["$","span",null,{"className":"font-semibold","children":"tiler size"}]
28:["$","$L5",null,{"latex":"\\mu\\nu\\kappa","className":"mt-10"}]
29:["$","$L5",null,{"latex":"C","className":"mt-10"}]
2a:["$","span",null,{"className":"font-semibold","children":"main loop"}]
2b:["$","span",null,{"className":"italic","children":"accumulate"}]
2c:["$","span",null,{"className":"italic","children":"partial products"}]
2d:["$","span",null,{"className":"italic","children":"buffer"}]
2e:["$","span",null,{"className":"italic","children":"stage"}]
2f:["$","div","2",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Cutlass and CuTe"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"The philosophy of computing matrix product on a GPU is to apply block multiplication to leverage concurrency as fully as possible with respect to the parallel units of execution supported by hardware such as the CTA, the warp, the MMA instruction, and partial products are accumulated at different memory state spaces like tensor memory, shared memory, and register memory. Eventaully, results are stored back to global memory."}]}]]}]
30:["$","div","3",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Example"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Consider a matrix multiplication with problem size ",["$","$L5",null,{"latex":"(m,n,k) = (8192, 8129, 8192)","className":"mt-10"}],". For the CTA level of concurrency we consider a block matrix multiplication with tile sizes ",["$","$L5",null,{"latex":"(\\mu, \\nu, \\kappa)=(128, 256, 64)","className":"mt-10"}],". This corresponds ",["$","$L5",null,{"latex":"(P,R, Q)= (64, 32, 128)","className":"mt-10"}]," which means ",["$","$L5",null,{"latex":"C","className":"mt-10"}]," is divided into ",["$","$L5",null,{"latex":"P\\times R = 64\\times 32","className":"mt-10"}]," blocks, each computed a sum over ",["$","$L5",null,{"latex":"Q=128","className":"mt-10"}]," partial products."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["To schedule this onto the GPU, we launch ",["$","$L5",null,{"latex":"P\\times R = 64\\times 32","className":"mt-10"}]," thread blocks. Each thread block initiates a tensor memory of size ",["$","$L5",null,{"latex":"\\mu \\times \\nu=128\\times 256,","className":"mt-10"}]," to accumulate the partial products of this same. For each partial product we load a block matrix of ",["$","$L5",null,{"latex":"A","className":"mt-10"}]," of size ",["$","$L5",null,{"latex":"128\\times 64","className":"mt-10"}]," and a block matrix of ",["$","$L5",null,{"latex":"B","className":"mt-10"}]," of size ",["$","$L5",null,{"latex":"256\\times 64","className":"mt-10"}]," into the shared memory (smem)."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Suppose each MMA matrix multiply instruction  can deal with ",["$","$L5",null,{"latex":"mnk","className":"mt-10"}]," size of ",["$","$L5",null,{"latex":"128\\times 256\\times 16 ","className":"mt-10"}],", then we need to partition the smem block of ",["$","$L5",null,{"latex":"A","className":"mt-10"}]," and ",["$","$L5",null,{"latex":"B","className":"mt-10"}]," into four equal parts along the reduction dimension so to match the size of the MMA instruction."]}]]}]]}]
31:["$","div","4",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Specifying the Problem"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["To compute the matrix product ",["$","$L5",null,{"latex":"C=AB","className":"mt-10"}]," we first need to fix the ",["$","span",null,{"className":"font-semibold","children":"IO data type"}]," for each of the matrices. We then need to specify the ",["$","span",null,{"className":"font-semibold","children":"accumulator data type"}],", this means setting the datatype for summation (math terminology) or reduction (numerics lingo) both of which refer to the operation ",["$","$L5",null,{"latex":"c_{ij}=\\sum_k{a_{ik}b_{kj}}","displayMode":true,"className":"mt-10"}]," where ",["$","$L5",null,{"latex":"a_{ik}, b_{kj}, c_{ij}","className":"mt-10"}]," are entries, or blocked matrices in ",["$","$L5",null,{"latex":"A, B, C","className":"mt-10"}],". A common accumulator dtype is FP32 to avoid overflow in the summation process. When the accumulator dtype differs from ",["$","$L5",null,{"latex":"C","className":"mt-10"}],"'s IO type then conversion is required. To produce the block matrix data we use the tensor memory accelerator to load some number of blocks to shared memory, this number is determined by the size of the smem and is sometimes called the number of ",["$","span",null,{"className":"font-semibold","children":"TMA pipeline stages"}],". The block matrix product is accumulated on tensor memory, and there are some number of ",["$","span",null,{"className":"font-semibold","children":"accumulation stages"}],", which is one in our example."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Having specified the data and IO, we turn to compute specifications. We provide the shape for the matrix multiplication instruction for 5th generation TensorCore and specify the block matrix size (aka tiler size in cutlass world) that either one or two CTAs will process at once (the number of CTAs is called the ",["$","span",null,{"className":"font-semibold","children":"issue granularity"}]," of the instruction). Further we decide the number of threads in each CTA: to ensure full SM usage we use at least the number of threads in a warp group (128)."]}]]}]]}]
32:["$","div","5",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Host Code"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"We now describe the host code, which runs on the CPU, and is used to lanuch the GEMM kernel on the GPU."}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"The following specifies the matrix multiply and accumulate instruction to use"}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"$L3c"}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["To specify shared memory layout for the matrix ",["$","$L5",null,{"latex":"A","className":"mt-10"}],". This layout has at least ",["$","$L5",null,{"latex":"3","className":"mt-10"}]," modes, where the first mode is the shape of the block matrix for a single MMA instruction. The next two modes describe how many times the MMA is repeated traversing the row and columns of ",["$","$L5",null,{"latex":"A","className":"mt-10"}],". This layout is ",["$","span",null,{"className":"font-semibold","children":"swizzled"}]," to avoid smem bank conflict."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"$L3d"}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Tensor memory accelerator to load ",["$","$L5",null,{"latex":"A","className":"mt-10"}]," from global to shared memory state space is configured as follows ","$L3e"]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"Then the kernel is lanuched, specifying the thread block and grid dimensions as appropriate."}]]}]]}]
33:["$","div","6",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"GEMM Stages"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"Before delving into device code let us give a conceptual overview of what happens during the GEMM computation."}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The ",["$","span",null,{"className":"font-semibold","children":"prologue"}]," is what happens before the matrix multiply instruction occurs. The most important function is to load the data via TMA. This is done by performing the necessary indexing: block, thread, warp ID, which is useful for locating which block matrix the MMA will calculate, which data it should load, the TMA and MMA tensor view. The relevant shared and tensor memory needs to be allocated. Setting up pipelines PipelineTmaUmma for consumer-producer between data loading and MMA, PipelineUmmaAsync for signally accumulation completion."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The ",["$","span",null,{"className":"font-semibold","children":"mainloop"}]," iteratively fetches data, computes MMA and accumulates across the reduction dimension."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The ",["$","span",null,{"className":"font-semibold","children":"epilogue"}]," loads data from tensor memory to register, fuse operation on ",["$","$L5",null,{"latex":"C","className":"mt-10"}]," matrix and performs necessary data conversion. This stages deallocates tensor memory and stores results back to global memory."]}]]}]]}]
34:["$","div","7",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Device Code"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":"$undefined"}]]}]
35:["$","$L3f",null,{}]
36:["$","script","script-0",{"src":"/_next/static/chunks/8850d1d03d80978f.js","async":true,"nonce":"$undefined"}]
37:["$","script","script-1",{"src":"/_next/static/chunks/9b725c66512530e6.js","async":true,"nonce":"$undefined"}]
38:["$","script","script-2",{"src":"/_next/static/chunks/5ad9eb95768fc0a4.js","async":true,"nonce":"$undefined"}]
39:["$","$L40",null,{"children":["$","$41",null,{"name":"Next.MetadataOutlet","children":"$@42"}]}]
3a:["$","$1","h",{"children":[null,["$","$L43",null,{"children":"$L44"}],["$","div",null,{"hidden":true,"children":["$","$L45",null,{"children":["$","$41",null,{"name":"Next.Metadata","children":"$L46"}]}]}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]
44:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
47:I[87718,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"IconMark"]
42:null
46:[["$","title","0",{"children":"Claire Zhao Blog"}],["$","meta","1",{"name":"description","content":"Claire Zhao's Blog on AI and Math"}],["$","link","2",{"rel":"icon","href":"/favicon.ico?favicon.e10ff5c7.ico","sizes":"1176x1032","type":"image/x-icon"}],["$","$L47","3",{}]]
48:I[66832,["/_next/static/chunks/8850d1d03d80978f.js","/_next/static/chunks/9b725c66512530e6.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
3c:["$","$L48",null,{"code":"op = tcgen05.MmaF16BF16Op(\n    io_dtype,\n    acc_dtype,\n    mma_inst_shape_mnk,\n    tcgen05.CtaGroup.ONE,   # issue granularity\n    tcgen05.OperandSource.SMEM,\n    tcgen05.OperandMajorMode.K, # matrix A reduction dimension\n    tcgen05.OperandMajorMode.K, # matrix B reduction dimension\n)\ntiled_mma = cute.make_tiled_mma(op)\n","tokens":[[{"content":"op = tcgen05.","offset":0,"color":"#EEF0F9","fontStyle":0},{"content":"MmaF16BF16Op","offset":13,"color":"#00DAEF","fontStyle":0},{"content":"(","offset":25,"color":"#EEF0F9","fontStyle":0}],[{"content":"    io_dtype,","offset":27,"color":"#EEF0F9","fontStyle":0}],[{"content":"    acc_dtype,","offset":41,"color":"#EEF0F9","fontStyle":0}],[{"content":"    mma_inst_shape_mnk,","offset":56,"color":"#EEF0F9","fontStyle":0}],[{"content":"    tcgen05.CtaGroup.","offset":80,"color":"#EEF0F9","fontStyle":0},{"content":"ONE","offset":101,"color":"#FFD493","fontStyle":0},{"content":",   ","offset":104,"color":"#EEF0F9","fontStyle":0},{"content":"# issue granularity","offset":108,"color":"#EEF0F98F","fontStyle":1}],[{"content":"    tcgen05.OperandSource.","offset":128,"color":"#EEF0F9","fontStyle":0},{"content":"SMEM","offset":154,"color":"#FFD493","fontStyle":0},{"content":",","offset":158,"color":"#EEF0F9","fontStyle":0}],[{"content":"    tcgen05.OperandMajorMode.K, ","offset":160,"color":"#EEF0F9","fontStyle":0},{"content":"# matrix A reduction dimension","offset":192,"color":"#EEF0F98F","fontStyle":1}],[{"content":"    tcgen05.OperandMajorMode.K, ","offset":223,"color":"#EEF0F9","fontStyle":0},{"content":"# matrix B reduction dimension","offset":255,"color":"#EEF0F98F","fontStyle":1}],[{"content":")","offset":286,"color":"#EEF0F9","fontStyle":0}],[{"content":"tiled_mma = cute.","offset":288,"color":"#EEF0F9","fontStyle":0},{"content":"make_tiled_mma","offset":305,"color":"#00DAEF","fontStyle":0},{"content":"(op)","offset":319,"color":"#EEF0F9","fontStyle":0}],[]]}]
3d:["$","$L48",null,{"code":"a_smem_layout = cutlass.utils.blackwell_helpers.make_smem_layout_a(\n    tiled_mma,\n    mma_tiler_mnk,\n    a.element_type,\n    ab_stages,\n)\na_smem_layout_one_stage = cute.select(a_smem_layout, mode=[0, 1, 2])\n","tokens":[[{"content":"a_smem_layout = cutlass.utils.blackwell_helpers.","offset":0,"color":"#EEF0F9","fontStyle":0},{"content":"make_smem_layout_a","offset":48,"color":"#00DAEF","fontStyle":0},{"content":"(","offset":66,"color":"#EEF0F9","fontStyle":0}],[{"content":"    tiled_mma,","offset":68,"color":"#EEF0F9","fontStyle":0}],[{"content":"    mma_tiler_mnk,","offset":83,"color":"#EEF0F9","fontStyle":0}],[{"content":"    a.element_type,","offset":102,"color":"#EEF0F9","fontStyle":0}],[{"content":"    ab_stages,","offset":122,"color":"#EEF0F9","fontStyle":0}],[{"content":")","offset":137,"color":"#EEF0F9","fontStyle":0}],[{"content":"a_smem_layout_one_stage = cute.","offset":139,"color":"#EEF0F9","fontStyle":0},{"content":"select","offset":170,"color":"#00DAEF","fontStyle":0},{"content":"(a_smem_layout, ","offset":176,"color":"#EEF0F9","fontStyle":0},{"content":"mode","offset":192,"color":"#4BF3C8","fontStyle":1},{"content":"=[","offset":196,"color":"#EEF0F9","fontStyle":0},{"content":"0","offset":198,"color":"#FFD493","fontStyle":0},{"content":", ","offset":199,"color":"#EEF0F9","fontStyle":0},{"content":"1","offset":201,"color":"#FFD493","fontStyle":0},{"content":", ","offset":202,"color":"#EEF0F9","fontStyle":0},{"content":"2","offset":204,"color":"#FFD493","fontStyle":0},{"content":"])","offset":205,"color":"#EEF0F9","fontStyle":0}],[]]}]
3e:["$","$L48",null,{"code":"op = cute.nvgpu.cpasync.CopyBulkTensorTileG2SOp(tcgen05.CtaGroup.ONE)\na_tma_atom, a_tma_tensor = cute.nvgpu.make_tiled_tma_atom_A(\n    op,\n    a,  # the matrix A\n    a_smem_layout_one_stage,\n    mma_tiler_mnk,\n    tiled_mma,\n)\n","tokens":[[{"content":"op = cute.nvgpu.cpasync.","offset":0,"color":"#EEF0F9","fontStyle":0},{"content":"CopyBulkTensorTileG2SOp","offset":24,"color":"#00DAEF","fontStyle":0},{"content":"(tcgen05.CtaGroup.","offset":47,"color":"#EEF0F9","fontStyle":0},{"content":"ONE","offset":65,"color":"#FFD493","fontStyle":0},{"content":")","offset":68,"color":"#EEF0F9","fontStyle":0}],[{"content":"a_tma_atom, a_tma_tensor = cute.nvgpu.","offset":70,"color":"#EEF0F9","fontStyle":0},{"content":"make_tiled_tma_atom_A","offset":108,"color":"#00DAEF","fontStyle":0},{"content":"(","offset":129,"color":"#EEF0F9","fontStyle":0}],[{"content":"    op,","offset":131,"color":"#EEF0F9","fontStyle":0}],[{"content":"    a,  ","offset":139,"color":"#EEF0F9","fontStyle":0},{"content":"# the matrix A","offset":147,"color":"#EEF0F98F","fontStyle":1}],[{"content":"    a_smem_layout_one_stage,","offset":162,"color":"#EEF0F9","fontStyle":0}],[{"content":"    mma_tiler_mnk,","offset":191,"color":"#EEF0F9","fontStyle":0}],[{"content":"    tiled_mma,","offset":210,"color":"#EEF0F9","fontStyle":0}],[{"content":")","offset":225,"color":"#EEF0F9","fontStyle":0}],[]]}]
