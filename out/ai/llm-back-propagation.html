<!DOCTYPE html><!--_q_PAJOHlzGsitc36faMO--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/chunks/e23ef66a1dfd4af0.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/4057cc9dbcc744c0.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/be439e04947a2cba.js"/><script src="/_next/static/chunks/922a6be1c048f7e7.js" async=""></script><script src="/_next/static/chunks/748df87cf5306c08.js" async=""></script><script src="/_next/static/chunks/turbopack-0af7db035ef23dbc.js" async=""></script><script src="/_next/static/chunks/988ea292de4c4c73.js" async=""></script><script src="/_next/static/chunks/7a959126392b4956.js" async=""></script><script src="/_next/static/chunks/e5d291186d0cbb54.js" async=""></script><script src="/_next/static/chunks/bc166ea53d390db7.js" async=""></script><meta name="next-size-adjust" content=""/><title>Claire Zhao Blog</title><meta name="description" content="Claire Zhao&#x27;s Blog on AI and Math"/><link rel="icon" href="/favicon.ico?favicon.e10ff5c7.ico" sizes="1176x1032" type="image/x-icon"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased"><div hidden=""><!--$--><!--/$--></div><div class="flex flex-col justify-center min-h-screen"><div class="flex flex-row justify-center pt-15"><div class="flex flex-row w-full md:max-w-5/10 relative"><div class="flex flex-row md:basis-1/3 "><a href="/"><div class="text-2xl font-bold pl-5 md:pl-0 pt-5 md:pt-0">Claire Zhao</div></a></div><div class="md:hidden grow flex justify-end items-center pr-10"><button><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="size-5"><path d="M8 2H13.5C13.7761 2 14 2.22386 14 2.5V12.5C14 12.7761 13.7761 13 13.5 13H8V2ZM7 2H1.5C1.22386 2 1 2.22386 1 2.5V12.5C1 12.7761 1.22386 13 1.5 13H7V2ZM0 2.5C0 1.67157 0.671573 1 1.5 1H13.5C14.3284 1 15 1.67157 15 2.5V12.5C15 13.3284 14.3284 14 13.5 14H1.5C0.671573 14 0 13.3284 0 12.5V2.5Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg></button></div><nav class="hidden md:flex basis-2/3 flex-row justify-end gap-x-8"><a href="/ai"><div class="flex items-center gap-2 font-medium hover:underline">Machine Learning <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9 4L9 11L4.5 7.5L9 4Z" fill="currentColor"></path></svg></div></a><a href="/math"><div class="flex items-center gap-2 font-medium hover:underline">Mathematics </div></a></nav></div></div><div class="grow flex justify-center w-full pt-10 md:pt-15 px-3"><div class="flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl"><div class="text-3xl font-bold flex justify-center">LLM Backward Pass</div><div class="text-sm flex justify-center pt-5">Feb 1</div><div class=""><div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Modern LLMs architectures have two core computations: the attention operator, which exchanges information between tokens, and multilayer perceptron (aka dense or MoE feedforward layer) which is a position-wise operation. Both these operators have three phases: forward pass, backward pass (used interchangably with backpropagation), and optimization (weight update using gradients).</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">In MLP forward pass, a fundamental math operation is projection through a layer with weights matrix <span class="mt-10"></span> defined by <span class="mt-10"></span>. In attention forward pass, there are several matrix multiplications such as <span class="mt-10"></span>. So the key question for backward pass is <span class="italic">how to backpropagate through a matrix product</span>?</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Activation Gradient and Weight Gradient</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">In LLM forward pass, the <span class="font-semibold">activation</span> of one layer <span class="mt-10"></span> is defined as the output of that layer and it becomes the input of the next layer <span class="mt-10"></span>. If <span class="mt-10"></span> computes <span class="mt-10"></span> then <span class="mt-10"></span> is in fact the activation of <span class="mt-10"></span>. During backwards pass through <span class="mt-10"></span>, the gradient of the loss with respect to <span class="mt-10"></span> needs to be computed. This gradient with respect to the weights is called <span class="font-semibold">weight gradient</span>. During backpropgation through <span class="mt-10"></span> we additionally need to compute the gradient of the loss with respect to <span class="mt-10"></span> in order to later compute the weight gradient associated with <span class="mt-10"></span>. This gradient with respect to <span class="mt-10"></span> is called <span class="font-semibold">activation gradient</span>. By contrast, when backpropagating through an operation such as <span class="mt-10"></span> in the attention operation, the gradient with respect to <span class="mt-10"></span> and <span class="mt-10"></span> are both activation gradients, since these are not the gradient of any weights in the neural network, but rather gradients used to compute weight gradients further along backward pass.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">The question <span class="italic">how to backpropagate through a matrix product</span>? is made more precise as follows: let <span class="mt-10"></span> have continuous partial derivatives of the first order, and let <span class="mt-10"></span> be defined to be the matrix product <span class="mt-10"></span>, when any vector <span class="mt-10"></span> is considered as a <span class="mt-10"></span> matrix, and likewise for <span class="mt-10"></span> and the product element <span class="mt-10"></span>. Define <span class="mt-10"></span>. We want to compute the derivative <span class="mt-10"></span>.</div></div></div></div></div></div><div class="flex justify-center my-5 md:my-10">Claire Zhao Â© 2026</div></div><!--$--><!--/$--><script src="/_next/static/chunks/be439e04947a2cba.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[48523,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"default\"]\n3:I[82281,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"default\"]\n4:I[1565,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/e5d291186d0cbb54.js\",\"/_next/static/chunks/bc166ea53d390db7.js\"],\"default\"]\n5:I[96045,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/e5d291186d0cbb54.js\",\"/_next/static/chunks/bc166ea53d390db7.js\"],\"default\"]\nd:I[75067,[],\"default\"]\n:HL[\"/_next/static/chunks/e23ef66a1dfd4af0.css\",\"style\"]\n:HL[\"/_next/static/chunks/4057cc9dbcc744c0.css\",\"style\"]\n:HL[\"/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"_q-PAJOHlzGsitc36faMO\",\"c\":[\"\",\"ai\",\"llm-back-propagation\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"ai\",{\"children\":[\"llm-back-propagation\",{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/e23ef66a1dfd4af0.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/4057cc9dbcc744c0.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col justify-center min-h-screen\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-row justify-center pt-15\",\"children\":[\"$\",\"$L4\",null,{}]}],[\"$\",\"div\",null,{\"className\":\"grow flex justify-center w-full pt-10 md:pt-15 px-3\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-3xl font-bold flex justify-center\",\"children\":\"LLM Backward Pass\"}],[\"$\",\"div\",null,{\"className\":\"text-sm flex justify-center pt-5\",\"children\":\"Feb 1\"}],[\"$\",\"div\",null,{\"className\":\"\",\"children\":[[\"$\",\"div\",\"0\",{\"children\":[\"$undefined\",[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":\"Modern LLMs architectures have two core computations: the attention operator, which exchanges information between tokens, and multilayer perceptron (aka dense or MoE feedforward layer) which is a position-wise operation. Both these operators have three phases: forward pass, backward pass (used interchangably with backpropagation), and optimization (weight update using gradients).\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"In MLP forward pass, a fundamental math operation is projection through a layer with weights matrix \",[\"$\",\"$L5\",null,{\"latex\":\"W\",\"className\":\"mt-10\"}],\" defined by \",[\"$\",\"$L5\",null,{\"latex\":\"X\\\\mapsto WX\",\"className\":\"mt-10\"}],\". In attention forward pass, there are several matrix multiplications such as \",[\"$\",\"$L5\",null,{\"latex\":\"Q,K\\\\mapsto QK\",\"className\":\"mt-10\"}],\". So the key question for backward pass is \",[\"$\",\"span\",null,{\"className\":\"italic\",\"children\":\"how to backpropagate through a matrix product\"}],\"?\"]}]]}]]}],\"$L6\"]}]]}]}],\"$L7\"]}],[\"$L8\",\"$L9\",\"$La\"],\"$Lb\"]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],\"$Lc\",false]],\"m\":\"$undefined\",\"G\":[\"$d\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"e:I[56691,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/e5d291186d0cbb54.js\",\"/_next/static/chunks/bc166ea53d390db7.js\"],\"default\"]\nf:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"OutletBoundary\"]\n10:\"$Sreact.suspense\"\n12:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"ViewportBoundary\"]\n14:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"MetadataBoundary\"]\n"])</script><script>self.__next_f.push([1,"6:[\"$\",\"div\",\"1\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Activation Gradient and Weight Gradient\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"In LLM forward pass, the \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"activation\"}],\" of one layer \",[\"$\",\"$L5\",null,{\"latex\":\"L_1\",\"className\":\"mt-10\"}],\" is defined as the output of that layer and it becomes the input of the next layer \",[\"$\",\"$L5\",null,{\"latex\":\"L_2\",\"className\":\"mt-10\"}],\". If \",[\"$\",\"$L5\",null,{\"latex\":\"L_2\",\"className\":\"mt-10\"}],\" computes \",[\"$\",\"$L5\",null,{\"latex\":\"X\\\\mapsto W_2X\",\"className\":\"mt-10\"}],\" then \",[\"$\",\"$L5\",null,{\"latex\":\"X\",\"className\":\"mt-10\"}],\" is in fact the activation of \",[\"$\",\"$L5\",null,{\"latex\":\"L_1\",\"className\":\"mt-10\"}],\". During backwards pass through \",[\"$\",\"$L5\",null,{\"latex\":\"L_2\",\"className\":\"mt-10\"}],\", the gradient of the loss with respect to \",[\"$\",\"$L5\",null,{\"latex\":\"W_2\",\"className\":\"mt-10\"}],\" needs to be computed. This gradient with respect to the weights is called \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"weight gradient\"}],\". During backpropgation through \",[\"$\",\"$L5\",null,{\"latex\":\"L_2\",\"className\":\"mt-10\"}],\" we additionally need to compute the gradient of the loss with respect to \",[\"$\",\"$L5\",null,{\"latex\":\"X\",\"className\":\"mt-10\"}],\" in order to later compute the weight gradient associated with \",[\"$\",\"$L5\",null,{\"latex\":\"L_1\",\"className\":\"mt-10\"}],\". This gradient with respect to \",[\"$\",\"$L5\",null,{\"latex\":\"X\",\"className\":\"mt-10\"}],\" is called \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"activation gradient\"}],\". By contrast, when backpropagating through an operation such as \",[\"$\",\"$L5\",null,{\"latex\":\"Q,K\\\\mapsto QK\",\"className\":\"mt-10\"}],\" in the attention operation, the gradient with respect to \",[\"$\",\"$L5\",null,{\"latex\":\"Q\",\"className\":\"mt-10\"}],\" and \",[\"$\",\"$L5\",null,{\"latex\":\"K\",\"className\":\"mt-10\"}],\" are both activation gradients, since these are not the gradient of any weights in the neural network, but rather gradients used to compute weight gradients further along backward pass.\"]}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"The question \",[\"$\",\"span\",null,{\"className\":\"italic\",\"children\":\"how to backpropagate through a matrix product\"}],\"? is made more precise as follows: let \",[\"$\",\"$L5\",null,{\"latex\":\"f:\\\\mathbb{R}^{mn}\\\\mathbb\\\\longrightarrow {R}\",\"className\":\"mt-10\"}],\" have continuous partial derivatives of the first order, and let \",[\"$\",\"$L5\",null,{\"latex\":\"g:\\\\mathbb{R}^{mk}\\\\times \\\\mathbb{R}^{kn} \\\\longrightarrow \\\\mathbb{R}^{mn}\",\"className\":\"mt-10\"}],\" be defined to be the matrix product \",[\"$\",\"$L5\",null,{\"latex\":\"x,y\\\\mapsto xy\",\"className\":\"mt-10\"}],\", when any vector \",[\"$\",\"$L5\",null,{\"latex\":\"x\\\\in\\\\mathbb{R}^{mk}\",\"className\":\"mt-10\"}],\" is considered as a \",[\"$\",\"$L5\",null,{\"latex\":\"m\\\\times k\",\"className\":\"mt-10\"}],\" matrix, and likewise for \",[\"$\",\"$L5\",null,{\"latex\":\"y\",\"className\":\"mt-10\"}],\" and the product element \",[\"$\",\"$L5\",null,{\"latex\":\"xy\\\\in\\\\mathbb{R}^{mn}\",\"className\":\"mt-10\"}],\". Define \",[\"$\",\"$L5\",null,{\"latex\":\"\\\\ell = f\\\\circ g: \\\\mathbb{R}^{mk}\\\\times \\\\mathbb{R}^{kn}\\\\longrightarrow \\\\mathbb{R}\",\"className\":\"mt-10\"}],\". We want to compute the derivative \",[\"$\",\"$L5\",null,{\"latex\":\"D\\\\ell\",\"className\":\"mt-10\"}],\".\"]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"7:[\"$\",\"$Le\",null,{}]\n8:[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/988ea292de4c4c73.js\",\"async\":true,\"nonce\":\"$undefined\"}]\n9:[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/e5d291186d0cbb54.js\",\"async\":true,\"nonce\":\"$undefined\"}]\na:[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/bc166ea53d390db7.js\",\"async\":true,\"nonce\":\"$undefined\"}]\nb:[\"$\",\"$Lf\",null,{\"children\":[\"$\",\"$10\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@11\"}]}]\nc:[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L12\",null,{\"children\":\"$L13\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$L14\",null,{\"children\":[\"$\",\"$10\",null,{\"name\":\"Next.Metadata\",\"children\":\"$L15\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}]\n"])</script><script>self.__next_f.push([1,"13:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"16:I[87718,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"IconMark\"]\n11:null\n15:[[\"$\",\"title\",\"0\",{\"children\":\"Claire Zhao Blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Claire Zhao's Blog on AI and Math\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/favicon.ico?favicon.e10ff5c7.ico\",\"sizes\":\"1176x1032\",\"type\":\"image/x-icon\"}],[\"$\",\"$L16\",\"3\",{}]]\n"])</script></body></html>