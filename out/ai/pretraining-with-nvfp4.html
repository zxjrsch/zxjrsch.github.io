<!DOCTYPE html><!--b6i6FOYyn11YbpDT79n3X--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/chunks/e23ef66a1dfd4af0.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/4057cc9dbcc744c0.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/be439e04947a2cba.js"/><script src="/_next/static/chunks/922a6be1c048f7e7.js" async=""></script><script src="/_next/static/chunks/748df87cf5306c08.js" async=""></script><script src="/_next/static/chunks/turbopack-0af7db035ef23dbc.js" async=""></script><script src="/_next/static/chunks/988ea292de4c4c73.js" async=""></script><script src="/_next/static/chunks/7a959126392b4956.js" async=""></script><script src="/_next/static/chunks/673699ad6df5bf6b.js" async=""></script><script src="/_next/static/chunks/e5d291186d0cbb54.js" async=""></script><script src="/_next/static/chunks/bc166ea53d390db7.js" async=""></script><meta name="next-size-adjust" content=""/><title>Claire Zhao Blog</title><meta name="description" content="Claire Zhao&#x27;s Blog on AI and Math"/><link rel="icon" href="/favicon.ico?favicon.e10ff5c7.ico" sizes="1176x1032" type="image/x-icon"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased"><div hidden=""><!--$--><!--/$--></div><div class="flex flex-col justify-center min-h-screen"><div class="flex flex-row justify-center pt-15"><div class="flex flex-row w-full md:max-w-5/10 relative"><div class="flex flex-row md:basis-1/3 "><a href="/"><div class="text-2xl font-bold pl-5 md:pl-0 pt-5 md:pt-0">Claire Zhao</div></a></div><div class="md:hidden grow flex justify-end items-center pr-10"><button><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="size-5"><path d="M8 2H13.5C13.7761 2 14 2.22386 14 2.5V12.5C14 12.7761 13.7761 13 13.5 13H8V2ZM7 2H1.5C1.22386 2 1 2.22386 1 2.5V12.5C1 12.7761 1.22386 13 1.5 13H7V2ZM0 2.5C0 1.67157 0.671573 1 1.5 1H13.5C14.3284 1 15 1.67157 15 2.5V12.5C15 13.3284 14.3284 14 13.5 14H1.5C0.671573 14 0 13.3284 0 12.5V2.5Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg></button></div><nav class="hidden md:flex basis-2/3 flex-row justify-end gap-x-8"><a href="/ai"><div class="flex items-center gap-2 font-medium hover:underline">Machine Learning <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9 4L9 11L4.5 7.5L9 4Z" fill="currentColor"></path></svg></div></a><a href="/math"><div class="flex items-center gap-2 font-medium hover:underline">Mathematics </div></a></nav></div></div><div class="grow flex justify-center w-full pt-10 md:pt-15 px-3"><div class="flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl"><div class="text-3xl font-bold flex justify-center">Training LLMs in NVFP4</div><div class="text-sm flex justify-center pt-5">Modified on Jan 31</div><div class=""><div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Consider how a real number <span class="mt-10"></span> can be represented on a computer. If <span class="mt-10"></span> then <span class="mt-10"></span> where <span class="mt-10"></span> is the sign of <span class="mt-10"></span> and can be represented with one bit <span class="mt-10"></span>. For the positive number <span class="mt-10"></span> there exists a smallest <span class="mt-10"></span> such that <span class="mt-10"></span>. Now we can divide this interval into two equal halves. By definition of <span class="mt-10"></span> we know that <span class="mt-10"></span>. We can again divide the interval <span class="mt-10"></span> into two equal halves and <span class="mt-10"></span> lies in either the left half or the right half (to break the tie, if <span class="mt-10"></span> lies in the middle, then we say it lies on the right half). As we make the sequence of interval partitions, the positive real number <span class="mt-10"></span> lies in either the left or the right half. Associate left with <span class="mt-10"></span> right with <span class="mt-10"></span> we obtain a binary sequence <span class="mt-10"></span>. It is clear that for the <span class="mt-10"></span>-th partition the situation is <span class="mt-10"></span> where <span class="mt-10"></span> is less than the length of the half interval in the <span class="mt-10"></span>-th partition. So the positive real number <span class="mt-10"></span>, and a desired precision <span class="mt-10"></span>, we can choose an integer <span class="mt-10"></span> such that <span class="mt-10"></span> so that the number <span class="mt-10"></span> which is representable by a binary sequence of length <span class="mt-10"></span>, approximates <span class="mt-10"></span> to within <span class="mt-10"></span>, namely <span class="mt-10"></span> Since <span class="mt-10"></span> we can write <span class="mt-10"></span> Hardware approximates <span class="mt-10"></span> by storing one bit <span class="mt-10"></span> for the <span class="font-semibold">sign</span> of <span class="mt-10"></span>, another <span class="mt-10"></span> bits <span class="mt-10"></span> called the mantissa of <span class="mt-10"></span>, the bit length of which determines an upper bounds on the error <span class="mt-10"></span> of approximating <span class="mt-10"></span> by <span class="mt-10"></span>, together with a representation of the <span class="font-semibold">exponent</span> <span class="mt-10"></span>.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Now we need a scheme for representing the integer <span class="mt-10"></span> in bits. Suppose that <span class="mt-10"></span> bits are designated to store the exponent, then there are <span class="mt-10"></span> configurations, meaning that the real number the float can approximate spans <span class="mt-10"></span> orders of magnitudes. Naturally we want to choose <span class="mt-10"></span> orders of magnitudes about the multiplicative unit <span class="mt-10"></span>. Customarily we designate <span class="mt-10"></span> of the <span class="mt-10"></span> orders of magnitudes to be at or below the unit, and another <span class="mt-10"></span> orders of magnitudes to be above the unit. The hardware implementation is to store the integer exponent <span class="mt-10"></span> with an non-negative integer <span class="mt-10"></span> such that <span class="mt-10"></span> this way the smallest order of magnitude (with respect this dtype) <span class="mt-10"></span> is stored as <span class="mt-10"></span> whereas the largest order of magnitude <span class="mt-10"></span> is stored as <span class="mt-10"></span>. This scheme is nothing but a bijection from <span class="mt-10"></span> The quantity <span class="mt-10"></span> is sometimes given the nondescript name <span class="font-semibold">bias</span> just to be confusing! The mantissa <span class="mt-10"></span>, which we have until now defined as the bit string <span class="mt-10"></span> should really be identified with the real number that it represents <span class="mt-10"></span> and together with <span class="mt-10"></span> and the sign <span class="mt-10"></span> represents the real number (approximately) on hardware as <span class="mt-10"></span></div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">The above analysis applies to <span class="mt-10"></span>. Since <span class="mt-10"></span> is a limit point of the set <span class="mt-10"></span> there does not exist a <span class="italic">smallest</span> integer <span class="mt-10"></span> such that <span class="mt-10"></span>. Suppose that a float dtype has <span class="mt-10"></span> bits to store the mantissa, it is natural to approximate zero with the smallest element in the set <span class="mt-10"></span> the minimum value being <span class="mt-10"></span> and corresponds <span class="mt-10"></span> and <span class="mt-10"></span>. Equivalently zero can be approximated by the maximum of the set <span class="mt-10"></span> consisting the negative elements of the set <span class="mt-10"></span>. Therefore we have two representations <span class="mt-10"></span> of zero by the dtype. Now this argument is true to first-order, and needs slight modification when we introduce the so called <span class="font-semibold">subnormal</span> numbers in practical floating point implementations. The idea is that <span class="mt-10"></span> is not the smallest order of magnitude we can represent if in our implementation we cutomzarily agree that when <span class="mt-10"></span> we replace <span class="mt-10"></span> by <span class="mt-10"></span> where observe that <span class="mt-10"></span> whereby <span class="mt-10"></span> is the smallest <span class="italic">normal</span> order of magnitude. If the mantissa has <span class="mt-10"></span> bits, then the smallest subnormal number is <span class="mt-10"></span> and the largest subnormal is <span class="mt-10"></span> corresponding <span class="mt-10"></span> with mantissa <span class="mt-10"></span> and <span class="mt-10"></span> respecitively. The case <span class="mt-10"></span> with <span class="mt-10"></span> is reserved to represent zero. Observe that such smaller than <span class="mt-10"></span> magnitudes are only expressible near zero, with subnormal numbers. That is, for a general real numbers <span class="mt-10"></span>, the upper bound of error of approximation is still <span class="mt-10"></span> even though for numbers below the smallest normal the error of approximation has bound <span class="mt-10"></span>.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Let us build some intuition. (1) observing that every mantissa bit of a normal number is <span class="italic">significant</span> but for subnormal number this need not be the case. For instance for a normal number with <span class="mt-10"></span>, every bit is signitficant. For the subnormal number with mantissa <span class="mt-10"></span> every bit is significant, but a subnormal with <span class="mt-10"></span> only has 1 significant bit. The leading zeros are placeholders and not significant. (2) A hand wavy way to look at gaps between int and floats. Let <span class="mt-10"></span>, the gaps between consecutive Int-<span class="mt-10"></span> numbers are the same, whereas for Float-<span class="mt-10"></span> numbers with <span class="mt-10"></span> bit mantissa, the ratio of gap sizes between nearby numbers are evenly spaced on a log-scale. This is meant in the sense that for a float <span class="mt-10"></span> the nearby elements are spaced <span class="mt-10"></span>. In particular these gaps are larger by a factor of <span class="mt-10"></span> depending on the exponent <span class="mt-10"></span> for the float <span class="mt-10"></span>.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Let <span class="mt-10"></span>. We say that a floating point data type has bit structure <span class="font-semibold">ExMy</span> if every float is represented (in hardware or software) as 1 sign bit together with <span class="mt-10"></span> exponent bits and <span class="mt-10"></span> mantissa bits. It is sometimes the convention that if <span class="mt-10"></span> then the number has no sign bit.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Since any hardware floating point number consists of a finite number of bits, the float has a limited range of expressible order of magnitudes. For numbers outside this range, a data type sometimes designate bit patters for the sign, mantissa, and exponent to represent <span class="mt-10"></span>. However this is optional, for example in the so called <span class="italic">microscaled</span> (MX) classes of floats, infinities are excluded in E4M3 MXFP8 but are included in E5M2 MXFP8. Other special values for a float are NaN or not a number, which designate the results of undefined operations such as <span class="mt-10"></span>. How to implement NaN for a given float is a choice. In some designs, NaN is omitted, while in others only one bit pattern (up to equivalence in sign) <span class="mt-10"></span> denotes NaN, and yet in others, multiple such pairs are treated NaN (compare E2M3, E4M3, E5M2 microscaled floats).</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">The aforementioned microscaled floats works as follows, instead of representing one real number at a time, MX floats represents collections of real numbers simultaneously. Suppose <span class="mt-10"></span>, with <span class="mt-10"></span>. Let <span class="mt-10"></span> be the set of all <span class="mt-10"></span> floats, and let <span class="mt-10"></span> be the set of all <span class="mt-10"></span> floats. The set <span class="mt-10"></span> is called the <span class="mt-10"></span>-dimensional <span class="font-semibold">microscaling (MX) float</span> with scalers of type <span class="mt-10"></span> and block of type <span class="mt-10"></span>. An element <span class="mt-10"></span> is associated with <span class="mt-10"></span>. There are dtype specific rules like <span class="mt-10"></span> for all <span class="mt-10"></span>, and implementation defined rule for situations such as when <span class="mt-10"></span>. Within microscaled data types there are floats MXFP4 where scalars <span class="mt-10"></span> are of type E8M0 and <span class="mt-10"></span> of type E2M1 and dimension <span class="mt-10"></span> togehter with MX integers like MXINT8. Encodings for mantissa are somewhat different than our discussion so far, in the sense that the mantissa has <span class="mt-10"></span> factored out (<span class="mt-10"></span> being the number of mantissa bits), so that <span class="mt-10"></span> so the unsigned integer <span class="mt-10"></span> is stored as opposed to <span class="mt-10"></span>. In this scheme a normal number has <span class="mt-10"></span> and value <span class="mt-10"></span> and a subnormal has <span class="mt-10"></span> and <span class="mt-10"></span></div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">The dot product of two MX floats of the same dtype corresponds the dot product of the <span class="mt-10"></span> vectors they represent: for every <span class="mt-10"></span> and every <span class="mt-10"></span> we define their <span class="font-semibold">dot product</span> to be <span class="mt-10"></span> The product between an element of <span class="mt-10"></span> and an element of <span class="mt-10"></span> and likewise the product between elements of <span class="mt-10"></span> and elements of <span class="mt-10"></span> are specified in the implementation. Specification of the output type depends on the situation.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Let <span class="mt-10"></span>. More generally dot product can be defined between every vector <span class="mt-10"></span> and every <span class="mt-10"></span> by <span class="mt-10"></span> This dot product requires the length of the vectors to be a multiple of the number <span class="mt-10"></span> of elements per MX block. This can be relaxed to any length <span class="mt-10"></span> by padding to the nearest multiple of <span class="mt-10"></span> greater than <span class="mt-10"></span> and truncation the result back to length <span class="mt-10"></span>.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">To represent a vector <span class="mt-10"></span> in MX with maximal value <span class="mt-10"></span> the corresponding scale factor <span class="mt-10"></span> is determined by <span class="mt-10"></span> In other words, the scale factor depends on <span class="mt-10"></span> and the largest normal number <span class="mt-10"></span> for the block <span class="mt-10"></span> data type. The scale factor  is the ratio of the largest power of two less than or equal to <span class="mt-10"></span> to the largest power of two less than or equal to <span class="mt-10"></span>. Observe that the scaling factor of MX is an integer power of <span class="mt-10"></span> (hence in MXFP8 the block scale are E8M0 integers).</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Let <span class="mt-10"></span> be the map that quantizes real numbers to <span class="mt-10"></span> with the scheme in the begining of the blog, and which assigns values exceeding the maximal value to <span class="mt-10"></span> and those values below the minimal normal value to <span class="mt-10"></span>. The corresponding block values <span class="mt-10"></span></div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Definition</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Hardware represents floating point numbers in the form <span class="mt-10"></span> where <span class="mt-10"></span> determines the sign of the number and is stored with one bit. Depending on the datatype, a choice of <span class="mt-10"></span> bits are devoted to the mantissa <span class="mt-10"></span>, and <span class="mt-10"></span> bits are devoted to the exponent <span class="mt-10"></span>. The datatype thus determined requires <span class="mt-10"></span> bits to represent a float, and is denoted EeMm. For instance, FP32 is E8M23, FP16 is E5M10 where as BF16 is E8M7.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Quantization is the operation that maps numbers repreented in a given datatype to numbers in another datatype requiring less number <span class="mt-10"></span> of bits. Thus quantization is a compression mechanism that reduces storage and communication footprint, and increase compute throughput. The efficiency gained via compression is to be trade-off with degradation in accuracy in one form or antoher.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">In practice one quantizes a set <span class="mt-10"></span> of <span class="mt-10"></span> numbers together. For example, to quantize real numbers <span class="mt-10"></span> into <span class="mt-10"></span> bit integers <span class="mt-10"></span>, we can choose the mapping <span class="mt-10"></span> In particular the scale factor <span class="mt-10"></span> is chosen so that the maximum element of <span class="mt-10"></span> gets mapped to <span class="mt-10"></span>. Since the mapping <span class="mt-10"></span> is many-to-one we can only hope to dequantize approximately, with error. More precisely, given an integer <span class="mt-10"></span> we can map it to <span class="mt-10"></span> and this introduces an error <span class="mt-10"></span>.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">In general given a pair of quantization and dequantization maps <span class="mt-10"></span> one can measure the <span class="mt-10"></span> error  <span class="mt-10"></span>.</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Training in NVFP4</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Based on the papers and documentations I can find, here is a sketch of training LLMs in NVFP4.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Like FP4 and MXFP4, NVFP4 has the bit strcuture of E2M1. The distinction lies in how NVFP4 represent a set <span class="mt-10"></span> of numbers, which we refer to as a tensor. In particular, NVFP4 partitions <span class="mt-10"></span> into subsets of <span class="mt-10"></span> numbers each called a block. Each block is associated with an 8-bit E4M3 number called a block scale factor <span class="mt-10"></span> such that each one of the <span class="mt-10"></span> four bit numbers <span class="mt-10"></span> belonging to the same block is reconstructed with <span class="mt-10"></span> . Additionally, a FP32 number is asspciated with the tensor <span class="mt-10"></span> itself, called the tensor scale factor.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">By contrast each MXFP4 partitions consists of <span class="mt-10"></span> numbers, and its block scale factor is E8M0 (i.e. round to the nearest power of two). It can be shown that the expected square error with E8M0 is larger than that of E4M3, with the tradeoff being E8M0 has less overhead. The said parititon and scaling in NVFP4 is handled by specialized tensor core harware.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Given that E2M1 and E4M3 can represent numbers with maximum absolute value of <span class="mt-10"></span> and <span class="mt-10"></span> respectively, the tensor scale factor for a tensor <span class="mt-10"></span> indexed by a set <span class="mt-10"></span> is <span class="mt-10"></span> the tensor dequantization scale is <span class="mt-10"></span> and is stored in FP32. Let <span class="mt-10"></span> be an indexing set for a block in <span class="mt-10"></span>, the corresponding block scale factor that is <span class="mt-10"></span> In fact, the block dequantization scale factor <span class="mt-10"></span> is stored in FP8 on the tensor core as <span class="mt-10"></span></div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Each block <span class="mt-10"></span> gets quantized as <span class="mt-10"></span> and partial dot during GEMM product is computed as <span class="mt-10"></span> where <span class="mt-10"></span> After GEMM the tensor dequantization scales are applied.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">There are experiments showing NVFP4 should be used in earlier layers of the forward pass direction of a transformer, while keeping later layers in higher precision.</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Random Hadamard Transform</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Hadamard matrices of dimension <span class="mt-10"></span> for an integer <span class="mt-10"></span> satisfies <span class="mt-10"></span> and <span class="mt-10"></span>. We shall consider a randomized Hadamard matrix <span class="mt-10"></span> where <span class="mt-10"></span> is a diagonal matrix of values <span class="mt-10"></span> chosen uniformly at random. In training, instead of operating on tensors <span class="mt-10"></span> one qpplies the above NVFP4 on the random Hadamard transformed tiles <span class="mt-10"></span> of the tensor. In some experiments <span class="mt-10"></span> is applied to inputs of weight gradient GEMM.</div></div></div></div></div></div><div class="flex justify-center my-5 md:my-10">Claire Zhao Â© 2026</div></div><!--$--><!--/$--><script src="/_next/static/chunks/be439e04947a2cba.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[48523,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"default\"]\n3:I[82281,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"default\"]\n4:I[50544,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"ClientPageRoot\"]\n5:I[13068,[\"/_next/static/chunks/673699ad6df5bf6b.js\",\"/_next/static/chunks/e5d291186d0cbb54.js\",\"/_next/static/chunks/bc166ea53d390db7.js\"],\"default\"]\n8:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"OutletBoundary\"]\n9:\"$Sreact.suspense\"\nb:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"ViewportBoundary\"]\nd:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"MetadataBoundary\"]\nf:I[75067,[],\"default\"]\n:HL[\"/_next/static/chunks/e23ef66a1dfd4af0.css\",\"style\"]\n:HL[\"/_next/static/chunks/4057cc9dbcc744c0.css\",\"style\"]\n:HL[\"/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"b6i6FOYyn11YbpDT79n3X\",\"c\":[\"\",\"ai\",\"pretraining-with-nvfp4\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"ai\",{\"children\":[\"pretraining-with-nvfp4\",{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/e23ef66a1dfd4af0.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/4057cc9dbcc744c0.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"$L4\",null,{\"Component\":\"$5\",\"serverProvidedParams\":{\"searchParams\":{},\"params\":{},\"promises\":[\"$@6\",\"$@7\"]}}],[[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/673699ad6df5bf6b.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/e5d291186d0cbb54.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/bc166ea53d390db7.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L8\",null,{\"children\":[\"$\",\"$9\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@a\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$Lb\",null,{\"children\":\"$Lc\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$Ld\",null,{\"children\":[\"$\",\"$9\",null,{\"name\":\"Next.Metadata\",\"children\":\"$Le\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$f\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"6:{}\n7:\"$0:f:0:1:1:children:1:children:1:children:0:props:children:0:props:serverProvidedParams:params\"\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"10:I[87718,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"IconMark\"]\na:null\ne:[[\"$\",\"title\",\"0\",{\"children\":\"Claire Zhao Blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Claire Zhao's Blog on AI and Math\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/favicon.ico?favicon.e10ff5c7.ico\",\"sizes\":\"1176x1032\",\"type\":\"image/x-icon\"}],[\"$\",\"$L10\",\"3\",{}]]\n"])</script></body></html>