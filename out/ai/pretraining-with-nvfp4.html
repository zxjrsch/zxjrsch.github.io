<!DOCTYPE html><!--5WFIPOjLToAzy8YsZyGOU--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/chunks/41f3a8da8bca9d77.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/4057cc9dbcc744c0.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/30ba86a1a8c9981d.js"/><script src="/_next/static/chunks/1573b7123d74d4c9.js" async=""></script><script src="/_next/static/chunks/922a6be1c048f7e7.js" async=""></script><script src="/_next/static/chunks/a03c011691672700.js" async=""></script><script src="/_next/static/chunks/turbopack-10cc5bb296a53f96.js" async=""></script><script src="/_next/static/chunks/988ea292de4c4c73.js" async=""></script><script src="/_next/static/chunks/7a959126392b4956.js" async=""></script><script src="/_next/static/chunks/126be0e244848aa8.js" async=""></script><script src="/_next/static/chunks/a3d11cb923da6e55.js" async=""></script><script src="/_next/static/chunks/684309c07918831c.js" async=""></script><meta name="next-size-adjust" content=""/><title>Claire Zhao Blog</title><meta name="description" content="Claire Zhao&#x27;s Blog on AI and Math"/><link rel="icon" href="/favicon.ico?favicon.e10ff5c7.ico" sizes="1176x1032" type="image/x-icon"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased"><div hidden=""><!--$--><!--/$--></div><div class="flex flex-col justify-center min-h-screen"><div class="flex flex-row justify-center pt-15"><div class="flex flex-row w-full md:max-w-5/10"><div class="flex flex-row md:basis-1/3 "><a href="/"><div class="text-2xl font-bold pl-5 md:pl-0">Claire Zhao Blog</div></a></div><div class="md:hidden grow flex justify-end items-center pr-10"><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="size-5"><path d="M8 2H13.5C13.7761 2 14 2.22386 14 2.5V12.5C14 12.7761 13.7761 13 13.5 13H8V2ZM7 2H1.5C1.22386 2 1 2.22386 1 2.5V12.5C1 12.7761 1.22386 13 1.5 13H7V2ZM0 2.5C0 1.67157 0.671573 1 1.5 1H13.5C14.3284 1 15 1.67157 15 2.5V12.5C15 13.3284 14.3284 14 13.5 14H1.5C0.671573 14 0 13.3284 0 12.5V2.5Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg></div></div></div><div class="grow flex justify-center w-full pt-10 md:pt-15 px-3"><div class="flex flex-col lg:min-w-1/2 lg:max-w-1/2"><div class="text-3xl font-bold flex justify-center">Pretraining LLMs with NVFP4</div><div class="text-sm flex justify-center pt-5"></div><div class=""><div><div class=" pt-20 font-bold text-2xl" id="section-2">Definition</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Hardware represents floating point numbers in the form <span class="mt-10"></span> where <span class="mt-10"></span> determines the sign of the number and is stored with one bit. Depending on the datatype, a choice of <span class="mt-10"></span> bits are devoted to the mantissa <span class="mt-10"></span>, and <span class="mt-10"></span> bits are devoted to the exponent <span class="mt-10"></span>. The datatype thus determined requires <span class="mt-10"></span> bits to represent a float, and is denoted EeMm. For instance, FP32 is E8M23, FP16 is E5M10 where as BF16 is E8M7.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Quantization is the operation that maps numbers repreented in a given datatype to numbers in another datatype requiring less number <span class="mt-10"></span> of bits. Thus quantization is a compression mechanism that reduces storage and communication footprint, and increase compute throughput. The efficiency gained via compression is to be trade-off with degradation in accuracy in one form or antoher.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">In practice one quantizes a set <span class="mt-10"></span> of <span class="mt-10"></span> numbers together. For example, to quantize real numbers <span class="mt-10"></span> into <span class="mt-10"></span> bit integers <span class="mt-10"></span>, we can choose the mapping <span class="mt-10"></span> In particular the scale factor <span class="mt-10"></span> is chosen so that the maximum element of <span class="mt-10"></span> gets mapped to <span class="mt-10"></span>. Since the mapping <span class="mt-10"></span> is many-to-one we can only hope to dequantize approximately, with error. More precisely, given an integer <span class="mt-10"></span> we can map it to <span class="mt-10"></span> and this introduces an error <span class="mt-10"></span>.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">In general given a pair of quantization and dequantization maps <span class="mt-10"></span> one can measure the <span class="mt-10"></span> error  <span class="mt-10"></span>.</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Trainng in NVFP4</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Based on the papers and documentations I can find, here is a sketch of training LLMs in NVFP4.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Like FP4 and MXFP4, NVFP4 has the bit strcuture of E2M1. The distinction lies in how NVFP4 represent a set <span class="mt-10"></span> of numbers, which we refer to as a tensor. In particular, NVFP4 partitions <span class="mt-10"></span> into subsets of <span class="mt-10"></span> numbers each called a block. Each block is associated with an 8-bit E4M3 number called a block scale factor <span class="mt-10"></span> such that each one of the <span class="mt-10"></span> four bit numbers <span class="mt-10"></span> belonging to the same block is reconstructed with <span class="mt-10"></span> . Additionally, a FP32 number is asspciated with the tensor <span class="mt-10"></span> itself, called the tensor scale factor.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">By contrast each MXFP4 partitions consists of <span class="mt-10"></span> numbers, and its block scale factor is E8M0 (i.e. round to the nearest power of two). It can be shown that the expected square error with E8M0 is larger than that of E4M3, with the tradeoff being E8M0 has less overhead. The said parititon and scaling in NVFP4 is handled by specialized tensor core harware.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Given that E2M1 and E4M3 can represent numbers with maximum absolute value of <span class="mt-10"></span> and <span class="mt-10"></span> respectively, the tensor scale factor for a tensor <span class="mt-10"></span> indexed by a set <span class="mt-10"></span> is <span class="mt-10"></span> the tensor dequantization scale is <span class="mt-10"></span> and is stored in FP32. Let <span class="mt-10"></span> be an indexing set for a block in <span class="mt-10"></span>, the corresponding block scale factor that is <span class="mt-10"></span> In fact, the block dequantization scale factor <span class="mt-10"></span> is stored in FP8 on the tensor core as <span class="mt-10"></span></div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Each block <span class="mt-10"></span> gets quantized as <span class="mt-10"></span> and partial dot during GEMM product is computed as <span class="mt-10"></span> where <span class="mt-10"></span> After GEMM the tensor dequantization scales are applied.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">There are experiments showing NVFP4 should be used in earlier layers of the forward pass direction of a transformer, while keeping later layers in higher precision.</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Random Hadamard Transform</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Hadamard matrices of dimension <span class="mt-10"></span> for an integer <span class="mt-10"></span> satisfies <span class="mt-10"></span> and <span class="mt-10"></span>. We shall consider a randomized Hadamard matrix <span class="mt-10"></span> where <span class="mt-10"></span> is a diagonal matrix of values <span class="mt-10"></span> chosen uniformly at random. In training, instead of operating on tensors <span class="mt-10"></span> one qpplies the above NVFP4 on the random Hadamard transformed tiles <span class="mt-10"></span> of the tensor. In some experiments <span class="mt-10"></span> is applied to inputs of weight gradient GEMM.</div></div></div></div></div></div><div class="flex justify-center my-5 md:my-10">Claire Zhao Â© 2026</div></div><!--$--><!--/$--><script src="/_next/static/chunks/30ba86a1a8c9981d.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[48523,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"default\"]\n3:I[82281,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"default\"]\n4:I[50544,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"ClientPageRoot\"]\n5:I[13068,[\"/_next/static/chunks/126be0e244848aa8.js\",\"/_next/static/chunks/a3d11cb923da6e55.js\",\"/_next/static/chunks/684309c07918831c.js\"],\"default\"]\n8:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"OutletBoundary\"]\n9:\"$Sreact.suspense\"\nb:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"ViewportBoundary\"]\nd:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"MetadataBoundary\"]\nf:I[75067,[],\"default\"]\n:HL[\"/_next/static/chunks/41f3a8da8bca9d77.css\",\"style\"]\n:HL[\"/_next/static/chunks/4057cc9dbcc744c0.css\",\"style\"]\n:HL[\"/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"5WFIPOjLToAzy8YsZyGOU\",\"c\":[\"\",\"ai\",\"pretraining-with-nvfp4\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"ai\",{\"children\":[\"pretraining-with-nvfp4\",{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/41f3a8da8bca9d77.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/4057cc9dbcc744c0.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"$L4\",null,{\"Component\":\"$5\",\"serverProvidedParams\":{\"searchParams\":{},\"params\":{},\"promises\":[\"$@6\",\"$@7\"]}}],[[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/126be0e244848aa8.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/a3d11cb923da6e55.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/684309c07918831c.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L8\",null,{\"children\":[\"$\",\"$9\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@a\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$Lb\",null,{\"children\":\"$Lc\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$Ld\",null,{\"children\":[\"$\",\"$9\",null,{\"name\":\"Next.Metadata\",\"children\":\"$Le\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$f\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"6:{}\n7:\"$0:f:0:1:1:children:1:children:1:children:0:props:children:0:props:serverProvidedParams:params\"\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"10:I[87718,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"IconMark\"]\na:null\ne:[[\"$\",\"title\",\"0\",{\"children\":\"Claire Zhao Blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Claire Zhao's Blog on AI and Math\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/favicon.ico?favicon.e10ff5c7.ico\",\"sizes\":\"1176x1032\",\"type\":\"image/x-icon\"}],[\"$\",\"$L10\",\"3\",{}]]\n"])</script></body></html>