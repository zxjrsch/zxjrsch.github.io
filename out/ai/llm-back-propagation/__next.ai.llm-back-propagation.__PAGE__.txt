1:"$Sreact.fragment"
2:I[1565,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/e5d291186d0cbb54.js","/_next/static/chunks/bc166ea53d390db7.js"],"default"]
3:I[96045,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/e5d291186d0cbb54.js","/_next/static/chunks/bc166ea53d390db7.js"],"default"]
a:I[56691,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/e5d291186d0cbb54.js","/_next/static/chunks/bc166ea53d390db7.js"],"default"]
b:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"OutletBoundary"]
c:"$Sreact.suspense"
0:{"buildId":"_q-PAJOHlzGsitc36faMO","rsc":["$","$1","c",{"children":[["$","div",null,{"className":"flex flex-col justify-center min-h-screen","children":[["$","div",null,{"className":"flex flex-row justify-center pt-15","children":["$","$L2",null,{}]}],["$","div",null,{"className":"grow flex justify-center w-full pt-10 md:pt-15 px-3","children":["$","div",null,{"className":"flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl","children":[["$","div",null,{"className":"text-3xl font-bold flex justify-center","children":"LLM Backward Pass"}],["$","div",null,{"className":"text-sm flex justify-center pt-5","children":"Feb 1"}],["$","div",null,{"className":"","children":[["$","div","0",{"children":["$undefined",["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"Modern LLMs architectures have two core computations: the attention operator, which exchanges information between tokens, and multilayer perceptron (aka dense or MoE feedforward layer) which is a position-wise operation. Both these operators have three phases: forward pass, backward pass (used interchangably with backpropagation), and optimization (weight update using gradients)."}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["In MLP forward pass, a fundamental math operation is projection through a layer with weights matrix ",["$","$L3",null,{"latex":"W","className":"mt-10"}]," defined by ",["$","$L3",null,{"latex":"X\\mapsto WX","className":"mt-10"}],". In attention forward pass, there are several matrix multiplications such as ",["$","$L3",null,{"latex":"Q,K\\mapsto QK","className":"mt-10"}],". So the key question for backward pass is ",["$","span",null,{"className":"italic","children":"how to backpropagate through a matrix product"}],"?"]}]]}]]}],["$","div","1",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Activation Gradient and Weight Gradient"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["In LLM forward pass, the ",["$","span",null,{"className":"font-semibold","children":"activation"}]," of one layer ",["$","$L3",null,{"latex":"L_1","className":"mt-10"}]," is defined as the output of that layer and it becomes the input of the next layer ",["$","$L3",null,{"latex":"L_2","className":"mt-10"}],". If ",["$","$L3",null,{"latex":"L_2","className":"mt-10"}]," computes ",["$","$L3",null,{"latex":"X\\mapsto W_2X","className":"mt-10"}]," then ",["$","$L3",null,{"latex":"X","className":"mt-10"}]," is in fact the activation of ",["$","$L3",null,{"latex":"L_1","className":"mt-10"}],". During backwards pass through ",["$","$L3",null,{"latex":"L_2","className":"mt-10"}],", the gradient of the loss with respect to ",["$","$L3",null,{"latex":"W_2","className":"mt-10"}]," needs to be computed. This gradient with respect to the weights is called ",["$","span",null,{"className":"font-semibold","children":"weight gradient"}],". During backpropgation through ",["$","$L3",null,{"latex":"L_2","className":"mt-10"}]," we additionally need to compute the gradient of the loss with respect to ",["$","$L3",null,{"latex":"X","className":"mt-10"}]," in order to later compute the weight gradient associated with ",["$","$L3",null,{"latex":"L_1","className":"mt-10"}],". This gradient with respect to ",["$","$L3",null,{"latex":"X","className":"mt-10"}]," is called ",["$","span",null,{"className":"font-semibold","children":"activation gradient"}],". By contrast, when backpropagating through an operation such as ",["$","$L3",null,{"latex":"Q,K\\mapsto QK","className":"mt-10"}]," in the attention operation, the gradient with respect to ",["$","$L3",null,{"latex":"Q","className":"mt-10"}]," and ",["$","$L3",null,{"latex":"K","className":"mt-10"}]," are both activation gradients, since these are not the gradient of any weights in the neural network, but rather gradients used to compute weight gradients further along backward pass."]}],"$L4"]}]]}]]}]]}]}],"$L5"]}],["$L6","$L7","$L8"],"$L9"]}],"loading":null,"isPartial":false}
4:["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The question ",["$","span",null,{"className":"italic","children":"how to backpropagate through a matrix product"}],"? is made more precise as follows: let ",["$","$L3",null,{"latex":"f:\\mathbb{R}^{mn}\\mathbb\\longrightarrow {R}","className":"mt-10"}]," have continuous partial derivatives of the first order, and let ",["$","$L3",null,{"latex":"g:\\mathbb{R}^{mk}\\times \\mathbb{R}^{kn} \\longrightarrow \\mathbb{R}^{mn}","className":"mt-10"}]," be defined to be the matrix product ",["$","$L3",null,{"latex":"x,y\\mapsto xy","className":"mt-10"}],", when any vector ",["$","$L3",null,{"latex":"x\\in\\mathbb{R}^{mk}","className":"mt-10"}]," is considered as a ",["$","$L3",null,{"latex":"m\\times k","className":"mt-10"}]," matrix, and likewise for ",["$","$L3",null,{"latex":"y","className":"mt-10"}]," and the product element ",["$","$L3",null,{"latex":"xy\\in\\mathbb{R}^{mn}","className":"mt-10"}],". Define ",["$","$L3",null,{"latex":"\\ell = f\\circ g: \\mathbb{R}^{mk}\\times \\mathbb{R}^{kn}\\longrightarrow \\mathbb{R}","className":"mt-10"}],". We want to compute the derivative ",["$","$L3",null,{"latex":"D\\ell","className":"mt-10"}],"."]}]
5:["$","$La",null,{}]
6:["$","script","script-0",{"src":"/_next/static/chunks/988ea292de4c4c73.js","async":true}]
7:["$","script","script-1",{"src":"/_next/static/chunks/e5d291186d0cbb54.js","async":true}]
8:["$","script","script-2",{"src":"/_next/static/chunks/bc166ea53d390db7.js","async":true}]
9:["$","$Lb",null,{"children":["$","$c",null,{"name":"Next.MetadataOutlet","children":"$@d"}]}]
d:null
