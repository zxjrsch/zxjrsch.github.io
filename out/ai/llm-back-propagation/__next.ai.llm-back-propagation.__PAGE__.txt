1:"$Sreact.fragment"
2:I[1565,["/_next/static/chunks/db988e7f896ac083.js","/_next/static/chunks/9b725c66512530e6.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
3:I[96045,["/_next/static/chunks/db988e7f896ac083.js","/_next/static/chunks/9b725c66512530e6.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
25:I[56691,["/_next/static/chunks/db988e7f896ac083.js","/_next/static/chunks/9b725c66512530e6.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
26:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"OutletBoundary"]
27:"$Sreact.suspense"
0:{"buildId":"voNjnk-1CqFxO63BuH1js","rsc":["$","$1","c",{"children":[["$","div",null,{"className":"flex flex-col justify-center min-h-screen","children":[["$","div",null,{"className":"flex flex-row justify-center pt-15","children":["$","$L2",null,{}]}],["$","div",null,{"className":"grow flex justify-center w-full pt-10 md:pt-15 px-3","children":["$","div",null,{"className":"flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl","children":[["$","div",null,{"className":"text-3xl font-bold flex justify-center","children":"LLM Backward Pass"}],["$","div",null,{"className":"text-sm flex justify-center pt-5","children":"Feb 1"}],["$","div",null,{"className":"","children":[["$","div","0",{"children":["$undefined",["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"Modern LLMs architectures have two core computations: the attention operator, which exchanges information between tokens, and multilayer perceptron (aka dense or MoE feedforward layer) which is a position-wise operation. Both these operators have three phases: forward pass, backward pass (used interchangably with backpropagation), and optimization (weight update using gradients)."}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["In MLP forward pass, a fundamental math operation is projection through a layer with weights matrix ",["$","$L3",null,{"latex":"W","className":"mt-10"}]," defined by ",["$","$L3",null,{"latex":"X\\mapsto WX","className":"mt-10"}],". In attention forward pass, there are several matrix multiplications such as ",["$","$L3",null,{"latex":"Q,K\\mapsto QK","className":"mt-10"}],". So the key question for backward pass is ",["$","span",null,{"className":"italic","children":"how to backpropagate through a matrix product"}],"?"]}]]}]]}],["$","div","1",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Activation Gradient and Weight Gradient"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["In LLM forward pass, the ",["$","span",null,{"className":"font-semibold","children":"activation"}]," of one layer ",["$","$L3",null,{"latex":"L_1","className":"mt-10"}]," is defined as the output of that layer and it becomes the input of the next layer ",["$","$L3",null,{"latex":"L_2","className":"mt-10"}],". If ",["$","$L3",null,{"latex":"L_2","className":"mt-10"}]," computes ",["$","$L3",null,{"latex":"X\\mapsto W_2X","className":"mt-10"}]," then ",["$","$L3",null,{"latex":"X","className":"mt-10"}]," is in fact the activation of ",["$","$L3",null,{"latex":"L_1","className":"mt-10"}],". During backwards pass through ",["$","$L3",null,{"latex":"L_2","className":"mt-10"}],", the gradient of the loss with respect to ",["$","$L3",null,{"latex":"W_2","className":"mt-10"}]," needs to be computed. This gradient with respect to the weights is called ",["$","span",null,{"className":"font-semibold","children":"weight gradient"}],". During backpropgation through ",["$","$L3",null,{"latex":"L_2","className":"mt-10"}]," we additionally need to compute the gradient of the loss with respect to ",["$","$L3",null,{"latex":"X","className":"mt-10"}]," in order to later compute the weight gradient associated with ",["$","$L3",null,{"latex":"L_1","className":"mt-10"}],". This gradient with respect to ",["$","$L3",null,{"latex":"X","className":"mt-10"}]," is called ",["$","span",null,{"className":"font-semibold","children":"activation gradient"}],". By contrast, when backpropagating through an operation such as ",["$","$L3",null,{"latex":"Q,K\\mapsto QK","className":"mt-10"}]," in the attention operation, the gradient with respect to ",["$","$L3",null,{"latex":"Q","className":"mt-10"}]," and ",["$","$L3",null,{"latex":"K","className":"mt-10"}]," are both activation gradients, since these are not the gradient of any weights in the neural network, but rather gradients used to compute weight gradients further along backward pass."]}]}]]}],"$L4","$L5","$L6","$L7"]}]]}]}],"$L8"]}],["$L9","$La","$Lb"],"$Lc"]}],"loading":null,"isPartial":false}
4:["$","div","2",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Interlude"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["In LLM literature, the derivative of a real valued function ",["$","$L3",null,{"latex":"\\phi: \\mathbb{R}^{pq}\\rightarrow \\mathbb{R}","className":"mt-10"}]," defined on a set of matrices is defined as the ",["$","$L3",null,{"latex":"p\\times q","className":"mt-10"}]," matrix ",["$","$L3",null,{"latex":"M","className":"mt-10"}]," whose entries are the partial derivatives ",["$","$L3",null,{"latex":"\\partial\\phi/\\partial x_{ij}","className":"mt-10"}],". While the dimension of this matrix is useful for implementing neural network optimization in a framework such as PyTorch, ",["$","$L3",null,{"latex":"M","className":"mt-10"}]," is not the Jacobian matrix (i.e. the mathematical definition of the derivative of a function), and the chain rule is not conveniently expressed in terms of ",["$","$L3",null,{"latex":"M","className":"mt-10"}],". To ensure correctness in our derivations, let us specify a few definitions to make clear this distinction."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Let ",["$","$L3",null,{"latex":"U","className":"mt-10"}]," be an open subset of ",["$","$L3",null,{"latex":"\\mathbb{R}^m","className":"mt-10"}],", and given a mapping ",["$","$L3",null,{"latex":"f:U\\rightarrow \\mathbb{R}^n","className":"mt-10"}],". We say that ",["$","$L3",null,{"latex":"f","className":"mt-10"}]," is differentiable at a point ",["$","$L3",null,{"latex":"a\\in U","className":"mt-10"}]," if there is a ",["$","span",null,{"className":"italic","children":"linear"}]," mapping ",["$","$L3",null,{"latex":"A: \\mathbb{R}^m\\rightarrow \\mathbb{R}^n","className":"mt-10"}],", described by a ",["$","$L3",null,{"latex":"n\\times m","className":"mt-10"}]," matrix ",["$","$L3",null,{"latex":"A","className":"mt-10"}]," such that for all ",["$","$L3",null,{"latex":"h","className":"mt-10"}]," in an open neightbourhood of the origin in ",["$","$L3",null,{"latex":"\\mathbb{R}^m","className":"mt-10"}]," we have ",["$","$L3",null,{"latex":"f(a+h)=f(a)+Ah + \\varepsilon(h)","displayMode":true,"className":"mt-10"}]," where ",["$","$L3",null,{"latex":"\\lim_{h\\to0} \\frac{||\\varepsilon(h)||}{||h||} = 0","displayMode":true,"className":"mt-10"}]," here ",["$","$L3",null,{"latex":"||\\cdot||","className":"mt-10"}]," is  ",["$","$L3",null,{"latex":"||x||_\\infty=\\max_i |x_i|","className":"mt-10"}]," norm or any of its equivalents on ",["$","$L3",null,{"latex":"\\mathbb{R}^m","className":"mt-10"}]," or ",["$","$L3",null,{"latex":"\\mathbb{R}^n","className":"mt-10"}],". If this condition is not satisfied then the function is not differentiable at that point, and a sufficient condition for differentiability of ",["$","$L3",null,{"latex":"f","className":"mt-10"}]," at ",["$","$L3",null,{"latex":"a","className":"mt-10"}]," is the existence and continuity of all first order partial derivatives of ",["$","$L3",null,{"latex":"f","className":"mt-10"}]," at ",["$","$L3",null,{"latex":"a","className":"mt-10"}],". Recall that ",["$","$L3",null,{"latex":"f_i:U\\rightarrow \\mathbb{R}","className":"mt-10"}]," is the ",["$","$L3",null,{"latex":"i","className":"mt-10"}],"-th component function of ",["$","$L3",null,{"latex":"f","className":"mt-10"}],". If the derivative exists then ",["$","$L3",null,{"latex":"A","className":"mt-10"}]," is the ",["$","$L3",null,{"latex":"n\\times m","className":"mt-10"}]," matrix of partial derivatives ",["$","$L3",null,{"latex":"\\frac{\\partial f_i}{ \\partial x_j}","className":"mt-10"}]," and ",["$","$L3",null,{"latex":"A","className":"mt-10"}]," is called the derivative (or Jacobian matrix) of ",["$","$L3",null,{"latex":"f","className":"mt-10"}]," at ",["$","$L3",null,{"latex":"a","className":"mt-10"}]," and is denoted ",["$","$L3",null,{"latex":"A=Df(a)","className":"mt-10"}],". We write ",["$","$L3",null,{"latex":"f\\in \\mathscr{C}^1(U)","className":"mt-10"}]," if ",["$","$L3",null,{"latex":"f","className":"mt-10"}]," has continuous first order partial derivatives everywhere on ",["$","$L3",null,{"latex":"U","className":"mt-10"}],", in which case we also say ","$Ld"," is ","$Le"," smooth when the domain is understood tacitly from the context."]}],"$Lf","$L10"]}]]}]
5:["$","div","3",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Problem Formulation"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The question ",["$","span",null,{"className":"font-semibold","children":"how to backpropagate through a matrix product"}]," is made more precise as follows. Let ",["$","$L3",null,{"latex":"f:\\mathbb{R}^{mn}\\mathbb\\longrightarrow {R}","className":"mt-10"}]," be ",["$","$L3",null,{"latex":"\\mathscr{C}^1","className":"mt-10"}]," smooth, and let ",["$","$L3",null,{"latex":"g:\\mathbb{R}^{mk}\\times \\mathbb{R}^{kn} \\longrightarrow \\mathbb{R}^{mn}","className":"mt-10"}]," be defined to be the matrix product ",["$","$L3",null,{"latex":"x,y\\mapsto xy","className":"mt-10"}],", when any vector ",["$","$L3",null,{"latex":"x\\in\\mathbb{R}^{mk}","className":"mt-10"}]," is considered as a ",["$","$L3",null,{"latex":"m\\times k","className":"mt-10"}]," matrix, and likewise for ",["$","$L3",null,{"latex":"y","className":"mt-10"}]," and the product element ",["$","$L3",null,{"latex":"xy\\in\\mathbb{R}^{mn}","className":"mt-10"}],". The map ",["$","$L3",null,{"latex":"g","className":"mt-10"}]," is smooth because each component is a polynomial in its input. Define ",["$","$L3",null,{"latex":"\\ell = f\\circ g: \\mathbb{R}^{mk}\\times \\mathbb{R}^{kn}\\longrightarrow \\mathbb{R}","className":"mt-10"}],". We can regard ",["$","$L3",null,{"latex":"\\ell = \\ell(x, y)=f(xy)","className":"mt-10"}]," Then we know that  ",["$","$L3",null,{"latex":"D\\ell","className":"mt-10"}]," (the math version derivative) exists and has shape ",["$","$L3",null,{"latex":"1\\times k(m+n)","className":"mt-10"}],". For the ML learning version of the derivative ",["$","$L3",null,{"latex":"D_{ML}","className":"mt-10"}]," we want the reshape ",["$","$L3",null,{"latex":"D\\ell","className":"mt-10"}]," as a block matrix ",["$","$L3",null,{"latex":"D_{ML} = \\begin{bmatrix}A \\\\ B \\end{bmatrix} ","displayMode":true,"className":"mt-10"}]," where ",["$","$L3",null,{"latex":"A","className":"mt-10"}]," is the ",["$","$L3",null,{"latex":"m\\times k","className":"mt-10"}]," matrix of partial derivatives of ",["$","$L3",null,{"latex":"x\\mapsto \\ell(x, y)","className":"mt-10"}]," when ",["$","$L3",null,{"latex":"y","className":"mt-10"}]," is fixed, and ",["$","$L3",null,{"latex":"B","className":"mt-10"}]," is the ",["$","$L3",null,{"latex":"n\\times k","className":"mt-10"}]," matrix of partial derivatives of ",["$","$L3",null,{"latex":"y\\mapsto \\ell(x, y)","className":"mt-10"}]," when ",["$","$L3",null,{"latex":"x","className":"mt-10"}]," is fixed. We thus introduce the matrix derivative notation (i.e. this is the ML version of the derivative) ",["$","$L3",null,{"latex":"\\frac{\\partial \\ell}{dx}=\\left[\\frac{\\partial \\ell}{dx_{ij}}\\right]=A","displayMode":true,"className":"mt-10"}],"  and"]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["$","$L3",null,{"latex":"\\frac{\\partial \\ell}{dy}=\\left[\\frac{\\partial \\ell}{dy_{st}}\\right]=B","displayMode":true,"className":"mt-10"}]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The logic behind shaping the gradients this way is as follows. We enumerate the layers of a nerual network in the direction of forward pass ",["$","$L3",null,{"latex":"\\{L_k\\}_{k=1}^N","className":"mt-10"}],", where the first layer ",["$","$L3",null,{"latex":"L_1","className":"mt-10"}]," receives the inputs ",["$","$L3",null,{"latex":"\\xi","className":"mt-10"}]," (tokens say), and the final layer ",["$","$L3",null,{"latex":"L_N","className":"mt-10"}]," has loss as output. For intermediate layers ",["$","$L3",null,{"latex":"L_k","className":"mt-10"}]," takes as input the activation of the previous layer ",["$","$L3",null,{"latex":"X=L_{k-1}\\circ\\dots\\circ L_1(\\xi)","className":"mt-10"}]," and weight ",["$","$L3",null,{"latex":"W","className":"mt-10"}]," and computes ",["$","$L3",null,{"latex":"g(W,X)=WX","className":"mt-10"}],". This is the activate of ",["$","$L3",null,{"latex":"L_k","className":"mt-10"}]," which becomes the input of subsequent layers until the loss is computed as ","$L11"," where ","$L12",". Therefore ","$L13"," is the weight gradient and ","$L14"," is the activation gradient."]}]]}]]}]
6:["$","div","4",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Derivation"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The hardest part was to formulate the problem precisely as above. The solution is rather straight forward. Using our notation, backward gradient computation happens from ",["$","$L3",null,{"latex":"L_N","className":"mt-10"}]," to ",["$","$L3",null,{"latex":"L_1","className":"mt-10"}],". Focus on an intermediate layer ",["$","$L3",null,{"latex":"L_n","className":"mt-10"}]," with ",["$","$L3",null,{"latex":"1<n<N","className":"mt-10"}]," that computes ",["$","$L3",null,{"latex":"g","className":"mt-10"}]," as the forward pass, we have ",["$","$L3",null,{"latex":"D(f\\circ g)(x, y) = Df(xy)Dg(x,y)","displayMode":true,"className":"mt-10"}]," where ",["$","$L3",null,{"latex":"Df(xy)","className":"mt-10"}]," is the activation gradient computed during backward pass through ",["$","$L3",null,{"latex":"L_{n+1}","className":"mt-10"}],". So we first need to compute ",["$","$L3",null,{"latex":"Dg(x,y)","className":"mt-10"}],"."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Recall that ",["$","$L3",null,{"latex":"x","className":"mt-10"}]," is an ",["$","$L3",null,{"latex":"m\\times k","className":"mt-10"}]," matrix and ",["$","$L3",null,{"latex":"y","className":"mt-10"}]," is a ",["$","$L3",null,{"latex":"k \\times n","className":"mt-10"}],". For all ",["$","$L3",null,{"latex":"1\\leq a \\leq m","className":"mt-10"}]," and ",["$","$L3",null,{"latex":"1\\leq b\\leq n","className":"mt-10"}]," we have (using einsum) ",["$","$L3",null,{"latex":"g_{ab} = x_{a\\mu}y_{\\mu b}\\quad","displayMode":true,"className":"mt-10"}]," For all ",["$","$L3",null,{"latex":"1\\leq i \\leq m","className":"mt-10"}]," and ",["$","$L3",null,{"latex":"1\\leq j \\leq k","className":"mt-10"}]," ",["$","$L3",null,{"latex":"\\frac{\\partial g_{ab}}{\\partial x_{ij}}=\\delta^a_i y_{jb}","displayMode":true,"className":"mt-10"}]," and for all ",["$","$L3",null,{"latex":"1\\leq s \\leq k","className":"mt-10"}]," and ",["$","$L3",null,{"latex":"1\\leq t \\leq n","className":"mt-10"}]," ",["$","$L3",null,{"latex":"\\frac{\\partial g_{ab}}{\\partial y_{st}}=\\delta^b_t x_{as}","displayMode":true,"className":"mt-10"}]," where ",["$","$L3",null,{"latex":"\\delta^j_i=1","className":"mt-10"}]," when ",["$","$L3",null,{"latex":"i=j","className":"mt-10"}]," and zero otherwise. Using the componentwise chain rule defined above, the components of ",["$","$L3",null,{"latex":"D\\ell = D(f\\circ g)(x, y)","className":"mt-10"}]," are ",["$","$L3",null,{"latex":"\\frac{\\partial \\ell}{\\partial x_{ij}} = \\frac{\\partial \\ell}{\\partial g_{ab}}\\frac{\\partial g_{ab}}{\\partial x_{ij}}= \\frac{\\partial \\ell}{\\partial g_{ab}}\\delta^a_i y_{jb} = \\frac{\\partial \\ell}{\\partial g_{ib}}y_{jb} ","displayMode":true,"className":"mt-10"}]," and ",["$","$L3",null,{"latex":"\\frac{\\partial \\ell}{\\partial y_{st}} = \\frac{\\partial \\ell}{\\partial g_{ab}}\\frac{\\partial g_{ab}}{\\partial y_{st}}= \\frac{\\partial \\ell}{\\partial g_{ab}}\\delta^b_t x_{as} = \\frac{\\partial \\ell}{\\partial g_{at}} x_{as}","displayMode":true,"className":"mt-10"}]," The above two lines specify how components are computed, in matrix notation ",["$","$L3",null,{"latex":"A=\\frac{\\partial \\ell}{\\partial x} = \\frac{\\partial \\ell}{\\partial g}y^\\top ","displayMode":true,"className":"mt-10"}]," and ",["$","$L3",null,{"latex":"B=\\frac{\\partial \\ell}{\\partial y} = x^\\top \\frac{\\partial \\ell}{\\partial g}","displayMode":true,"className":"mt-10"}]," We can even sanity check the matrix shapes agree."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["That's it! In reflection, the backward pass derivation takes just two lines, one for the activation gradient and one for the weight gradient. These gradients at layer ",["$","$L3",null,{"latex":"L_n","className":"mt-10"}]," are computed using the transpose of the input matrices ","$L15"," which needs to be kept in memory or recomputed during backward, as well as the activation gradient ","$L16"," from the backward pass through ","$L17","."]}]]}]]}]
7:["$","div","5",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Backwards Pass Through Attention"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Let us look at how to backward through scaled dot product attention ",["$","$L3",null,{"latex":"S=QK^\\top / \\sqrt{d}","displayMode":true,"className":"mt-10"}]," Softmax is applied over the rows (dim ",["$","$L3",null,{"latex":"= -1","className":"mt-10"}],") of ",["$","$L3",null,{"latex":"S","className":"mt-10"}],". Thus let ",["$","$L3",null,{"latex":"P_i","className":"mt-10"}]," be row ",["$","$L3",null,{"latex":"i","className":"mt-10"}]," in the matrix ",["$","$L3",null,{"latex":"P","className":"mt-10"}]," with the same shape as ",["$","$L3",null,{"latex":"S","className":"mt-10"}]," then ",["$","$L3",null,{"latex":"P_i=\\text{softmax}(S_{ij}),  \\quad P_{ij} = \\frac{e^{S_{ij}}}{\\sum_k e^{S_{ik}}}","displayMode":true,"className":"mt-10"}]," Each row of ",["$","$L3",null,{"latex":"P","className":"mt-10"}]," becomes a probability distribution which is used as coefficients in a linear combination of the corresponding rows of the value matrix ",["$","$L3",null,{"latex":"V","className":"mt-10"}]," ",["$","$L3",null,{"latex":"O=PV","displayMode":true,"className":"mt-10"}]," There are no weights, so we compute activation gradients, which we can compute using the formula for ",["$","$L3",null,{"latex":"A","className":"mt-10"}]," and ",["$","$L3",null,{"latex":"B","className":"mt-10"}]," we derived in the previous section ",["$","$L3",null,{"latex":"\\frac{\\partial \\ell}{\\partial P}= \\frac{\\partial \\ell}{\\partial O}V^\\top","displayMode":true,"className":"mt-10"}]," ",["$","$L3",null,{"latex":"\\frac{\\partial \\ell}{\\partial V}= P^\\top\\frac{\\partial \\ell}{\\partial O}","displayMode":true,"className":"mt-10"}]," To see how to differentiate through softmax, let ",["$","$L3",null,{"latex":"g:\\mathbb{R}^{mn}\\rightarrow \\mathbb{R}^{mn}","className":"mt-10"}]," be the row-wise softmax function on ",["$","$L3",null,{"latex":"m\\times n","className":"mt-10"}]," matrices ",["$","$L3",null,{"latex":"x\\in \\mathbb{R}^{mn}","className":"mt-10"}],". Let ",["$","$L3",null,{"latex":"f:\\mathbb{R}^{mn}\\rightarrow \\mathbb{R}","className":"mt-10"}]," be the rest of the forward pass after ",["$","$L3",null,{"latex":"g","className":"mt-10"}]," that results in the loss ",["$","$L3",null,{"latex":"\\ell = f\\circ g: \\mathbb{R}^{mn} \\rightarrow \\mathbb{R}","className":"mt-10"}],". Observe that the components in ",["$","$L3",null,{"latex":"Df","className":"mt-10"}]," are precisely the components in the activation gradient matrix ",["$","$L3",null,{"latex":"\\partial \\ell / \\partial P","className":"mt-10"}]," we derived above ",["$","$L3",null,{"latex":"\\frac{\\partial \\ell}{\\partial g_{rc}}=\\left(\\frac{\\partial \\ell}{\\partial P}\\right)_{rc}","displayMode":true,"className":"mt-10"}]," where the right side denotes the matrix entry in row ",["$","$L3",null,{"latex":"r","className":"mt-10"}]," and column ",["$","$L3",null,{"latex":"c","className":"mt-10"}],". The partial derivatives of ",["$","$L3",null,{"latex":"g","className":"mt-10"}]," the row-wise softmax are"]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["$","$L3",null,{"latex":"\\begin{align*}\\frac{\\partial g_{rc}}{\\partial x_{ij}} &= \\frac{\\partial }{\\partial x_{ij}}\\left(\\frac{e^{x_{rc}}}{\\sum_{k} e^{x_{rk}}}\\right) \\\\ &= \\delta_i^r \\times \\frac{\\delta_j^c\\exp(x_{rj})\\sum_{k} \\exp{(x_{rk})}-\\exp(x_{rc})\\exp(x_{rj})}{\\left[\\sum_{k} \\exp{(x_{rk})}\\right]^2}\\\\ &= \\delta_i^r \\cdot g_{rj}(x)\\cdot\\left(\\delta_j^c\\, - g_{rc}(x)\\right)\\end{align*}","displayMode":true,"className":"mt-10"}]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["In particular this says ",["$","$L3",null,{"latex":"\\frac{\\partial g_{rc}}{\\partial x_{ij}}=0\\quad if\\, i\\neq r","displayMode":true,"className":"mt-10"}]," otherwise ","$L18"," This can be restated as follows, if ","$L19"," is a vector, and ","$L1a"," then its derivative (Jacobian) is ","$L1b"," Returning to calculating ","$L1c"," using the componentwise chain rule we defined above ","$L1d"," Thus ","$L1e"," where ","$L1f"," is the ","$L20","-th row of the matrix ","$L21",". As a sanity check let us inspect the dimentions ","$L22"," Finally the remaining backward pass are differentiation through a matrix product, and using the formula we derived earlier, we have ","$L23"," ","$L24"]}]]}]]}]
8:["$","$L25",null,{}]
9:["$","script","script-0",{"src":"/_next/static/chunks/db988e7f896ac083.js","async":true}]
a:["$","script","script-1",{"src":"/_next/static/chunks/9b725c66512530e6.js","async":true}]
b:["$","script","script-2",{"src":"/_next/static/chunks/5ad9eb95768fc0a4.js","async":true}]
c:["$","$L26",null,{"children":["$","$27",null,{"name":"Next.MetadataOutlet","children":"$@28"}]}]
d:["$","$L3",null,{"latex":"f","className":"mt-10"}]
e:["$","$L3",null,{"latex":"\\mathscr{C}^1","className":"mt-10"}]
f:["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Chain rule says that if ",["$","$L3",null,{"latex":"U, V, W","className":"mt-10"}]," are open subset of Euclidean spaces (possibly of different dimension), and ",["$","$L3",null,{"latex":"f: U \\rightarrow V","className":"mt-10"}]," and ",["$","$L3",null,{"latex":"g:V\\rightarrow W","className":"mt-10"}]," satisfies smoothness conditions ",["$","$L3",null,{"latex":"f\\in\\mathscr{C}^1(U)","className":"mt-10"}]," ",["$","$L3",null,{"latex":"g\\in\\mathscr{C}^1(V)","className":"mt-10"}],". Then ",["$","$L3",null,{"latex":"g\\circ f: U \\rightarrow W","className":"mt-10"}]," is differentiable at all points ",["$","$L3",null,{"latex":"x\\in U","className":"mt-10"}]," and the derivative (i.e. Jacobian, math version) is given by the matrix product ",["$","$L3",null,{"latex":"D(g\\circ f)(x) = (Dg\\circ f)(x)Df(x)","displayMode":true,"className":"mt-10"}]]}]
10:["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Let us compare the ML version and the math version for the derivative of a matrix input, real valued function  ",["$","$L3",null,{"latex":"\\phi: \\mathbb{R}^{pq}\\rightarrow \\mathbb{R}","className":"mt-10"}],". The ML version is a matrix of partial derivatives with size ",["$","$L3",null,{"latex":"m\\times n","className":"mt-10"}]," matching the input. The Jacobian has shape ",["$","$L3",null,{"latex":"1\\times mn","className":"mt-10"}],". In order to apply the chain rule to the ML version, it is convenient to use the special case of the chain rule where ",["$","$L3",null,{"latex":"f:\\mathbb{R}^m\\rightarrow \\mathbb{R}^n","className":"mt-10"}]," and ",["$","$L3",null,{"latex":"g:\\mathbb{R}^n\\rightarrow \\mathbb{R}","className":"mt-10"}]," and ",["$","$L3",null,{"latex":"\\phi = g \\circ f","className":"mt-10"}],". This is called the ",["$","span",null,{"className":"font-semibold","children":"component-wise chain rule"}]," and it says ",["$","$L3",null,{"latex":"\\begin{bmatrix}\\frac{\\partial\\phi}{\\partial x_1} & \\dots & \\frac{\\partial\\phi}{\\partial x_m}\\end{bmatrix} = \\begin{bmatrix}\\frac{\\partial g}{\\partial x_1} & \\dots & \\frac{\\partial g}{\\partial x_n}\\end{bmatrix}\\begin{bmatrix}\\frac{\\partial f_1}{\\partial x_1} & \\dots & \\frac{\\partial f_1}{\\partial x_m} \\\\ \\vdots & & \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} & \\dots & \\frac{\\partial f_n}{\\partial x_m}\\end{bmatrix}","displayMode":true,"className":"mt-10"}]," In particular for all ",["$","$L3",null,{"latex":"1\\leq k\\leq m","className":"mt-10"}],"  we have ",["$","$L3",null,{"latex":"\\frac{\\partial \\phi}{\\partial x_i} = \\sum_{k=1}^n \\frac{\\partial g}{\\partial x_k} \\frac{\\partial f_k}{\\partial x_i}","displayMode":true,"className":"mt-10"}]," Sometimes we will use the ",["$","span",null,{"className":"font-semibold","children":"Einstein summation notation"}]," to express the above as ",["$","$L3",null,{"latex":"\\frac{\\partial \\phi}{\\partial x_i} = \\frac{\\partial g}{\\partial x_k} \\frac{\\partial f_k}{\\partial x_i}\\quad (\\text{einsum})","displayMode":true,"className":"mt-10"}]," where the sum symbol is omitted in the expression on the right for clarity and it is understood that summation occurs over all repeated indices within the same expression (",["$","$L3",null,{"latex":"k","className":"mt-10"}]," in this case)."]}]
11:["$","$L3",null,{"latex":"\\ell = f\\circ g","className":"mt-10"}]
12:["$","$L3",null,{"latex":"f=L_{N}\\circ\\dots \\circ L_{k-1}","className":"mt-10"}]
13:["$","$L3",null,{"latex":"A","className":"mt-10"}]
14:["$","$L3",null,{"latex":"B","className":"mt-10"}]
15:["$","$L3",null,{"latex":"x, y","className":"mt-10"}]
16:["$","$L3",null,{"latex":"\\partial \\ell / \\partial g","className":"mt-10"}]
17:["$","$L3",null,{"latex":"L_{n+1}","className":"mt-10"}]
18:["$","$L3",null,{"latex":"\\frac{\\partial g_{rc}}{\\partial x_{rj}}= g_{rj}(x)\\cdot\\left(\\delta_j^c\\, - g_{rc}(x)\\right)","displayMode":true,"className":"mt-10"}]
19:["$","$L3",null,{"latex":"x","className":"mt-10"}]
1a:["$","$L3",null,{"latex":"y=\\text{softmax}(x)","className":"mt-10"}]
1b:["$","$L3",null,{"latex":"Dy = \\text{diag}(y)-yy^\\top","displayMode":true,"className":"mt-10"}]
1c:["$","$L3",null,{"latex":"\\partial \\ell / \\partial x_{ij}","className":"mt-10"}]
1d:["$","$L3",null,{"latex":"\\frac{\\partial \\ell}{\\partial x_{ij}}=\\sum_{r,c}\\frac{\\partial \\ell}{\\partial g_{rc}}\\frac{\\partial g_{rc}}{\\partial x_{ij}}=\\sum_{c} \\left(\\frac{\\partial \\ell}{\\partial P}\\right)_{ic}\\frac{\\partial g_{ic}}{\\partial x_{ij}}=\\sum_{c} \\left(\\frac{\\partial \\ell}{\\partial P}\\right)_{ic} \\left[\\text{diag}(g_i)-g_ig_i^\\top\\right]_{cj}","displayMode":true,"className":"mt-10"}]
1e:["$","$L3",null,{"latex":"\\left(\\frac{\\partial \\ell}{\\partial S}\\right)_i=\\left(\\frac{\\partial \\ell}{\\partial x}\\right)_i= \\left(\\frac{\\partial \\ell}{\\partial P}\\right)_i \\left(\\text{diag}(g_i)-g_ig_i^\\top\\right)","displayMode":true,"className":"mt-10"}]
1f:["$","$L3",null,{"latex":"M_i","className":"mt-10"}]
20:["$","$L3",null,{"latex":"i","className":"mt-10"}]
21:["$","$L3",null,{"latex":"M","className":"mt-10"}]
22:["$","$L3",null,{"latex":"\\text{Shape}(1\\times n) = \\text{Shape}(1\\times n) \\times \\text{Shape}(n\\times n)","displayMode":true,"className":"mt-10"}]
23:["$","$L3",null,{"latex":"\\frac{\\partial \\ell}{\\partial Q}= \\frac{\\partial \\ell}{\\partial S}K/\\sqrt{d}","displayMode":true,"className":"mt-10"}]
24:["$","$L3",null,{"latex":"\\frac{\\partial \\ell}{\\partial K}= \\frac{\\partial \\ell}{\\partial S}^\\top Q/\\sqrt{d}","displayMode":true,"className":"mt-10"}]
28:null
