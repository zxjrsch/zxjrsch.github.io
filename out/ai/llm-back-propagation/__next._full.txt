1:"$Sreact.fragment"
2:I[48523,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"default"]
3:I[82281,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"default"]
4:I[1565,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/e5d291186d0cbb54.js","/_next/static/chunks/bc166ea53d390db7.js"],"default"]
5:I[96045,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/e5d291186d0cbb54.js","/_next/static/chunks/bc166ea53d390db7.js"],"default"]
d:I[75067,[],"default"]
:HL["/_next/static/chunks/e23ef66a1dfd4af0.css","style"]
:HL["/_next/static/chunks/4057cc9dbcc744c0.css","style"]
:HL["/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
0:{"P":null,"b":"wZ6h8tbQjuConBuVR56rU","c":["","ai","llm-back-propagation"],"q":"","i":false,"f":[[["",{"children":["ai",{"children":["llm-back-propagation",{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/e23ef66a1dfd4af0.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/chunks/4057cc9dbcc744c0.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased","children":["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[["$","div",null,{"className":"flex flex-col justify-center min-h-screen","children":[["$","div",null,{"className":"flex flex-row justify-center pt-15","children":["$","$L4",null,{}]}],["$","div",null,{"className":"grow flex justify-center w-full pt-10 md:pt-15 px-3","children":["$","div",null,{"className":"flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl","children":[["$","div",null,{"className":"text-3xl font-bold flex justify-center","children":"LLM Backward Pass"}],["$","div",null,{"className":"text-sm flex justify-center pt-5","children":"Feb 1"}],["$","div",null,{"className":"","children":[["$","div","0",{"children":["$undefined",["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"Modern LLMs architectures have two core computations: the attention operator, which exchanges information between tokens, and multilayer perceptron (aka dense or MoE feedforward layer) which is a position-wise operation. Both these operators have three phases: forward pass, backward pass (used interchangably with backpropagation), and optimization (weight update using gradients)."}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["In MLP forward pass, a fundamental math operation is projection through a layer with weights matrix ",["$","$L5",null,{"latex":"W","className":"mt-10"}]," defined by ",["$","$L5",null,{"latex":"X\\mapsto WX","className":"mt-10"}],". In attention forward pass, there are several matrix multiplications such as ",["$","$L5",null,{"latex":"Q,K\\mapsto QK","className":"mt-10"}],". So the key question for backward pass is ",["$","span",null,{"className":"italic","children":"how to backpropagate through a matrix product"}],"?"]}]]}]]}],"$L6"]}]]}]}],"$L7"]}],["$L8","$L9","$La"],"$Lb"]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],"$Lc",false]],"m":"$undefined","G":["$d",[]],"S":true}
13:I[56691,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/e5d291186d0cbb54.js","/_next/static/chunks/bc166ea53d390db7.js"],"default"]
14:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"OutletBoundary"]
15:"$Sreact.suspense"
17:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"ViewportBoundary"]
19:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"MetadataBoundary"]
6:["$","div","1",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Activation Gradient and Weight Gradient"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["In LLM forward pass, the ",["$","span",null,{"className":"font-semibold","children":"activation"}]," of one layer ",["$","$L5",null,{"latex":"L_1","className":"mt-10"}]," is defined as the output of that layer and it becomes the input of the next layer ",["$","$L5",null,{"latex":"L_2","className":"mt-10"}],". If ",["$","$L5",null,{"latex":"L_2","className":"mt-10"}]," computes ",["$","$L5",null,{"latex":"X\\mapsto W_2X","className":"mt-10"}]," then ",["$","$L5",null,{"latex":"X","className":"mt-10"}]," is in fact the activation of ",["$","$L5",null,{"latex":"L_1","className":"mt-10"}],". During backwards pass through ",["$","$L5",null,{"latex":"L_2","className":"mt-10"}],", the gradient of the loss with respect to ",["$","$L5",null,{"latex":"W_2","className":"mt-10"}]," needs to be computed. This gradient with respect to the weights is called ",["$","span",null,{"className":"font-semibold","children":"weight gradient"}],". During backpropgation through ",["$","$L5",null,{"latex":"L_2","className":"mt-10"}]," we additionally need to compute the gradient of the loss with respect to ",["$","$L5",null,{"latex":"X","className":"mt-10"}]," in order to later compute the weight gradient associated with ",["$","$L5",null,{"latex":"L_1","className":"mt-10"}],". This gradient with respect to ",["$","$L5",null,{"latex":"X","className":"mt-10"}]," is called ",["$","span",null,{"className":"font-semibold","children":"activation gradient"}],". By contrast, when backpropagating through an operation such as ",["$","$L5",null,{"latex":"Q,K\\mapsto QK","className":"mt-10"}]," in the attention operation, the gradient with respect to ",["$","$L5",null,{"latex":"Q","className":"mt-10"}]," and ",["$","$L5",null,{"latex":"K","className":"mt-10"}]," are both activation gradients, since these are not the gradient of any weights in the neural network, but rather gradients used to compute weight gradients further along backward pass."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The question ",["$","span",null,{"className":"italic","children":"how to backpropagate through a matrix product"}],"? is made more precise as follows: let ",["$","$L5",null,{"latex":"f:\\mathbb{R}^{mn}\\mathbb\\longrightarrow {R}","className":"mt-10"}]," have continuous partial derivatives of the first order, and let ",["$","$L5",null,{"latex":"g:\\mathbb{R}^{mk}\\times \\mathbb{R}^{kn} \\longrightarrow \\mathbb{R}^{mn}","className":"mt-10"}]," be defined to be the matrix product ",["$","$L5",null,{"latex":"x,y\\mapsto xy","className":"mt-10"}],", when any vector ",["$","$L5",null,{"latex":"x\\in\\mathbb{R}^{mk}","className":"mt-10"}]," is considered as a ",["$","$L5",null,{"latex":"m\\times k","className":"mt-10"}]," matrix, and likewise for ",["$","$L5",null,{"latex":"y","className":"mt-10"}]," and the product element ",["$","$L5",null,{"latex":"xy\\in\\mathbb{R}^{mn}","className":"mt-10"}],". Define ",["$","$L5",null,{"latex":"\\ell = f\\circ g: \\mathbb{R}^{mk}\\times \\mathbb{R}^{kn}\\longrightarrow \\mathbb{R}","className":"mt-10"}],". We want to compute the derivative ",["$","$L5",null,{"latex":"D\\ell","className":"mt-10"}],"."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["In LLM literature, the derivative of a real valued function ",["$","$L5",null,{"latex":"\\phi: \\mathbb{R}^{pq}\\rightarrow \\mathbb{R}","className":"mt-10"}]," defined on a set of matrices is defined as the ",["$","$L5",null,{"latex":"p\\times q","className":"mt-10"}]," matrix ",["$","$L5",null,{"latex":"M","className":"mt-10"}]," whose entries are the partial derivatives ",["$","$L5",null,{"latex":"\\partial\\phi/\\partial x_{ij}","className":"mt-10"}],". While the dimension of this matrix is useful for implementing neural network optimization in a framework such as PyTorch, ","$Le"," is not the Jacobian matrix (i.e. the mathematical definition of the derivative of a function), and the chain rule is not conveniently expressed in terms of ","$Lf",". To ensure correctness in our derivations, let us specify a few definitions to make clear this distinction."]}],"$L10","$L11","$L12"]}]]}]
7:["$","$L13",null,{}]
8:["$","script","script-0",{"src":"/_next/static/chunks/988ea292de4c4c73.js","async":true,"nonce":"$undefined"}]
9:["$","script","script-1",{"src":"/_next/static/chunks/e5d291186d0cbb54.js","async":true,"nonce":"$undefined"}]
a:["$","script","script-2",{"src":"/_next/static/chunks/bc166ea53d390db7.js","async":true,"nonce":"$undefined"}]
b:["$","$L14",null,{"children":["$","$15",null,{"name":"Next.MetadataOutlet","children":"$@16"}]}]
c:["$","$1","h",{"children":[null,["$","$L17",null,{"children":"$L18"}],["$","div",null,{"hidden":true,"children":["$","$L19",null,{"children":["$","$15",null,{"name":"Next.Metadata","children":"$L1a"}]}]}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]
e:["$","$L5",null,{"latex":"M","className":"mt-10"}]
f:["$","$L5",null,{"latex":"M","className":"mt-10"}]
10:["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Let ",["$","$L5",null,{"latex":"U","className":"mt-10"}]," be an open subset of ",["$","$L5",null,{"latex":"\\mathbb{R}^m","className":"mt-10"}],", and given a mapping ",["$","$L5",null,{"latex":"f:U\\rightarrow \\mathbb{R}^n","className":"mt-10"}],". We say that ",["$","$L5",null,{"latex":"f","className":"mt-10"}]," is differentiable at a point ",["$","$L5",null,{"latex":"a\\in U","className":"mt-10"}]," if there is a ",["$","span",null,{"className":"italic","children":"linear"}]," mapping ",["$","$L5",null,{"latex":"A: \\mathbb{R}^m\\rightarrow \\mathbb{R}^n","className":"mt-10"}],", described by a ",["$","$L5",null,{"latex":"n\\times m","className":"mt-10"}]," matrix ",["$","$L5",null,{"latex":"A","className":"mt-10"}]," such that for all ",["$","$L5",null,{"latex":"h","className":"mt-10"}]," in an open neightbourhood of the origin in ",["$","$L5",null,{"latex":"\\mathbb{R}^m","className":"mt-10"}]," we have ",["$","$L5",null,{"latex":"f(a+h)=f(a)+Ah + \\varepsilon(h)","displayMode":true,"className":"mt-10"}]," where ",["$","$L5",null,{"latex":"\\lim_{h\\to0} \\frac{||\\varepsilon(h)||}{||h||} = 0","displayMode":true,"className":"mt-10"}]," here ",["$","$L5",null,{"latex":"||\\cdot||","className":"mt-10"}]," is  ",["$","$L5",null,{"latex":"||x||_\\infty=\\max_i |x_i|","className":"mt-10"}]," norm or any of its equivalents on ",["$","$L5",null,{"latex":"\\mathbb{R}^m","className":"mt-10"}]," or ",["$","$L5",null,{"latex":"\\mathbb{R}^n","className":"mt-10"}],". If this condition is not satisfied then the function is not differentiable at that point, and a sufficient condition for differentiability of ",["$","$L5",null,{"latex":"f","className":"mt-10"}]," at ",["$","$L5",null,{"latex":"a","className":"mt-10"}]," is the existence and continuity of all first order partial derivatives of ",["$","$L5",null,{"latex":"f","className":"mt-10"}]," at ",["$","$L5",null,{"latex":"a","className":"mt-10"}],". Recall that ",["$","$L5",null,{"latex":"f_i:U\\rightarrow \\mathbb{R}","className":"mt-10"}]," is the ",["$","$L5",null,{"latex":"i","className":"mt-10"}],"-th component function of ",["$","$L5",null,{"latex":"f","className":"mt-10"}],". If the derivative exists then ",["$","$L5",null,{"latex":"A","className":"mt-10"}]," is the ",["$","$L5",null,{"latex":"n\\times m","className":"mt-10"}]," matrix of partial derivatives ",["$","$L5",null,{"latex":"\\frac{\\partial f_i}{ \\partial x_j}","className":"mt-10"}]," and ",["$","$L5",null,{"latex":"A","className":"mt-10"}]," is called the derivative (or Jacobian matrix) of ",["$","$L5",null,{"latex":"f","className":"mt-10"}]," at ",["$","$L5",null,{"latex":"a","className":"mt-10"}]," and is denoted ",["$","$L5",null,{"latex":"A=Df(a)","className":"mt-10"}],". We write ",["$","$L5",null,{"latex":"f\\in \\mathscr{C}^1(U)","className":"mt-10"}]," if ",["$","$L5",null,{"latex":"f","className":"mt-10"}]," has continuous first order partial derivatives everywhere on ",["$","$L5",null,{"latex":"U","className":"mt-10"}],"."]}]
11:["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Chain rule says that if ",["$","$L5",null,{"latex":"U, V, W","className":"mt-10"}]," are open subset of Euclidean spaces (possibly of different dimension), and ",["$","$L5",null,{"latex":"f: U \\rightarrow V","className":"mt-10"}]," and ",["$","$L5",null,{"latex":"g:V\\rightarrow W","className":"mt-10"}]," satisfies smoothness conditions ",["$","$L5",null,{"latex":"f\\in\\mathscr{C}^1(U)","className":"mt-10"}]," ",["$","$L5",null,{"latex":"g\\in\\mathscr{C}^1(V)","className":"mt-10"}],". Then ",["$","$L5",null,{"latex":"g\\circ f: U \\rightarrow W","className":"mt-10"}]," is differentiable at all points ",["$","$L5",null,{"latex":"x\\in U","className":"mt-10"}]," and the derivative (i.e. Jacobian, math version) is given by the matrix product ",["$","$L5",null,{"latex":"D(g\\circ f)(x) = (Dg\\circ f)(x)Df(x)","displayMode":true,"className":"mt-10"}]]}]
12:["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Let us compare the ML version and the math version for the derivative of a matrix input, real valued function  ",["$","$L5",null,{"latex":"\\phi: \\mathbb{R}^{pq}\\rightarrow \\mathbb{R}","className":"mt-10"}],". The ML version is a matrix of partial derivatives with size ",["$","$L5",null,{"latex":"m\\times n","className":"mt-10"}]," matching the input. The Jacobian has shape ",["$","$L5",null,{"latex":"1\\times mn","className":"mt-10"}],". In order to apply the chain rule to the ML version, it is convenient to use the special case of the chain rule where ",["$","$L5",null,{"latex":"f:\\mathbb{R}^m\\rightarrow \\mathbb{R}^n","className":"mt-10"}]," and ",["$","$L5",null,{"latex":"g:\\mathbb{R}^n\\rightarrow \\mathbb{R}","className":"mt-10"}]," and ",["$","$L5",null,{"latex":"\\phi = g \\circ f","className":"mt-10"}],". The chain rule says ",["$","$L5",null,{"latex":"\\begin{bmatrix}\\frac{\\partial\\phi}{\\partial x_1} & \\dots & \\frac{\\partial\\phi}{\\partial x_m}\\end{bmatrix} = \\begin{bmatrix}\\frac{\\partial g}{\\partial x_1} & \\dots & \\frac{\\partial g}{\\partial x_n}\\end{bmatrix}\\begin{bmatrix}\\frac{\\partial f_1}{\\partial x_1} & \\dots & \\frac{\\partial f_1}{\\partial x_m} \\\\ \\vdots & & \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} & \\dots & \\frac{\\partial f_n}{\\partial x_m}\\end{bmatrix}","displayMode":true,"className":"mt-10"}]," In particular for all ",["$","$L5",null,{"latex":"1\\leq k\\leq m","className":"mt-10"}],"  we have ",["$","$L5",null,{"latex":"\\frac{\\partial \\phi}{\\partial x_i} = \\sum_{k=1}^n \\frac{\\partial g}{\\partial x_k} \\frac{\\partial f_k}{\\partial x_i}","displayMode":true,"className":"mt-10"}]," Sometimes we will use the ",["$","span",null,{"className":"font-semibold","children":"Einstein summation notation"}]," to express the above as ",["$","$L5",null,{"latex":"\\frac{\\partial \\phi}{\\partial x_i} = \\frac{\\partial g}{\\partial x_k} \\frac{\\partial f_k}{\\partial x_i}\\quad (\\text{einsum})","displayMode":true,"className":"mt-10"}]," where the sum symbol is omitted in the expression on the right for clarity and it is understood that summation occurs over all repeated indices within the same expression (",["$","$L5",null,{"latex":"k","className":"mt-10"}]," in this case)."]}]
18:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
1b:I[87718,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"IconMark"]
16:null
1a:[["$","title","0",{"children":"Claire Zhao Blog"}],["$","meta","1",{"name":"description","content":"Claire Zhao's Blog on AI and Math"}],["$","link","2",{"rel":"icon","href":"/favicon.ico?favicon.e10ff5c7.ico","sizes":"1176x1032","type":"image/x-icon"}],["$","$L1b","3",{}]]
