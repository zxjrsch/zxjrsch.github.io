1:"$Sreact.fragment"
2:I[48523,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"default"]
3:I[82281,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"default"]
4:I[1565,["/_next/static/chunks/e02326bdb730da03.js","/_next/static/chunks/bc166ea53d390db7.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
1b:I[75067,[],"default"]
:HL["/_next/static/chunks/1fcb9d377ba4db76.css","style"]
:HL["/_next/static/chunks/4057cc9dbcc744c0.css","style"]
:HL["/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
0:{"P":null,"b":"JuMVSYJ3RIXYyuKTjUJ0h","c":["","ai","cute-dsl"],"q":"","i":false,"f":[[["",{"children":["ai",{"children":["cute-dsl",{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/1fcb9d377ba4db76.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/chunks/4057cc9dbcc744c0.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased","children":["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[["$","div",null,{"className":"flex flex-col justify-center min-h-screen","children":[["$","div",null,{"className":"flex flex-row justify-center pt-15","children":["$","$L4",null,{}]}],["$","div",null,{"className":"grow flex justify-center w-full pt-10 md:pt-15 px-3","children":["$","div",null,{"className":"flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl","children":[["$","div",null,{"className":"text-3xl font-bold flex justify-center","children":"Programming Blackwell Tensor Core with Cutlass and CuTe DSL"}],["$","div",null,{"className":"text-sm flex justify-center pt-5","children":null}],["$","div",null,{"className":"","children":[["$","div","0",{"children":["$undefined",["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The CuTe ",["$","span",null,{"className":"italic","children":"domain specific language"}]," (DSL) provides a Python interface that enables efficient matrix muliplication GPU kernels implementation leveraging the tile based abstraction. The DSL offers Blackwell specific support in the ",["$","span",null,{"className":"font-semibold","children":"cutlass.cute.nvgpu.tcgen05"}]," module. Since our discussions about this module will largely be hardware aware, let us preface the discussion by an excusrion of Blackwell tensor core programming at the more fundamental ",["$","span",null,{"className":"italic","children":"parallel thread excution"}]," (PTX) virtual instruction set level."]}]}]]}],["$","div","1",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Tensor Memory"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["A cooperative thread array (CTA) is a set of threads that excute a kernel concurrently or in parallel. A ","$L5"," of a CTA is a maximum subset of the set of threads in the CTA that can execute the same instruction simultaneously. In common hardware, a warp is a ","$L6"," thread subset of the CTA. Threads in a CTA can be organized into a block of 1D, 2D, or 3D block of dimensions ","$L7"," and each thread indexed by a tuple ","$L8",". If ","$L9"," then the CTA is a 1D block and so forth. One can enumerate the threads in an ","$La"," CTA by ","$Lb","We can enumerate all threads in a CTA and every four consecutive warps where the begining thread has thread ID ","$Lc"," is called a is called a ","$Ld",". The warps in a warp group are associated with a ","$Le"," from ","$Lf","."]}],"$L10"]}]]}],"$L11","$L12","$L13","$L14"]}]]}]}],"$L15"]}],["$L16","$L17","$L18"],"$L19"]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],"$L1a",false]],"m":"$undefined","G":["$1b",[]],"S":true}
1c:I[96045,["/_next/static/chunks/e02326bdb730da03.js","/_next/static/chunks/bc166ea53d390db7.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
20:I[56691,["/_next/static/chunks/e02326bdb730da03.js","/_next/static/chunks/bc166ea53d390db7.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
21:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"OutletBoundary"]
22:"$Sreact.suspense"
24:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"ViewportBoundary"]
26:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"MetadataBoundary"]
5:["$","span",null,{"className":"font-semibold","children":"warp"}]
6:["$","$L1c",null,{"latex":"32","className":"mt-10"}]
7:["$","$L1c",null,{"latex":"X\\times Y\\times Z","className":"mt-10"}]
8:["$","$L1c",null,{"latex":"(i, j, k)","className":"mt-10"}]
9:["$","$L1c",null,{"latex":"Y=Z=1","className":"mt-10"}]
a:["$","$L1c",null,{"latex":"X\\times Y\\times Z","className":"mt-10"}]
b:["$","$L1c",null,{"latex":"(i,j,k)\\mapsto i + jX + kXY","displayMode":true,"className":"mt-10"}]
c:["$","$L1c",null,{"latex":"I\\equiv 0 \\mod 128","className":"mt-10"}]
d:["$","span",null,{"className":"font-semibold","children":"warp group"}]
e:["$","span",null,{"className":"font-semibold","children":"warp rank"}]
f:["$","$L1c",null,{"latex":"0\\sim 3","className":"mt-10"}]
10:["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["For each cooperative thread array (CTA), the structure of tensor memory is a ",["$","$L1c",null,{"latex":"128 \\times 512","className":"mt-10"}]," matrix of ",["$","$L1c",null,{"latex":"32","className":"mt-10"}]," bit cells. Each of the ",["$","$L1c",null,{"latex":"128","className":"mt-10"}]," rows in tensor memory is also referred to as a ",["$","span",null,{"className":"font-semibold","children":"lane"}],", which is ",["$","$L1c",null,{"latex":"2","className":"mt-10"}]," KB in size. Thus the lanes are in bijective correspondence with the threads in a warp group. In fact the lanes are divided between the warps, so that warp rank ",["$","$L1c",null,{"latex":"0","className":"mt-10"}]," can only access lanes ",["$","$L1c",null,{"latex":"0-31","className":"mt-10"}]," using ",["$","span",null,{"className":"font-semibold","children":"ld"}]," load, ",["$","span",null,{"className":"font-semibold","children":"st"}]," store,     ",["$","span",null,{"className":"font-semibold","children":"cp"}]," copy tcgen05 instructions which are issued on inputs of lane ",["$","$L1c",null,{"latex":"\\times","className":"mt-10"}]," size. Each warp can access all columns within is accessible lane. Tensor memory allocation and deallocation use the ",["$","span",null,{"className":"font-semibold","children":"alloc"}]," and ",["$","span",null,{"className":"font-semibold","children":"dealloc"}]," instructions and are allocated in units of ",["$","$L1c",null,{"latex":"32","className":"mt-10"}]," columns, and the number of allocated columns must be a power of ",["$","$L1c",null,{"latex":"2","className":"mt-10"}],". Thus when a column is allocated, each of its ",["$","$L1c",null,{"latex":"128","className":"mt-10"}]," lanes are allocated."]}]
11:["$","div","2",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Tensor Memory Accelerator"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The instruction ",["$","span",null,{"className":"font-semibold","children":"cp.async.bulk.tensor"}]," is used to copy tensor asynchronously between different types of memory locations (ex. from global to shared memory and back). This is facilitated by a hardware unit called the tensor memory accelerator (TMA). CUDA exposes this instruction via ",["$","span",null,{"className":"font-semibold","children":"cuda::memcpy_async"}]," which can be issued by a single thread within a warp and the synchronization mechanism is ",["$","span",null,{"className":"font-semibold","children":"cuda::barrier"}],". The PTX instruction also allows copying from global memory to the shared memory of ",["$","span",null,{"className":"italic","children":"multiple"}]," CTAs in a cluster, this is so called multicasting and is done by invoking the instruction modifier ",["$","span",null,{"className":"font-semibold","children":".multicast::cluster"}],". One can even copy to distributed shared memory."]}]}]]}]
12:["$","div","3",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Matrix Multiplication Instructions"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":[["$","span",null,{"className":"font-semibold","children":"Matrix multiply and accumulate"}]," (MMA) are excuted by ",["$","span",null,{"className":"font-semibold","children":"mma"}]," instruction which depend on a number launch configurations, such as the data type of input pair of matrices and their shapes, the data type of accumlation or output matrix. Support for warp-specialization, whether one or two CTAs cooperate, and whether the matmul is dense or sparse. For instance 2CTA FP16 dense MMA can have largest shape ",["$","$L1c",null,{"latex":"M\\times N\\times K=256 \\times 256 \\times 16","className":"mt-10"}]," while NVFP4 dense MMA has largest shape ",["$","$L1c",null,{"latex":"M\\times N\\times K=256\\times 256 \\times 64 ","className":"mt-10"}]]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["MMA configuration are stored on register as a 32 bit ",["$","span",null,{"className":"font-semibold","children":"instruction descriptor"}],". For example bits 13 to 16 describe whether the input matrices needs to be ",["$","span",null,{"className":"font-semibold","children":"transposed"}]," or ",["$","span",null,{"className":"font-semibold","children":"negated"}],". Other data store includes sparsity, input output data type."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["MAA supports ",["$","span",null,{"className":"font-semibold","children":"block scaled"}]," matrix multiplication for data types such as ",["$","span",null,{"className":"font-semibold","children":"mxf4"}]," and ",["$","span",null,{"className":"font-semibold","children":"nvf4"}],"."]}]]}]]}]
13:["$","div","4",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Granularity and Synchronization"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["TensorCore Gen 5 Instructions such as ",["$","span",null,{"className":"font-semibold","children":"mma"}],", ",["$","span",null,{"className":"font-semibold","children":"cp"}]," can be issued by a single thread within a CTA or CTA-Pair. Instructions such as ",["$","span",null,{"className":"font-semibold","children":"alloc"}]," and ",["$","span",null,{"className":"font-semibold","children":"dealloc"}]," can be issued from a warp of a pair of warps one in each CTA-pair. Each warp can access a quarter of the tensor memory via ",["$","span",null,{"className":"font-semibold","children":"ld"}]," and ",["$","span",null,{"className":"font-semibold","children":"st"}]," so a warp group is required for complete tensor memory access. These characterize the instruction issuing granularity."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Instructions can be synchronous (alloc/dealloc, fence) or asynchronous (cp, mma, ld, st). Special pairs of asyncrhonous instructions, called ",["$","span",null,{"className":"font-semibold","children":"pipelined instructions"}]," are guarenteed to excute in the order they are issued, an example is ",["$","span",null,{"className":"font-semibold","children":"tcgen05.copy.cta_group::N"}]," and ",["$","span",null,{"className":"font-semibold","children":"tcgen05.mma.cta_group::N"}]," for the same N."]}]]}]]}]
14:["$","div","5",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"CuTe DSL"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"$L1d"}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"$L1e"}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":"$L1f"}]]}]]}]
15:["$","$L20",null,{}]
16:["$","script","script-0",{"src":"/_next/static/chunks/e02326bdb730da03.js","async":true,"nonce":"$undefined"}]
17:["$","script","script-1",{"src":"/_next/static/chunks/bc166ea53d390db7.js","async":true,"nonce":"$undefined"}]
18:["$","script","script-2",{"src":"/_next/static/chunks/5ad9eb95768fc0a4.js","async":true,"nonce":"$undefined"}]
19:["$","$L21",null,{"children":["$","$22",null,{"name":"Next.MetadataOutlet","children":"$@23"}]}]
1a:["$","$1","h",{"children":[null,["$","$L24",null,{"children":"$L25"}],["$","div",null,{"hidden":true,"children":["$","$L26",null,{"children":["$","$22",null,{"name":"Next.Metadata","children":"$L27"}]}]}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]
25:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
28:I[87718,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"IconMark"]
23:null
27:[["$","title","0",{"children":"Claire Zhao Blog"}],["$","meta","1",{"name":"description","content":"Claire Zhao's Blog on AI and Math"}],["$","link","2",{"rel":"icon","href":"/favicon.ico?favicon.e10ff5c7.ico","sizes":"1176x1032","type":"image/x-icon"}],["$","$L28","3",{}]]
29:I[66832,["/_next/static/chunks/e02326bdb730da03.js","/_next/static/chunks/bc166ea53d390db7.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
1d:["$","$L29",null,{"code":"# useful methods in cutlass.cute.nvgpu\nfrom cutlass.cute.nvgpu import (\n    CopyUniversalOp, MmaUniversalOp,\n    make_tiled_tma_atom_A, make_tiled_tma_atom_B\n)\n\n# warp level operations\nfrom cutlass.cute.nvgpu.warp import (\n    MmaF16BF16Op, MmaMXF4Op, MmaMXF4NVF4Op  # MMA \n    LdMatrix8x8x16bOp, StMatrix8x8x16bOp # load & store (&more for other shapes)\n)\n\n# warp group\nfrom cutlass.cute.nvgpu.warpgroup import (\n    MmaF16BF16Op,\n    make_smem_layout_atom,\n    fence, commit, wait_group\n)\n\n# TMA\nfrom class cutlass.cute.nvgpu.cpasync import (\n    CopyBulkTensorTileG2SOp, CopyBulkTensorTileS2GOp # G2S <-> S2G means global and shared memory\n    CopyBulkTensorTileG2SMulticastOp, # plus multicast copy in reverse direction \n    make_tiled_tma_atom,\n)\n","tokens":[[{"content":"# useful methods in cutlass.cute.nvgpu","offset":0,"color":"#EEF0F98F","fontStyle":1}],[{"content":"from","offset":39,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.cute.nvgpu ","offset":43,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":63,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":69,"color":"#EEF0F9","fontStyle":0}],[{"content":"    CopyUniversalOp, MmaUniversalOp,","offset":72,"color":"#EEF0F9","fontStyle":0}],[{"content":"    make_tiled_tma_atom_A, make_tiled_tma_atom_B","offset":109,"color":"#EEF0F9","fontStyle":0}],[{"content":")","offset":158,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"# warp level operations","offset":161,"color":"#EEF0F98F","fontStyle":1}],[{"content":"from","offset":185,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.cute.nvgpu.warp ","offset":189,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":214,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":220,"color":"#EEF0F9","fontStyle":0}],[{"content":"    MmaF16BF16Op, MmaMXF4Op, MmaMXF4NVF4Op  ","offset":223,"color":"#EEF0F9","fontStyle":0},{"content":"# MMA ","offset":267,"color":"#EEF0F98F","fontStyle":1}],[{"content":"    LdMatrix8x8x16bOp, StMatrix8x8x16bOp ","offset":274,"color":"#EEF0F9","fontStyle":0},{"content":"# load & store (&more for other shapes)","offset":315,"color":"#EEF0F98F","fontStyle":1}],[{"content":")","offset":355,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"# warp group","offset":358,"color":"#EEF0F98F","fontStyle":1}],[{"content":"from","offset":371,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.cute.nvgpu.warpgroup ","offset":375,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":405,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":411,"color":"#EEF0F9","fontStyle":0}],[{"content":"    MmaF16BF16Op,","offset":414,"color":"#EEF0F9","fontStyle":0}],[{"content":"    make_smem_layout_atom,","offset":432,"color":"#EEF0F9","fontStyle":0}],[{"content":"    fence, commit, wait_group","offset":459,"color":"#EEF0F9","fontStyle":0}],[{"content":")","offset":489,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"# TMA","offset":492,"color":"#EEF0F98F","fontStyle":1}],[{"content":"from","offset":498,"color":"#54B9FF","fontStyle":1},{"content":" ","offset":502,"color":"#EEF0F9","fontStyle":0},{"content":"class","offset":503,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.cute.nvgpu.cpasync ","offset":508,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":536,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":542,"color":"#EEF0F9","fontStyle":0}],[{"content":"    CopyBulkTensorTileG2SOp, CopyBulkTensorTileS2GOp ","offset":545,"color":"#EEF0F9","fontStyle":0},{"content":"# G2S <-> S2G means global and shared memory","offset":598,"color":"#EEF0F98F","fontStyle":1}],[{"content":"    CopyBulkTensorTileG2SMulticastOp, ","offset":643,"color":"#EEF0F9","fontStyle":0},{"content":"# plus multicast copy in reverse direction ","offset":681,"color":"#EEF0F98F","fontStyle":1}],[{"content":"    make_tiled_tma_atom,","offset":725,"color":"#EEF0F9","fontStyle":0}],[{"content":")","offset":750,"color":"#EEF0F9","fontStyle":0}],[]]}]
1e:["$","$L29",null,{"code":"# Pipeline \nfrom cutlass.pipeline import (\n    PipelineProducer, PipelineConsumer,\n    PipelineAsync,   # async producer-consumer pattern\n    PipelineTmaAsync,\n    PipelineTmaUmma,\n    PipelineAsyncUmma,\n    PipelineUmmaAsync,\n)\n\n# useful util\nfrom cutlass.utils import (\n    SmemAllocator,\n    TmemAllocator,\n    HardwareInfo,\n    print_latex,\n    print_latex_tv\n    # also things like cutlass.utils.sm100.make_smem_layout_a\n)\n","tokens":[[{"content":"# Pipeline ","offset":0,"color":"#EEF0F98F","fontStyle":1}],[{"content":"from","offset":12,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.pipeline ","offset":16,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":34,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":40,"color":"#EEF0F9","fontStyle":0}],[{"content":"    PipelineProducer, PipelineConsumer,","offset":43,"color":"#EEF0F9","fontStyle":0}],[{"content":"    PipelineAsync,   ","offset":83,"color":"#EEF0F9","fontStyle":0},{"content":"# async producer-consumer pattern","offset":104,"color":"#EEF0F98F","fontStyle":1}],[{"content":"    PipelineTmaAsync,","offset":138,"color":"#EEF0F9","fontStyle":0}],[{"content":"    PipelineTmaUmma,","offset":160,"color":"#EEF0F9","fontStyle":0}],[{"content":"    PipelineAsyncUmma,","offset":181,"color":"#EEF0F9","fontStyle":0}],[{"content":"    PipelineUmmaAsync,","offset":204,"color":"#EEF0F9","fontStyle":0}],[{"content":")","offset":227,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"# useful util","offset":230,"color":"#EEF0F98F","fontStyle":1}],[{"content":"from","offset":244,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.utils ","offset":248,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":263,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":269,"color":"#EEF0F9","fontStyle":0}],[{"content":"    SmemAllocator,","offset":272,"color":"#EEF0F9","fontStyle":0}],[{"content":"    TmemAllocator,","offset":291,"color":"#EEF0F9","fontStyle":0}],[{"content":"    HardwareInfo,","offset":310,"color":"#EEF0F9","fontStyle":0}],[{"content":"    print_latex,","offset":328,"color":"#EEF0F9","fontStyle":0}],[{"content":"    print_latex_tv","offset":345,"color":"#EEF0F9","fontStyle":0}],[{"content":"    ","offset":364,"color":"#EEF0F9","fontStyle":0},{"content":"# also things like cutlass.utils.sm100.make_smem_layout_a","offset":368,"color":"#EEF0F98F","fontStyle":1}],[{"content":")","offset":426,"color":"#EEF0F9","fontStyle":0}],[]]}]
1f:["$","$L29",null,{"code":"from cutlass.cute import (\n    make_layout,\n    zipped_product,\n    Swizzle,\n    gemm,\n    depth, rank, size, \n    struct, # decorator\n    printf, ceil_div # etc \n)\n\nfrom cutlass.cute.arch import (\n    thread_idx,\n    block_idx, block_dim,\n    warp.idx # etc\n)\n\nfrom cutlass.cute.runtime import (\n    from_dlpack,\n    make_ptr\n)\n","tokens":[[{"content":"from","offset":0,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.cute ","offset":4,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":18,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":24,"color":"#EEF0F9","fontStyle":0}],[{"content":"    make_layout,","offset":27,"color":"#EEF0F9","fontStyle":0}],[{"content":"    zipped_product,","offset":44,"color":"#EEF0F9","fontStyle":0}],[{"content":"    Swizzle,","offset":64,"color":"#EEF0F9","fontStyle":0}],[{"content":"    gemm,","offset":77,"color":"#EEF0F9","fontStyle":0}],[{"content":"    depth, rank, size, ","offset":87,"color":"#EEF0F9","fontStyle":0}],[{"content":"    struct, ","offset":111,"color":"#EEF0F9","fontStyle":0},{"content":"# decorator","offset":123,"color":"#EEF0F98F","fontStyle":1}],[{"content":"    printf, ceil_div ","offset":135,"color":"#EEF0F9","fontStyle":0},{"content":"# etc ","offset":156,"color":"#EEF0F98F","fontStyle":1}],[{"content":")","offset":163,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"from","offset":166,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.cute.arch ","offset":170,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":189,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":195,"color":"#EEF0F9","fontStyle":0}],[{"content":"    thread_idx,","offset":198,"color":"#EEF0F9","fontStyle":0}],[{"content":"    block_idx, block_dim,","offset":214,"color":"#EEF0F9","fontStyle":0}],[{"content":"    warp.idx ","offset":240,"color":"#EEF0F9","fontStyle":0},{"content":"# etc","offset":253,"color":"#EEF0F98F","fontStyle":1}],[{"content":")","offset":259,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"from","offset":262,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.cute.runtime ","offset":266,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":288,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":294,"color":"#EEF0F9","fontStyle":0}],[{"content":"    from_dlpack,","offset":297,"color":"#EEF0F9","fontStyle":0}],[{"content":"    make_ptr","offset":314,"color":"#EEF0F9","fontStyle":0}],[{"content":")","offset":327,"color":"#EEF0F9","fontStyle":0}],[]]}]
