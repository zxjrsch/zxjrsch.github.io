1:"$Sreact.fragment"
2:I[1565,["/_next/static/chunks/e02326bdb730da03.js","/_next/static/chunks/bc166ea53d390db7.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
3:I[96045,["/_next/static/chunks/e02326bdb730da03.js","/_next/static/chunks/bc166ea53d390db7.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
18:I[66832,["/_next/static/chunks/e02326bdb730da03.js","/_next/static/chunks/bc166ea53d390db7.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
1b:I[56691,["/_next/static/chunks/e02326bdb730da03.js","/_next/static/chunks/bc166ea53d390db7.js","/_next/static/chunks/5ad9eb95768fc0a4.js"],"default"]
1c:I[47259,["/_next/static/chunks/988ea292de4c4c73.js","/_next/static/chunks/7a959126392b4956.js"],"OutletBoundary"]
1d:"$Sreact.suspense"
0:{"buildId":"JuMVSYJ3RIXYyuKTjUJ0h","rsc":["$","$1","c",{"children":[["$","div",null,{"className":"flex flex-col justify-center min-h-screen","children":[["$","div",null,{"className":"flex flex-row justify-center pt-15","children":["$","$L2",null,{}]}],["$","div",null,{"className":"grow flex justify-center w-full pt-10 md:pt-15 px-3","children":["$","div",null,{"className":"flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl","children":[["$","div",null,{"className":"text-3xl font-bold flex justify-center","children":"Programming Blackwell Tensor Core with Cutlass and CuTe DSL"}],["$","div",null,{"className":"text-sm flex justify-center pt-5","children":null}],["$","div",null,{"className":"","children":[["$","div","0",{"children":["$undefined",["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The CuTe ",["$","span",null,{"className":"italic","children":"domain specific language"}]," (DSL) provides a Python interface that enables efficient matrix muliplication GPU kernels implementation leveraging the tile based abstraction. The DSL offers Blackwell specific support in the ",["$","span",null,{"className":"font-semibold","children":"cutlass.cute.nvgpu.tcgen05"}]," module. Since our discussions about this module will largely be hardware aware, let us preface the discussion by an excusrion of Blackwell tensor core programming at the more fundamental ",["$","span",null,{"className":"italic","children":"parallel thread excution"}]," (PTX) virtual instruction set level."]}]}]]}],["$","div","1",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Tensor Memory"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["A cooperative thread array (CTA) is a set of threads that excute a kernel concurrently or in parallel. A ",["$","span",null,{"className":"font-semibold","children":"warp"}]," of a CTA is a maximum subset of the set of threads in the CTA that can execute the same instruction simultaneously. In common hardware, a warp is a ",["$","$L3",null,{"latex":"32","className":"mt-10"}]," thread subset of the CTA. Threads in a CTA can be organized into a block of 1D, 2D, or 3D block of dimensions ",["$","$L3",null,{"latex":"X\\times Y\\times Z","className":"mt-10"}]," and each thread indexed by a tuple ",["$","$L3",null,{"latex":"(i, j, k)","className":"mt-10"}],". If ",["$","$L3",null,{"latex":"Y=Z=1","className":"mt-10"}]," then the CTA is a 1D block and so forth. One can enumerate the threads in an ",["$","$L3",null,{"latex":"X\\times Y\\times Z","className":"mt-10"}]," CTA by ",["$","$L3",null,{"latex":"(i,j,k)\\mapsto i + jX + kXY","displayMode":true,"className":"mt-10"}],"We can enumerate all threads in a CTA and every four consecutive warps where the begining thread has thread ID ",["$","$L3",null,{"latex":"I\\equiv 0 \\mod 128","className":"mt-10"}]," is called a is called a ",["$","span",null,{"className":"font-semibold","children":"warp group"}],". The warps in a warp group are associated with a ",["$","span",null,{"className":"font-semibold","children":"warp rank"}]," from ",["$","$L3",null,{"latex":"0\\sim 3","className":"mt-10"}],"."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["For each cooperative thread array (CTA), the structure of tensor memory is a ",["$","$L3",null,{"latex":"128 \\times 512","className":"mt-10"}]," matrix of ",["$","$L3",null,{"latex":"32","className":"mt-10"}]," bit cells. Each of the ",["$","$L3",null,{"latex":"128","className":"mt-10"}]," rows in tensor memory is also referred to as a ",["$","span",null,{"className":"font-semibold","children":"lane"}],", which is ",["$","$L3",null,{"latex":"2","className":"mt-10"}]," KB in size. Thus the lanes are in bijective correspondence with the threads in a warp group. In fact the lanes are divided between the warps, so that warp rank ","$L4"," can only access lanes ","$L5"," using ","$L6"," load, ","$L7"," store,     ","$L8"," copy tcgen05 instructions which are issued on inputs of lane ","$L9"," size. Each warp can access all columns within is accessible lane. Tensor memory allocation and deallocation use the ","$La"," and ","$Lb"," instructions and are allocated in units of ","$Lc"," columns, and the number of allocated columns must be a power of ","$Ld",". Thus when a column is allocated, each of its ","$Le"," lanes are allocated."]}]]}]]}],"$Lf","$L10","$L11","$L12"]}]]}]}],"$L13"]}],["$L14","$L15","$L16"],"$L17"]}],"loading":null,"isPartial":false}
4:["$","$L3",null,{"latex":"0","className":"mt-10"}]
5:["$","$L3",null,{"latex":"0-31","className":"mt-10"}]
6:["$","span",null,{"className":"font-semibold","children":"ld"}]
7:["$","span",null,{"className":"font-semibold","children":"st"}]
8:["$","span",null,{"className":"font-semibold","children":"cp"}]
9:["$","$L3",null,{"latex":"\\times","className":"mt-10"}]
a:["$","span",null,{"className":"font-semibold","children":"alloc"}]
b:["$","span",null,{"className":"font-semibold","children":"dealloc"}]
c:["$","$L3",null,{"latex":"32","className":"mt-10"}]
d:["$","$L3",null,{"latex":"2","className":"mt-10"}]
e:["$","$L3",null,{"latex":"128","className":"mt-10"}]
f:["$","div","2",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Tensor Memory Accelerator"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["The instruction ",["$","span",null,{"className":"font-semibold","children":"cp.async.bulk.tensor"}]," is used to copy tensor asynchronously between different types of memory locations (ex. from global to shared memory and back). This is facilitated by a hardware unit called the tensor memory accelerator (TMA). CUDA exposes this instruction via ",["$","span",null,{"className":"font-semibold","children":"cuda::memcpy_async"}]," which can be issued by a single thread within a warp and the synchronization mechanism is ",["$","span",null,{"className":"font-semibold","children":"cuda::barrier"}],". The PTX instruction also allows copying from global memory to the shared memory of ",["$","span",null,{"className":"italic","children":"multiple"}]," CTAs in a cluster, this is so called multicasting and is done by invoking the instruction modifier ",["$","span",null,{"className":"font-semibold","children":".multicast::cluster"}],". One can even copy to distributed shared memory."]}]}]]}]
10:["$","div","3",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Matrix Multiplication Instructions"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":[["$","span",null,{"className":"font-semibold","children":"Matrix multiply and accumulate"}]," (MMA) are excuted by ",["$","span",null,{"className":"font-semibold","children":"mma"}]," instruction which depend on a number launch configurations, such as the data type of input pair of matrices and their shapes, the data type of accumlation or output matrix. Support for warp-specialization, whether one or two CTAs cooperate, and whether the matmul is dense or sparse. For instance 2CTA FP16 dense MMA can have largest shape ",["$","$L3",null,{"latex":"M\\times N\\times K=256 \\times 256 \\times 16","className":"mt-10"}]," while NVFP4 dense MMA has largest shape ",["$","$L3",null,{"latex":"M\\times N\\times K=256\\times 256 \\times 64 ","className":"mt-10"}]]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["MMA configuration are stored on register as a 32 bit ",["$","span",null,{"className":"font-semibold","children":"instruction descriptor"}],". For example bits 13 to 16 describe whether the input matrices needs to be ",["$","span",null,{"className":"font-semibold","children":"transposed"}]," or ",["$","span",null,{"className":"font-semibold","children":"negated"}],". Other data store includes sparsity, input output data type."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["MAA supports ",["$","span",null,{"className":"font-semibold","children":"block scaled"}]," matrix multiplication for data types such as ",["$","span",null,{"className":"font-semibold","children":"mxf4"}]," and ",["$","span",null,{"className":"font-semibold","children":"nvf4"}],"."]}]]}]]}]
11:["$","div","4",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"Granularity and Synchronization"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["TensorCore Gen 5 Instructions such as ",["$","span",null,{"className":"font-semibold","children":"mma"}],", ",["$","span",null,{"className":"font-semibold","children":"cp"}]," can be issued by a single thread within a CTA or CTA-Pair. Instructions such as ",["$","span",null,{"className":"font-semibold","children":"alloc"}]," and ",["$","span",null,{"className":"font-semibold","children":"dealloc"}]," can be issued from a warp of a pair of warps one in each CTA-pair. Each warp can access a quarter of the tensor memory via ",["$","span",null,{"className":"font-semibold","children":"ld"}]," and ",["$","span",null,{"className":"font-semibold","children":"st"}]," so a warp group is required for complete tensor memory access. These characterize the instruction issuing granularity."]}],["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["Instructions can be synchronous (alloc/dealloc, fence) or asynchronous (cp, mma, ld, st). Special pairs of asyncrhonous instructions, called ",["$","span",null,{"className":"font-semibold","children":"pipelined instructions"}]," are guarenteed to excute in the order they are issued, an example is ",["$","span",null,{"className":"font-semibold","children":"tcgen05.copy.cta_group::N"}]," and ",["$","span",null,{"className":"font-semibold","children":"tcgen05.mma.cta_group::N"}]," for the same N."]}]]}]]}]
12:["$","div","5",{"children":[["$","div",null,{"className":" pt-20 font-bold text-2xl","id":"section-2","children":"CuTe DSL"}],["$","div",null,{"className":"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify","children":[["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["$","$L18",null,{"code":"# useful methods in cutlass.cute.nvgpu\nfrom cutlass.cute.nvgpu import (\n    CopyUniversalOp, MmaUniversalOp,\n    make_tiled_tma_atom_A, make_tiled_tma_atom_B\n)\n\n# warp level operations\nfrom cutlass.cute.nvgpu.warp import (\n    MmaF16BF16Op, MmaMXF4Op, MmaMXF4NVF4Op  # MMA \n    LdMatrix8x8x16bOp, StMatrix8x8x16bOp # load & store (&more for other shapes)\n)\n\n# warp group\nfrom cutlass.cute.nvgpu.warpgroup import (\n    MmaF16BF16Op,\n    make_smem_layout_atom,\n    fence, commit, wait_group\n)\n\n# TMA\nfrom class cutlass.cute.nvgpu.cpasync import (\n    CopyBulkTensorTileG2SOp, CopyBulkTensorTileS2GOp # G2S <-> S2G means global and shared memory\n    CopyBulkTensorTileG2SMulticastOp, # plus multicast copy in reverse direction \n    make_tiled_tma_atom,\n)\n","tokens":[[{"content":"# useful methods in cutlass.cute.nvgpu","offset":0,"color":"#EEF0F98F","fontStyle":1}],[{"content":"from","offset":39,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.cute.nvgpu ","offset":43,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":63,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":69,"color":"#EEF0F9","fontStyle":0}],[{"content":"    CopyUniversalOp, MmaUniversalOp,","offset":72,"color":"#EEF0F9","fontStyle":0}],[{"content":"    make_tiled_tma_atom_A, make_tiled_tma_atom_B","offset":109,"color":"#EEF0F9","fontStyle":0}],[{"content":")","offset":158,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"# warp level operations","offset":161,"color":"#EEF0F98F","fontStyle":1}],[{"content":"from","offset":185,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.cute.nvgpu.warp ","offset":189,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":214,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":220,"color":"#EEF0F9","fontStyle":0}],[{"content":"    MmaF16BF16Op, MmaMXF4Op, MmaMXF4NVF4Op  ","offset":223,"color":"#EEF0F9","fontStyle":0},{"content":"# MMA ","offset":267,"color":"#EEF0F98F","fontStyle":1}],[{"content":"    LdMatrix8x8x16bOp, StMatrix8x8x16bOp ","offset":274,"color":"#EEF0F9","fontStyle":0},{"content":"# load & store (&more for other shapes)","offset":315,"color":"#EEF0F98F","fontStyle":1}],[{"content":")","offset":355,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"# warp group","offset":358,"color":"#EEF0F98F","fontStyle":1}],[{"content":"from","offset":371,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.cute.nvgpu.warpgroup ","offset":375,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":405,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":411,"color":"#EEF0F9","fontStyle":0}],[{"content":"    MmaF16BF16Op,","offset":414,"color":"#EEF0F9","fontStyle":0}],[{"content":"    make_smem_layout_atom,","offset":432,"color":"#EEF0F9","fontStyle":0}],[{"content":"    fence, commit, wait_group","offset":459,"color":"#EEF0F9","fontStyle":0}],[{"content":")","offset":489,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"# TMA","offset":492,"color":"#EEF0F98F","fontStyle":1}],[{"content":"from","offset":498,"color":"#54B9FF","fontStyle":1},{"content":" ","offset":502,"color":"#EEF0F9","fontStyle":0},{"content":"class","offset":503,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.cute.nvgpu.cpasync ","offset":508,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":536,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":542,"color":"#EEF0F9","fontStyle":0}],[{"content":"    CopyBulkTensorTileG2SOp, CopyBulkTensorTileS2GOp ","offset":545,"color":"#EEF0F9","fontStyle":0},{"content":"# G2S <-> S2G means global and shared memory","offset":598,"color":"#EEF0F98F","fontStyle":1}],[{"content":"    CopyBulkTensorTileG2SMulticastOp, ","offset":643,"color":"#EEF0F9","fontStyle":0},{"content":"# plus multicast copy in reverse direction ","offset":681,"color":"#EEF0F98F","fontStyle":1}],[{"content":"    make_tiled_tma_atom,","offset":725,"color":"#EEF0F9","fontStyle":0}],[{"content":")","offset":750,"color":"#EEF0F9","fontStyle":0}],[]]}]}],"$L19","$L1a"]}]]}]
13:["$","$L1b",null,{}]
14:["$","script","script-0",{"src":"/_next/static/chunks/e02326bdb730da03.js","async":true}]
15:["$","script","script-1",{"src":"/_next/static/chunks/bc166ea53d390db7.js","async":true}]
16:["$","script","script-2",{"src":"/_next/static/chunks/5ad9eb95768fc0a4.js","async":true}]
17:["$","$L1c",null,{"children":["$","$1d",null,{"name":"Next.MetadataOutlet","children":"$@1e"}]}]
19:["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["$","$L18",null,{"code":"# Pipeline \nfrom cutlass.pipeline import (\n    PipelineProducer, PipelineConsumer,\n    PipelineAsync,   # async producer-consumer pattern\n    PipelineTmaAsync,\n    PipelineTmaUmma,\n    PipelineAsyncUmma,\n    PipelineUmmaAsync,\n)\n\n# useful util\nfrom cutlass.utils import (\n    SmemAllocator,\n    TmemAllocator,\n    HardwareInfo,\n    print_latex,\n    print_latex_tv\n    # also things like cutlass.utils.sm100.make_smem_layout_a\n)\n","tokens":[[{"content":"# Pipeline ","offset":0,"color":"#EEF0F98F","fontStyle":1}],[{"content":"from","offset":12,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.pipeline ","offset":16,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":34,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":40,"color":"#EEF0F9","fontStyle":0}],[{"content":"    PipelineProducer, PipelineConsumer,","offset":43,"color":"#EEF0F9","fontStyle":0}],[{"content":"    PipelineAsync,   ","offset":83,"color":"#EEF0F9","fontStyle":0},{"content":"# async producer-consumer pattern","offset":104,"color":"#EEF0F98F","fontStyle":1}],[{"content":"    PipelineTmaAsync,","offset":138,"color":"#EEF0F9","fontStyle":0}],[{"content":"    PipelineTmaUmma,","offset":160,"color":"#EEF0F9","fontStyle":0}],[{"content":"    PipelineAsyncUmma,","offset":181,"color":"#EEF0F9","fontStyle":0}],[{"content":"    PipelineUmmaAsync,","offset":204,"color":"#EEF0F9","fontStyle":0}],[{"content":")","offset":227,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"# useful util","offset":230,"color":"#EEF0F98F","fontStyle":1}],[{"content":"from","offset":244,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.utils ","offset":248,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":263,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":269,"color":"#EEF0F9","fontStyle":0}],[{"content":"    SmemAllocator,","offset":272,"color":"#EEF0F9","fontStyle":0}],[{"content":"    TmemAllocator,","offset":291,"color":"#EEF0F9","fontStyle":0}],[{"content":"    HardwareInfo,","offset":310,"color":"#EEF0F9","fontStyle":0}],[{"content":"    print_latex,","offset":328,"color":"#EEF0F9","fontStyle":0}],[{"content":"    print_latex_tv","offset":345,"color":"#EEF0F9","fontStyle":0}],[{"content":"    ","offset":364,"color":"#EEF0F9","fontStyle":0},{"content":"# also things like cutlass.utils.sm100.make_smem_layout_a","offset":368,"color":"#EEF0F98F","fontStyle":1}],[{"content":")","offset":426,"color":"#EEF0F9","fontStyle":0}],[]]}]}]
1a:["$","div",null,{"className":"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify","children":["$","$L18",null,{"code":"from cutlass.cute import (\n    make_layout,\n    zipped_product,\n    Swizzle,\n    gemm,\n    depth, rank, size, \n    struct, # decorator\n    printf, ceil_div # etc \n)\n\nfrom cutlass.cute.arch import (\n    thread_idx,\n    block_idx, block_dim,\n    warp.idx # etc\n)\n\nfrom cutlass.cute.runtime import (\n    from_dlpack,\n    make_ptr\n)\n","tokens":[[{"content":"from","offset":0,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.cute ","offset":4,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":18,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":24,"color":"#EEF0F9","fontStyle":0}],[{"content":"    make_layout,","offset":27,"color":"#EEF0F9","fontStyle":0}],[{"content":"    zipped_product,","offset":44,"color":"#EEF0F9","fontStyle":0}],[{"content":"    Swizzle,","offset":64,"color":"#EEF0F9","fontStyle":0}],[{"content":"    gemm,","offset":77,"color":"#EEF0F9","fontStyle":0}],[{"content":"    depth, rank, size, ","offset":87,"color":"#EEF0F9","fontStyle":0}],[{"content":"    struct, ","offset":111,"color":"#EEF0F9","fontStyle":0},{"content":"# decorator","offset":123,"color":"#EEF0F98F","fontStyle":1}],[{"content":"    printf, ceil_div ","offset":135,"color":"#EEF0F9","fontStyle":0},{"content":"# etc ","offset":156,"color":"#EEF0F98F","fontStyle":1}],[{"content":")","offset":163,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"from","offset":166,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.cute.arch ","offset":170,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":189,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":195,"color":"#EEF0F9","fontStyle":0}],[{"content":"    thread_idx,","offset":198,"color":"#EEF0F9","fontStyle":0}],[{"content":"    block_idx, block_dim,","offset":214,"color":"#EEF0F9","fontStyle":0}],[{"content":"    warp.idx ","offset":240,"color":"#EEF0F9","fontStyle":0},{"content":"# etc","offset":253,"color":"#EEF0F98F","fontStyle":1}],[{"content":")","offset":259,"color":"#EEF0F9","fontStyle":0}],[],[{"content":"from","offset":262,"color":"#54B9FF","fontStyle":1},{"content":" cutlass.cute.runtime ","offset":266,"color":"#EEF0F9","fontStyle":0},{"content":"import","offset":288,"color":"#54B9FF","fontStyle":1},{"content":" (","offset":294,"color":"#EEF0F9","fontStyle":0}],[{"content":"    from_dlpack,","offset":297,"color":"#EEF0F9","fontStyle":0}],[{"content":"    make_ptr","offset":314,"color":"#EEF0F9","fontStyle":0}],[{"content":")","offset":327,"color":"#EEF0F9","fontStyle":0}],[]]}]}]
1e:null
