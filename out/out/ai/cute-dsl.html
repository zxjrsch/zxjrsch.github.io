<!DOCTYPE html><!--JuMVSYJ3RIXYyuKTjUJ0h--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/chunks/1fcb9d377ba4db76.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/4057cc9dbcc744c0.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/be439e04947a2cba.js"/><script src="/_next/static/chunks/922a6be1c048f7e7.js" async=""></script><script src="/_next/static/chunks/748df87cf5306c08.js" async=""></script><script src="/_next/static/chunks/turbopack-0af7db035ef23dbc.js" async=""></script><script src="/_next/static/chunks/988ea292de4c4c73.js" async=""></script><script src="/_next/static/chunks/7a959126392b4956.js" async=""></script><script src="/_next/static/chunks/e02326bdb730da03.js" async=""></script><script src="/_next/static/chunks/bc166ea53d390db7.js" async=""></script><script src="/_next/static/chunks/5ad9eb95768fc0a4.js" async=""></script><meta name="next-size-adjust" content=""/><title>Claire Zhao Blog</title><meta name="description" content="Claire Zhao&#x27;s Blog on AI and Math"/><link rel="icon" href="/favicon.ico?favicon.e10ff5c7.ico" sizes="1176x1032" type="image/x-icon"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased"><div hidden=""><!--$--><!--/$--></div><div class="flex flex-col justify-center min-h-screen"><div class="flex flex-row justify-center pt-15"><div class="flex flex-row w-full md:max-w-5/10 relative"><div class="flex flex-row md:basis-1/3 "><a href="/"><div class="text-2xl font-bold pl-5 md:pl-0 pt-5 md:pt-0">Claire Zhao</div></a></div><div class="md:hidden grow flex justify-end items-center pr-10"><button><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="size-5"><path d="M8 2H13.5C13.7761 2 14 2.22386 14 2.5V12.5C14 12.7761 13.7761 13 13.5 13H8V2ZM7 2H1.5C1.22386 2 1 2.22386 1 2.5V12.5C1 12.7761 1.22386 13 1.5 13H7V2ZM0 2.5C0 1.67157 0.671573 1 1.5 1H13.5C14.3284 1 15 1.67157 15 2.5V12.5C15 13.3284 14.3284 14 13.5 14H1.5C0.671573 14 0 13.3284 0 12.5V2.5Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg></button></div><nav class="hidden md:flex basis-2/3 flex-row justify-end gap-x-8"><a href="/ai"><div class="flex items-center gap-2 font-medium hover:underline">Machine Learning <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9 4L9 11L4.5 7.5L9 4Z" fill="currentColor"></path></svg></div></a><a href="/math"><div class="flex items-center gap-2 font-medium hover:underline">Mathematics </div></a></nav></div></div><div class="grow flex justify-center w-full pt-10 md:pt-15 px-3"><div class="flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl"><div class="text-3xl font-bold flex justify-center">Programming Blackwell Tensor Core with Cutlass and CuTe DSL</div><div class="text-sm flex justify-center pt-5"></div><div class=""><div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">The CuTe <span class="italic">domain specific language</span> (DSL) provides a Python interface that enables efficient matrix muliplication GPU kernels implementation leveraging the tile based abstraction. The DSL offers Blackwell specific support in the <span class="font-semibold">cutlass.cute.nvgpu.tcgen05</span> module. Since our discussions about this module will largely be hardware aware, let us preface the discussion by an excusrion of Blackwell tensor core programming at the more fundamental <span class="italic">parallel thread excution</span> (PTX) virtual instruction set level.</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Tensor Memory</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">A cooperative thread array (CTA) is a set of threads that excute a kernel concurrently or in parallel. A <span class="font-semibold">warp</span> of a CTA is a maximum subset of the set of threads in the CTA that can execute the same instruction simultaneously. In common hardware, a warp is a <span class="mt-10"></span> thread subset of the CTA. Threads in a CTA can be organized into a block of 1D, 2D, or 3D block of dimensions <span class="mt-10"></span> and each thread indexed by a tuple <span class="mt-10"></span>. If <span class="mt-10"></span> then the CTA is a 1D block and so forth. One can enumerate the threads in an <span class="mt-10"></span> CTA by <span class="mt-10"></span>We can enumerate all threads in a CTA and every four consecutive warps where the begining thread has thread ID <span class="mt-10"></span> is called a is called a <span class="font-semibold">warp group</span>. The warps in a warp group are associated with a <span class="font-semibold">warp rank</span> from <span class="mt-10"></span>.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">For each cooperative thread array (CTA), the structure of tensor memory is a <span class="mt-10"></span> matrix of <span class="mt-10"></span> bit cells. Each of the <span class="mt-10"></span> rows in tensor memory is also referred to as a <span class="font-semibold">lane</span>, which is <span class="mt-10"></span> KB in size. Thus the lanes are in bijective correspondence with the threads in a warp group. In fact the lanes are divided between the warps, so that warp rank <span class="mt-10"></span> can only access lanes <span class="mt-10"></span> using <span class="font-semibold">ld</span> load, <span class="font-semibold">st</span> store,     <span class="font-semibold">cp</span> copy tcgen05 instructions which are issued on inputs of lane <span class="mt-10"></span> size. Each warp can access all columns within is accessible lane. Tensor memory allocation and deallocation use the <span class="font-semibold">alloc</span> and <span class="font-semibold">dealloc</span> instructions and are allocated in units of <span class="mt-10"></span> columns, and the number of allocated columns must be a power of <span class="mt-10"></span>. Thus when a column is allocated, each of its <span class="mt-10"></span> lanes are allocated.</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Tensor Memory Accelerator</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">The instruction <span class="font-semibold">cp.async.bulk.tensor</span> is used to copy tensor asynchronously between different types of memory locations (ex. from global to shared memory and back). This is facilitated by a hardware unit called the tensor memory accelerator (TMA). CUDA exposes this instruction via <span class="font-semibold">cuda::memcpy_async</span> which can be issued by a single thread within a warp and the synchronization mechanism is <span class="font-semibold">cuda::barrier</span>. The PTX instruction also allows copying from global memory to the shared memory of <span class="italic">multiple</span> CTAs in a cluster, this is so called multicasting and is done by invoking the instruction modifier <span class="font-semibold">.multicast::cluster</span>. One can even copy to distributed shared memory.</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Matrix Multiplication Instructions</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify"><span class="font-semibold">Matrix multiply and accumulate</span> (MMA) are excuted by <span class="font-semibold">mma</span> instruction which depend on a number launch configurations, such as the data type of input pair of matrices and their shapes, the data type of accumlation or output matrix. Support for warp-specialization, whether one or two CTAs cooperate, and whether the matmul is dense or sparse. For instance 2CTA FP16 dense MMA can have largest shape <span class="mt-10"></span> while NVFP4 dense MMA has largest shape <span class="mt-10"></span></div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">MMA configuration are stored on register as a 32 bit <span class="font-semibold">instruction descriptor</span>. For example bits 13 to 16 describe whether the input matrices needs to be <span class="font-semibold">transposed</span> or <span class="font-semibold">negated</span>. Other data store includes sparsity, input output data type.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">MAA supports <span class="font-semibold">block scaled</span> matrix multiplication for data types such as <span class="font-semibold">mxf4</span> and <span class="font-semibold">nvf4</span>.</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">Granularity and Synchronization</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">TensorCore Gen 5 Instructions such as <span class="font-semibold">mma</span>, <span class="font-semibold">cp</span> can be issued by a single thread within a CTA or CTA-Pair. Instructions such as <span class="font-semibold">alloc</span> and <span class="font-semibold">dealloc</span> can be issued from a warp of a pair of warps one in each CTA-pair. Each warp can access a quarter of the tensor memory via <span class="font-semibold">ld</span> and <span class="font-semibold">st</span> so a warp group is required for complete tensor memory access. These characterize the instruction issuing granularity.</div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify">Instructions can be synchronous (alloc/dealloc, fence) or asynchronous (cp, mma, ld, st). Special pairs of asyncrhonous instructions, called <span class="font-semibold">pipelined instructions</span> are guarenteed to excute in the order they are issued, an example is <span class="font-semibold">tcgen05.copy.cta_group::N</span> and <span class="font-semibold">tcgen05.mma.cta_group::N</span> for the same N.</div></div></div><div><div class=" pt-20 font-bold text-2xl" id="section-2">CuTe DSL</div><div class="indent-10 md:mt-10 grow text-lg font-medium text-base text-justify"><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify"><div class="relative group my-4 rounded-lg bg-[#1e1e1e] border border-white/10 shadow-lg"><button class="absolute right-3 top-3 z-10 p-2 rounded-md bg-white/10 text-gray-300 opacity-0 group-hover:opacity-100 transition-all duration-200 hover:bg-white/20 hover:text-white" aria-label="Copy code"><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="w-5 h-5"><path d="M1 9.50006C1 10.3285 1.67157 11.0001 2.5 11.0001H4L4 10.0001H2.5C2.22386 10.0001 2 9.7762 2 9.50006L2 2.50006C2 2.22392 2.22386 2.00006 2.5 2.00006L9.5 2.00006C9.77614 2.00006 10 2.22392 10 2.50006V4.00002H5.5C4.67158 4.00002 4 4.67159 4 5.50002V12.5C4 13.3284 4.67158 14 5.5 14H12.5C13.3284 14 14 13.3284 14 12.5V5.50002C14 4.67159 13.3284 4.00002 12.5 4.00002H11V2.50006C11 1.67163 10.3284 1.00006 9.5 1.00006H2.5C1.67157 1.00006 1 1.67163 1 2.50006V9.50006ZM5 5.50002C5 5.22388 5.22386 5.00002 5.5 5.00002H12.5C12.7761 5.00002 13 5.22388 13 5.50002V12.5C13 12.7762 12.7761 13 12.5 13H5.5C5.22386 13 5 12.7762 5 12.5V5.50002Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg></button><pre class="overflow-x-auto my-5 py-4 px-4 text-sm font-mono leading-relaxed"><code class=""><div class="table-row"><span style="color:#EEF0F98F;font-style:italic;font-weight:normal"># useful methods in cutlass.cute.nvgpu</span></div><div class="table-row"><span style="color:#54B9FF;font-style:italic;font-weight:normal">from</span><span style="color:#EEF0F9;font-style:normal;font-weight:normal"> cutlass.cute.nvgpu </span><span style="color:#54B9FF;font-style:italic;font-weight:normal">import</span><span style="color:#EEF0F9;font-style:normal;font-weight:normal"> (</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    CopyUniversalOp, MmaUniversalOp,</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    make_tiled_tma_atom_A, make_tiled_tma_atom_B</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">)</span></div><div class="table-row"><span class="inline-block select-none"> </span></div><div class="table-row"><span style="color:#EEF0F98F;font-style:italic;font-weight:normal"># warp level operations</span></div><div class="table-row"><span style="color:#54B9FF;font-style:italic;font-weight:normal">from</span><span style="color:#EEF0F9;font-style:normal;font-weight:normal"> cutlass.cute.nvgpu.warp </span><span style="color:#54B9FF;font-style:italic;font-weight:normal">import</span><span style="color:#EEF0F9;font-style:normal;font-weight:normal"> (</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    MmaF16BF16Op, MmaMXF4Op, MmaMXF4NVF4Op  </span><span style="color:#EEF0F98F;font-style:italic;font-weight:normal"># MMA </span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    LdMatrix8x8x16bOp, StMatrix8x8x16bOp </span><span style="color:#EEF0F98F;font-style:italic;font-weight:normal"># load &amp; store (&amp;more for other shapes)</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">)</span></div><div class="table-row"><span class="inline-block select-none"> </span></div><div class="table-row"><span style="color:#EEF0F98F;font-style:italic;font-weight:normal"># warp group</span></div><div class="table-row"><span style="color:#54B9FF;font-style:italic;font-weight:normal">from</span><span style="color:#EEF0F9;font-style:normal;font-weight:normal"> cutlass.cute.nvgpu.warpgroup </span><span style="color:#54B9FF;font-style:italic;font-weight:normal">import</span><span style="color:#EEF0F9;font-style:normal;font-weight:normal"> (</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    MmaF16BF16Op,</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    make_smem_layout_atom,</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    fence, commit, wait_group</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">)</span></div><div class="table-row"><span class="inline-block select-none"> </span></div><div class="table-row"><span style="color:#EEF0F98F;font-style:italic;font-weight:normal"># TMA</span></div><div class="table-row"><span style="color:#54B9FF;font-style:italic;font-weight:normal">from</span><span style="color:#EEF0F9;font-style:normal;font-weight:normal"> </span><span style="color:#54B9FF;font-style:italic;font-weight:normal">class</span><span style="color:#EEF0F9;font-style:normal;font-weight:normal"> cutlass.cute.nvgpu.cpasync </span><span style="color:#54B9FF;font-style:italic;font-weight:normal">import</span><span style="color:#EEF0F9;font-style:normal;font-weight:normal"> (</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    CopyBulkTensorTileG2SOp, CopyBulkTensorTileS2GOp </span><span style="color:#EEF0F98F;font-style:italic;font-weight:normal"># G2S &lt;-&gt; S2G means global and shared memory</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    CopyBulkTensorTileG2SMulticastOp, </span><span style="color:#EEF0F98F;font-style:italic;font-weight:normal"># plus multicast copy in reverse direction </span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    make_tiled_tma_atom,</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">)</span></div><div class="table-row"><span class="inline-block select-none"> </span></div></code></pre></div></div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify"><div class="relative group my-4 rounded-lg bg-[#1e1e1e] border border-white/10 shadow-lg"><button class="absolute right-3 top-3 z-10 p-2 rounded-md bg-white/10 text-gray-300 opacity-0 group-hover:opacity-100 transition-all duration-200 hover:bg-white/20 hover:text-white" aria-label="Copy code"><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="w-5 h-5"><path d="M1 9.50006C1 10.3285 1.67157 11.0001 2.5 11.0001H4L4 10.0001H2.5C2.22386 10.0001 2 9.7762 2 9.50006L2 2.50006C2 2.22392 2.22386 2.00006 2.5 2.00006L9.5 2.00006C9.77614 2.00006 10 2.22392 10 2.50006V4.00002H5.5C4.67158 4.00002 4 4.67159 4 5.50002V12.5C4 13.3284 4.67158 14 5.5 14H12.5C13.3284 14 14 13.3284 14 12.5V5.50002C14 4.67159 13.3284 4.00002 12.5 4.00002H11V2.50006C11 1.67163 10.3284 1.00006 9.5 1.00006H2.5C1.67157 1.00006 1 1.67163 1 2.50006V9.50006ZM5 5.50002C5 5.22388 5.22386 5.00002 5.5 5.00002H12.5C12.7761 5.00002 13 5.22388 13 5.50002V12.5C13 12.7762 12.7761 13 12.5 13H5.5C5.22386 13 5 12.7762 5 12.5V5.50002Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg></button><pre class="overflow-x-auto my-5 py-4 px-4 text-sm font-mono leading-relaxed"><code class=""><div class="table-row"><span style="color:#EEF0F98F;font-style:italic;font-weight:normal"># Pipeline </span></div><div class="table-row"><span style="color:#54B9FF;font-style:italic;font-weight:normal">from</span><span style="color:#EEF0F9;font-style:normal;font-weight:normal"> cutlass.pipeline </span><span style="color:#54B9FF;font-style:italic;font-weight:normal">import</span><span style="color:#EEF0F9;font-style:normal;font-weight:normal"> (</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    PipelineProducer, PipelineConsumer,</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    PipelineAsync,   </span><span style="color:#EEF0F98F;font-style:italic;font-weight:normal"># async producer-consumer pattern</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    PipelineTmaAsync,</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    PipelineTmaUmma,</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    PipelineAsyncUmma,</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    PipelineUmmaAsync,</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">)</span></div><div class="table-row"><span class="inline-block select-none"> </span></div><div class="table-row"><span style="color:#EEF0F98F;font-style:italic;font-weight:normal"># useful util</span></div><div class="table-row"><span style="color:#54B9FF;font-style:italic;font-weight:normal">from</span><span style="color:#EEF0F9;font-style:normal;font-weight:normal"> cutlass.utils </span><span style="color:#54B9FF;font-style:italic;font-weight:normal">import</span><span style="color:#EEF0F9;font-style:normal;font-weight:normal"> (</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    SmemAllocator,</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    TmemAllocator,</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    HardwareInfo,</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    print_latex,</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    print_latex_tv</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    </span><span style="color:#EEF0F98F;font-style:italic;font-weight:normal"># also things like cutlass.utils.sm100.make_smem_layout_a</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">)</span></div><div class="table-row"><span class="inline-block select-none"> </span></div></code></pre></div></div><div class="indent-10 md:mt-5 grow text-lg font-medium text-base text-justify"><div class="relative group my-4 rounded-lg bg-[#1e1e1e] border border-white/10 shadow-lg"><button class="absolute right-3 top-3 z-10 p-2 rounded-md bg-white/10 text-gray-300 opacity-0 group-hover:opacity-100 transition-all duration-200 hover:bg-white/20 hover:text-white" aria-label="Copy code"><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="w-5 h-5"><path d="M1 9.50006C1 10.3285 1.67157 11.0001 2.5 11.0001H4L4 10.0001H2.5C2.22386 10.0001 2 9.7762 2 9.50006L2 2.50006C2 2.22392 2.22386 2.00006 2.5 2.00006L9.5 2.00006C9.77614 2.00006 10 2.22392 10 2.50006V4.00002H5.5C4.67158 4.00002 4 4.67159 4 5.50002V12.5C4 13.3284 4.67158 14 5.5 14H12.5C13.3284 14 14 13.3284 14 12.5V5.50002C14 4.67159 13.3284 4.00002 12.5 4.00002H11V2.50006C11 1.67163 10.3284 1.00006 9.5 1.00006H2.5C1.67157 1.00006 1 1.67163 1 2.50006V9.50006ZM5 5.50002C5 5.22388 5.22386 5.00002 5.5 5.00002H12.5C12.7761 5.00002 13 5.22388 13 5.50002V12.5C13 12.7762 12.7761 13 12.5 13H5.5C5.22386 13 5 12.7762 5 12.5V5.50002Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg></button><pre class="overflow-x-auto my-5 py-4 px-4 text-sm font-mono leading-relaxed"><code class=""><div class="table-row"><span style="color:#54B9FF;font-style:italic;font-weight:normal">from</span><span style="color:#EEF0F9;font-style:normal;font-weight:normal"> cutlass.cute </span><span style="color:#54B9FF;font-style:italic;font-weight:normal">import</span><span style="color:#EEF0F9;font-style:normal;font-weight:normal"> (</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    make_layout,</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    zipped_product,</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    Swizzle,</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    gemm,</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    depth, rank, size, </span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    struct, </span><span style="color:#EEF0F98F;font-style:italic;font-weight:normal"># decorator</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    printf, ceil_div </span><span style="color:#EEF0F98F;font-style:italic;font-weight:normal"># etc </span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">)</span></div><div class="table-row"><span class="inline-block select-none"> </span></div><div class="table-row"><span style="color:#54B9FF;font-style:italic;font-weight:normal">from</span><span style="color:#EEF0F9;font-style:normal;font-weight:normal"> cutlass.cute.arch </span><span style="color:#54B9FF;font-style:italic;font-weight:normal">import</span><span style="color:#EEF0F9;font-style:normal;font-weight:normal"> (</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    thread_idx,</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    block_idx, block_dim,</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    warp.idx </span><span style="color:#EEF0F98F;font-style:italic;font-weight:normal"># etc</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">)</span></div><div class="table-row"><span class="inline-block select-none"> </span></div><div class="table-row"><span style="color:#54B9FF;font-style:italic;font-weight:normal">from</span><span style="color:#EEF0F9;font-style:normal;font-weight:normal"> cutlass.cute.runtime </span><span style="color:#54B9FF;font-style:italic;font-weight:normal">import</span><span style="color:#EEF0F9;font-style:normal;font-weight:normal"> (</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    from_dlpack,</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">    make_ptr</span></div><div class="table-row"><span style="color:#EEF0F9;font-style:normal;font-weight:normal">)</span></div><div class="table-row"><span class="inline-block select-none"> </span></div></code></pre></div></div></div></div></div></div></div><div class="flex justify-center my-5 md:my-10">Claire Zhao © 2026</div></div><!--$--><!--/$--><script src="/_next/static/chunks/be439e04947a2cba.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[48523,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"default\"]\n3:I[82281,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"default\"]\n4:I[1565,[\"/_next/static/chunks/e02326bdb730da03.js\",\"/_next/static/chunks/bc166ea53d390db7.js\",\"/_next/static/chunks/5ad9eb95768fc0a4.js\"],\"default\"]\n1b:I[75067,[],\"default\"]\n:HL[\"/_next/static/chunks/1fcb9d377ba4db76.css\",\"style\"]\n:HL[\"/_next/static/chunks/4057cc9dbcc744c0.css\",\"style\"]\n:HL[\"/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"JuMVSYJ3RIXYyuKTjUJ0h\",\"c\":[\"\",\"ai\",\"cute-dsl\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"ai\",{\"children\":[\"cute-dsl\",{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/1fcb9d377ba4db76.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/4057cc9dbcc744c0.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col justify-center min-h-screen\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-row justify-center pt-15\",\"children\":[\"$\",\"$L4\",null,{}]}],[\"$\",\"div\",null,{\"className\":\"grow flex justify-center w-full pt-10 md:pt-15 px-3\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col w-full max-w-full md:max-w-2xl lg:max-w-3xl\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-3xl font-bold flex justify-center\",\"children\":\"Programming Blackwell Tensor Core with Cutlass and CuTe DSL\"}],[\"$\",\"div\",null,{\"className\":\"text-sm flex justify-center pt-5\",\"children\":null}],[\"$\",\"div\",null,{\"className\":\"\",\"children\":[[\"$\",\"div\",\"0\",{\"children\":[\"$undefined\",[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"The CuTe \",[\"$\",\"span\",null,{\"className\":\"italic\",\"children\":\"domain specific language\"}],\" (DSL) provides a Python interface that enables efficient matrix muliplication GPU kernels implementation leveraging the tile based abstraction. The DSL offers Blackwell specific support in the \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"cutlass.cute.nvgpu.tcgen05\"}],\" module. Since our discussions about this module will largely be hardware aware, let us preface the discussion by an excusrion of Blackwell tensor core programming at the more fundamental \",[\"$\",\"span\",null,{\"className\":\"italic\",\"children\":\"parallel thread excution\"}],\" (PTX) virtual instruction set level.\"]}]}]]}],[\"$\",\"div\",\"1\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Tensor Memory\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"A cooperative thread array (CTA) is a set of threads that excute a kernel concurrently or in parallel. A \",\"$L5\",\" of a CTA is a maximum subset of the set of threads in the CTA that can execute the same instruction simultaneously. In common hardware, a warp is a \",\"$L6\",\" thread subset of the CTA. Threads in a CTA can be organized into a block of 1D, 2D, or 3D block of dimensions \",\"$L7\",\" and each thread indexed by a tuple \",\"$L8\",\". If \",\"$L9\",\" then the CTA is a 1D block and so forth. One can enumerate the threads in an \",\"$La\",\" CTA by \",\"$Lb\",\"We can enumerate all threads in a CTA and every four consecutive warps where the begining thread has thread ID \",\"$Lc\",\" is called a is called a \",\"$Ld\",\". The warps in a warp group are associated with a \",\"$Le\",\" from \",\"$Lf\",\".\"]}],\"$L10\"]}]]}],\"$L11\",\"$L12\",\"$L13\",\"$L14\"]}]]}]}],\"$L15\"]}],[\"$L16\",\"$L17\",\"$L18\"],\"$L19\"]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],\"$L1a\",false]],\"m\":\"$undefined\",\"G\":[\"$1b\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"1c:I[96045,[\"/_next/static/chunks/e02326bdb730da03.js\",\"/_next/static/chunks/bc166ea53d390db7.js\",\"/_next/static/chunks/5ad9eb95768fc0a4.js\"],\"default\"]\n20:I[56691,[\"/_next/static/chunks/e02326bdb730da03.js\",\"/_next/static/chunks/bc166ea53d390db7.js\",\"/_next/static/chunks/5ad9eb95768fc0a4.js\"],\"default\"]\n21:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"OutletBoundary\"]\n22:\"$Sreact.suspense\"\n24:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"ViewportBoundary\"]\n26:I[47259,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"MetadataBoundary\"]\n5:[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"warp\"}]\n6:[\"$\",\"$L1c\",null,{\"latex\":\"32\",\"className\":\"mt-10\"}]\n7:[\"$\",\"$L1c\",null,{\"latex\":\"X\\\\times Y\\\\times Z\",\"className\":\"mt-10\"}]\n8:[\"$\",\"$L1c\",null,{\"latex\":\"(i, j, k)\",\"className\":\"mt-10\"}]\n9:[\"$\",\"$L1c\",null,{\"latex\":\"Y=Z=1\",\"className\":\"mt-10\"}]\na:[\"$\",\"$L1c\",null,{\"latex\":\"X\\\\times Y\\\\times Z\",\"className\":\"mt-10\"}]\nb:[\"$\",\"$L1c\",null,{\"latex\":\"(i,j,k)\\\\mapsto i + jX + kXY\",\"displayMode\":true,\"className\":\"mt-10\"}]\nc:[\"$\",\"$L1c\",null,{\"latex\":\"I\\\\equiv 0 \\\\mod 128\",\"className\":\"mt-10\"}]\nd:[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"warp group\"}]\ne:[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"warp rank\"}]\nf:[\"$\",\"$L1c\",null,{\"latex\":\"0\\\\sim 3\",\"className\":\"mt-10\"}]\n"])</script><script>self.__next_f.push([1,"10:[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"For each cooperative thread array (CTA), the structure of tensor memory is a \",[\"$\",\"$L1c\",null,{\"latex\":\"128 \\\\times 512\",\"className\":\"mt-10\"}],\" matrix of \",[\"$\",\"$L1c\",null,{\"latex\":\"32\",\"className\":\"mt-10\"}],\" bit cells. Each of the \",[\"$\",\"$L1c\",null,{\"latex\":\"128\",\"className\":\"mt-10\"}],\" rows in tensor memory is also referred to as a \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"lane\"}],\", which is \",[\"$\",\"$L1c\",null,{\"latex\":\"2\",\"className\":\"mt-10\"}],\" KB in size. Thus the lanes are in bijective correspondence with the threads in a warp group. In fact the lanes are divided between the warps, so that warp rank \",[\"$\",\"$L1c\",null,{\"latex\":\"0\",\"className\":\"mt-10\"}],\" can only access lanes \",[\"$\",\"$L1c\",null,{\"latex\":\"0-31\",\"className\":\"mt-10\"}],\" using \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"ld\"}],\" load, \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"st\"}],\" store,     \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"cp\"}],\" copy tcgen05 instructions which are issued on inputs of lane \",[\"$\",\"$L1c\",null,{\"latex\":\"\\\\times\",\"className\":\"mt-10\"}],\" size. Each warp can access all columns within is accessible lane. Tensor memory allocation and deallocation use the \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"alloc\"}],\" and \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"dealloc\"}],\" instructions and are allocated in units of \",[\"$\",\"$L1c\",null,{\"latex\":\"32\",\"className\":\"mt-10\"}],\" columns, and the number of allocated columns must be a power of \",[\"$\",\"$L1c\",null,{\"latex\":\"2\",\"className\":\"mt-10\"}],\". Thus when a column is allocated, each of its \",[\"$\",\"$L1c\",null,{\"latex\":\"128\",\"className\":\"mt-10\"}],\" lanes are allocated.\"]}]\n"])</script><script>self.__next_f.push([1,"11:[\"$\",\"div\",\"2\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Tensor Memory Accelerator\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"The instruction \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"cp.async.bulk.tensor\"}],\" is used to copy tensor asynchronously between different types of memory locations (ex. from global to shared memory and back). This is facilitated by a hardware unit called the tensor memory accelerator (TMA). CUDA exposes this instruction via \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"cuda::memcpy_async\"}],\" which can be issued by a single thread within a warp and the synchronization mechanism is \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"cuda::barrier\"}],\". The PTX instruction also allows copying from global memory to the shared memory of \",[\"$\",\"span\",null,{\"className\":\"italic\",\"children\":\"multiple\"}],\" CTAs in a cluster, this is so called multicasting and is done by invoking the instruction modifier \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\".multicast::cluster\"}],\". One can even copy to distributed shared memory.\"]}]}]]}]\n"])</script><script>self.__next_f.push([1,"12:[\"$\",\"div\",\"3\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Matrix Multiplication Instructions\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Matrix multiply and accumulate\"}],\" (MMA) are excuted by \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"mma\"}],\" instruction which depend on a number launch configurations, such as the data type of input pair of matrices and their shapes, the data type of accumlation or output matrix. Support for warp-specialization, whether one or two CTAs cooperate, and whether the matmul is dense or sparse. For instance 2CTA FP16 dense MMA can have largest shape \",[\"$\",\"$L1c\",null,{\"latex\":\"M\\\\times N\\\\times K=256 \\\\times 256 \\\\times 16\",\"className\":\"mt-10\"}],\" while NVFP4 dense MMA has largest shape \",[\"$\",\"$L1c\",null,{\"latex\":\"M\\\\times N\\\\times K=256\\\\times 256 \\\\times 64 \",\"className\":\"mt-10\"}]]}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"MMA configuration are stored on register as a 32 bit \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"instruction descriptor\"}],\". For example bits 13 to 16 describe whether the input matrices needs to be \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"transposed\"}],\" or \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"negated\"}],\". Other data store includes sparsity, input output data type.\"]}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"MAA supports \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"block scaled\"}],\" matrix multiplication for data types such as \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"mxf4\"}],\" and \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"nvf4\"}],\".\"]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"13:[\"$\",\"div\",\"4\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"Granularity and Synchronization\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"TensorCore Gen 5 Instructions such as \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"mma\"}],\", \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"cp\"}],\" can be issued by a single thread within a CTA or CTA-Pair. Instructions such as \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"alloc\"}],\" and \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"dealloc\"}],\" can be issued from a warp of a pair of warps one in each CTA-pair. Each warp can access a quarter of the tensor memory via \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"ld\"}],\" and \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"st\"}],\" so a warp group is required for complete tensor memory access. These characterize the instruction issuing granularity.\"]}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":[\"Instructions can be synchronous (alloc/dealloc, fence) or asynchronous (cp, mma, ld, st). Special pairs of asyncrhonous instructions, called \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"pipelined instructions\"}],\" are guarenteed to excute in the order they are issued, an example is \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"tcgen05.copy.cta_group::N\"}],\" and \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"tcgen05.mma.cta_group::N\"}],\" for the same N.\"]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"14:[\"$\",\"div\",\"5\",{\"children\":[[\"$\",\"div\",null,{\"className\":\" pt-20 font-bold text-2xl\",\"id\":\"section-2\",\"children\":\"CuTe DSL\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify\",\"children\":[[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":\"$L1d\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":\"$L1e\"}],[\"$\",\"div\",null,{\"className\":\"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify\",\"children\":\"$L1f\"}]]}]]}]\n15:[\"$\",\"$L20\",null,{}]\n16:[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/e02326bdb730da03.js\",\"async\":true,\"nonce\":\"$undefined\"}]\n17:[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/bc166ea53d390db7.js\",\"async\":true,\"nonce\":\"$undefined\"}]\n18:[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/5ad9eb95768fc0a4.js\",\"async\":true,\"nonce\":\"$undefined\"}]\n19:[\"$\",\"$L21\",null,{\"children\":[\"$\",\"$22\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@23\"}]}]\n1a:[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L24\",null,{\"children\":\"$L25\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$L26\",null,{\"children\":[\"$\",\"$22\",null,{\"name\":\"Next.Metadata\",\"children\":\"$L27\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}]\n"])</script><script>self.__next_f.push([1,"25:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"28:I[87718,[\"/_next/static/chunks/988ea292de4c4c73.js\",\"/_next/static/chunks/7a959126392b4956.js\"],\"IconMark\"]\n23:null\n27:[[\"$\",\"title\",\"0\",{\"children\":\"Claire Zhao Blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Claire Zhao's Blog on AI and Math\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/favicon.ico?favicon.e10ff5c7.ico\",\"sizes\":\"1176x1032\",\"type\":\"image/x-icon\"}],[\"$\",\"$L28\",\"3\",{}]]\n"])</script><script>self.__next_f.push([1,"29:I[66832,[\"/_next/static/chunks/e02326bdb730da03.js\",\"/_next/static/chunks/bc166ea53d390db7.js\",\"/_next/static/chunks/5ad9eb95768fc0a4.js\"],\"default\"]\n"])</script><script>self.__next_f.push([1,"1d:[\"$\",\"$L29\",null,{\"code\":\"# useful methods in cutlass.cute.nvgpu\\nfrom cutlass.cute.nvgpu import (\\n    CopyUniversalOp, MmaUniversalOp,\\n    make_tiled_tma_atom_A, make_tiled_tma_atom_B\\n)\\n\\n# warp level operations\\nfrom cutlass.cute.nvgpu.warp import (\\n    MmaF16BF16Op, MmaMXF4Op, MmaMXF4NVF4Op  # MMA \\n    LdMatrix8x8x16bOp, StMatrix8x8x16bOp # load \u0026 store (\u0026more for other shapes)\\n)\\n\\n# warp group\\nfrom cutlass.cute.nvgpu.warpgroup import (\\n    MmaF16BF16Op,\\n    make_smem_layout_atom,\\n    fence, commit, wait_group\\n)\\n\\n# TMA\\nfrom class cutlass.cute.nvgpu.cpasync import (\\n    CopyBulkTensorTileG2SOp, CopyBulkTensorTileS2GOp # G2S \u003c-\u003e S2G means global and shared memory\\n    CopyBulkTensorTileG2SMulticastOp, # plus multicast copy in reverse direction \\n    make_tiled_tma_atom,\\n)\\n\",\"tokens\":[[{\"content\":\"# useful methods in cutlass.cute.nvgpu\",\"offset\":0,\"color\":\"#EEF0F98F\",\"fontStyle\":1}],[{\"content\":\"from\",\"offset\":39,\"color\":\"#54B9FF\",\"fontStyle\":1},{\"content\":\" cutlass.cute.nvgpu \",\"offset\":43,\"color\":\"#EEF0F9\",\"fontStyle\":0},{\"content\":\"import\",\"offset\":63,\"color\":\"#54B9FF\",\"fontStyle\":1},{\"content\":\" (\",\"offset\":69,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    CopyUniversalOp, MmaUniversalOp,\",\"offset\":72,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    make_tiled_tma_atom_A, make_tiled_tma_atom_B\",\"offset\":109,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\")\",\"offset\":158,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[],[{\"content\":\"# warp level operations\",\"offset\":161,\"color\":\"#EEF0F98F\",\"fontStyle\":1}],[{\"content\":\"from\",\"offset\":185,\"color\":\"#54B9FF\",\"fontStyle\":1},{\"content\":\" cutlass.cute.nvgpu.warp \",\"offset\":189,\"color\":\"#EEF0F9\",\"fontStyle\":0},{\"content\":\"import\",\"offset\":214,\"color\":\"#54B9FF\",\"fontStyle\":1},{\"content\":\" (\",\"offset\":220,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    MmaF16BF16Op, MmaMXF4Op, MmaMXF4NVF4Op  \",\"offset\":223,\"color\":\"#EEF0F9\",\"fontStyle\":0},{\"content\":\"# MMA \",\"offset\":267,\"color\":\"#EEF0F98F\",\"fontStyle\":1}],[{\"content\":\"    LdMatrix8x8x16bOp, StMatrix8x8x16bOp \",\"offset\":274,\"color\":\"#EEF0F9\",\"fontStyle\":0},{\"content\":\"# load \u0026 store (\u0026more for other shapes)\",\"offset\":315,\"color\":\"#EEF0F98F\",\"fontStyle\":1}],[{\"content\":\")\",\"offset\":355,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[],[{\"content\":\"# warp group\",\"offset\":358,\"color\":\"#EEF0F98F\",\"fontStyle\":1}],[{\"content\":\"from\",\"offset\":371,\"color\":\"#54B9FF\",\"fontStyle\":1},{\"content\":\" cutlass.cute.nvgpu.warpgroup \",\"offset\":375,\"color\":\"#EEF0F9\",\"fontStyle\":0},{\"content\":\"import\",\"offset\":405,\"color\":\"#54B9FF\",\"fontStyle\":1},{\"content\":\" (\",\"offset\":411,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    MmaF16BF16Op,\",\"offset\":414,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    make_smem_layout_atom,\",\"offset\":432,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    fence, commit, wait_group\",\"offset\":459,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\")\",\"offset\":489,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[],[{\"content\":\"# TMA\",\"offset\":492,\"color\":\"#EEF0F98F\",\"fontStyle\":1}],[{\"content\":\"from\",\"offset\":498,\"color\":\"#54B9FF\",\"fontStyle\":1},{\"content\":\" \",\"offset\":502,\"color\":\"#EEF0F9\",\"fontStyle\":0},{\"content\":\"class\",\"offset\":503,\"color\":\"#54B9FF\",\"fontStyle\":1},{\"content\":\" cutlass.cute.nvgpu.cpasync \",\"offset\":508,\"color\":\"#EEF0F9\",\"fontStyle\":0},{\"content\":\"import\",\"offset\":536,\"color\":\"#54B9FF\",\"fontStyle\":1},{\"content\":\" (\",\"offset\":542,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    CopyBulkTensorTileG2SOp, CopyBulkTensorTileS2GOp \",\"offset\":545,\"color\":\"#EEF0F9\",\"fontStyle\":0},{\"content\":\"# G2S \u003c-\u003e S2G means global and shared memory\",\"offset\":598,\"color\":\"#EEF0F98F\",\"fontStyle\":1}],[{\"content\":\"    CopyBulkTensorTileG2SMulticastOp, \",\"offset\":643,\"color\":\"#EEF0F9\",\"fontStyle\":0},{\"content\":\"# plus multicast copy in reverse direction \",\"offset\":681,\"color\":\"#EEF0F98F\",\"fontStyle\":1}],[{\"content\":\"    make_tiled_tma_atom,\",\"offset\":725,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\")\",\"offset\":750,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[]]}]\n"])</script><script>self.__next_f.push([1,"1e:[\"$\",\"$L29\",null,{\"code\":\"# Pipeline \\nfrom cutlass.pipeline import (\\n    PipelineProducer, PipelineConsumer,\\n    PipelineAsync,   # async producer-consumer pattern\\n    PipelineTmaAsync,\\n    PipelineTmaUmma,\\n    PipelineAsyncUmma,\\n    PipelineUmmaAsync,\\n)\\n\\n# useful util\\nfrom cutlass.utils import (\\n    SmemAllocator,\\n    TmemAllocator,\\n    HardwareInfo,\\n    print_latex,\\n    print_latex_tv\\n    # also things like cutlass.utils.sm100.make_smem_layout_a\\n)\\n\",\"tokens\":[[{\"content\":\"# Pipeline \",\"offset\":0,\"color\":\"#EEF0F98F\",\"fontStyle\":1}],[{\"content\":\"from\",\"offset\":12,\"color\":\"#54B9FF\",\"fontStyle\":1},{\"content\":\" cutlass.pipeline \",\"offset\":16,\"color\":\"#EEF0F9\",\"fontStyle\":0},{\"content\":\"import\",\"offset\":34,\"color\":\"#54B9FF\",\"fontStyle\":1},{\"content\":\" (\",\"offset\":40,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    PipelineProducer, PipelineConsumer,\",\"offset\":43,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    PipelineAsync,   \",\"offset\":83,\"color\":\"#EEF0F9\",\"fontStyle\":0},{\"content\":\"# async producer-consumer pattern\",\"offset\":104,\"color\":\"#EEF0F98F\",\"fontStyle\":1}],[{\"content\":\"    PipelineTmaAsync,\",\"offset\":138,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    PipelineTmaUmma,\",\"offset\":160,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    PipelineAsyncUmma,\",\"offset\":181,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    PipelineUmmaAsync,\",\"offset\":204,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\")\",\"offset\":227,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[],[{\"content\":\"# useful util\",\"offset\":230,\"color\":\"#EEF0F98F\",\"fontStyle\":1}],[{\"content\":\"from\",\"offset\":244,\"color\":\"#54B9FF\",\"fontStyle\":1},{\"content\":\" cutlass.utils \",\"offset\":248,\"color\":\"#EEF0F9\",\"fontStyle\":0},{\"content\":\"import\",\"offset\":263,\"color\":\"#54B9FF\",\"fontStyle\":1},{\"content\":\" (\",\"offset\":269,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    SmemAllocator,\",\"offset\":272,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    TmemAllocator,\",\"offset\":291,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    HardwareInfo,\",\"offset\":310,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    print_latex,\",\"offset\":328,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    print_latex_tv\",\"offset\":345,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    \",\"offset\":364,\"color\":\"#EEF0F9\",\"fontStyle\":0},{\"content\":\"# also things like cutlass.utils.sm100.make_smem_layout_a\",\"offset\":368,\"color\":\"#EEF0F98F\",\"fontStyle\":1}],[{\"content\":\")\",\"offset\":426,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[]]}]\n"])</script><script>self.__next_f.push([1,"1f:[\"$\",\"$L29\",null,{\"code\":\"from cutlass.cute import (\\n    make_layout,\\n    zipped_product,\\n    Swizzle,\\n    gemm,\\n    depth, rank, size, \\n    struct, # decorator\\n    printf, ceil_div # etc \\n)\\n\\nfrom cutlass.cute.arch import (\\n    thread_idx,\\n    block_idx, block_dim,\\n    warp.idx # etc\\n)\\n\\nfrom cutlass.cute.runtime import (\\n    from_dlpack,\\n    make_ptr\\n)\\n\",\"tokens\":[[{\"content\":\"from\",\"offset\":0,\"color\":\"#54B9FF\",\"fontStyle\":1},{\"content\":\" cutlass.cute \",\"offset\":4,\"color\":\"#EEF0F9\",\"fontStyle\":0},{\"content\":\"import\",\"offset\":18,\"color\":\"#54B9FF\",\"fontStyle\":1},{\"content\":\" (\",\"offset\":24,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    make_layout,\",\"offset\":27,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    zipped_product,\",\"offset\":44,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    Swizzle,\",\"offset\":64,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    gemm,\",\"offset\":77,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    depth, rank, size, \",\"offset\":87,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    struct, \",\"offset\":111,\"color\":\"#EEF0F9\",\"fontStyle\":0},{\"content\":\"# decorator\",\"offset\":123,\"color\":\"#EEF0F98F\",\"fontStyle\":1}],[{\"content\":\"    printf, ceil_div \",\"offset\":135,\"color\":\"#EEF0F9\",\"fontStyle\":0},{\"content\":\"# etc \",\"offset\":156,\"color\":\"#EEF0F98F\",\"fontStyle\":1}],[{\"content\":\")\",\"offset\":163,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[],[{\"content\":\"from\",\"offset\":166,\"color\":\"#54B9FF\",\"fontStyle\":1},{\"content\":\" cutlass.cute.arch \",\"offset\":170,\"color\":\"#EEF0F9\",\"fontStyle\":0},{\"content\":\"import\",\"offset\":189,\"color\":\"#54B9FF\",\"fontStyle\":1},{\"content\":\" (\",\"offset\":195,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    thread_idx,\",\"offset\":198,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    block_idx, block_dim,\",\"offset\":214,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    warp.idx \",\"offset\":240,\"color\":\"#EEF0F9\",\"fontStyle\":0},{\"content\":\"# etc\",\"offset\":253,\"color\":\"#EEF0F98F\",\"fontStyle\":1}],[{\"content\":\")\",\"offset\":259,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[],[{\"content\":\"from\",\"offset\":262,\"color\":\"#54B9FF\",\"fontStyle\":1},{\"content\":\" cutlass.cute.runtime \",\"offset\":266,\"color\":\"#EEF0F9\",\"fontStyle\":0},{\"content\":\"import\",\"offset\":288,\"color\":\"#54B9FF\",\"fontStyle\":1},{\"content\":\" (\",\"offset\":294,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    from_dlpack,\",\"offset\":297,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\"    make_ptr\",\"offset\":314,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[{\"content\":\")\",\"offset\":327,\"color\":\"#EEF0F9\",\"fontStyle\":0}],[]]}]\n"])</script></body></html>