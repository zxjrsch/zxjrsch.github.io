(globalThis.TURBOPACK||(globalThis.TURBOPACK=[])).push(["object"==typeof document?document.currentScript:void 0,646,(e,t,a)=>{"use strict";Object.defineProperty(a,"__esModule",{value:!0}),Object.defineProperty(a,"warnOnce",{enumerable:!0,get:function(){return i}});let i=e=>{}},72281,e=>{"use strict";var t=e.i(45360),a=e.i(87715),i=e.i(90320),s=e.i(96045);let n={title:(0,t.jsx)(t.Fragment,{children:"GPU Kernel Level Optimization for Efficient MoE Training"}),date:"Jan 20",tags:null,content:[{title:(0,t.jsx)(t.Fragment,{children:"Variable Length Grouped General Matrix Multiply"}),paragraph:(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["Recall that the definition of the general matrix multiply (GEMM) is the operation ",(0,t.jsx)(s.default,{latex:"\\alpha AB + \\beta Z",displayMode:!0,className:"mt-10"})," on matrices ",(0,t.jsx)(s.default,{latex:"A, B, Z",className:"mt-10"})," and scalers ",(0,t.jsx)(s.default,{latex:"\\alpha, \\beta",className:"mt-10"}),". We shall be concerned with the special case ",(0,t.jsx)(s.default,{latex:"C = AB",displayMode:!0,className:"mt-10"})," where ",(0,t.jsx)(s.default,{latex:"A",className:"mt-10"})," has dimensions ",(0,t.jsx)(s.default,{latex:"M\\times K",className:"mt-10"})," and ",(0,t.jsx)(s.default,{latex:"B",className:"mt-10"})," has dimensions ",(0,t.jsx)(s.default,{latex:"K\\times N",className:"mt-10"}),", and we say will denote the problem size by ",(0,t.jsx)(s.default,{latex:"(M, N, K)",className:"mt-10"}),"."]}),(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["This special case is relevant in the mixture of experts layers where we have a set of such multiplications for the different tokens routed to the various experts. Within an MoE layer, these multiplications have possibly with different shapes due to the tokens routed to each expert. For example if ",(0,t.jsx)(s.default,{latex:"M",className:"mt-10"})," is the token dimension, we shall refer to the problem as variable length M grouped GEMM."]}),(0,t.jsx)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:"The up-projection of a given expert requires tokens routed to it to be gathered, while the down-projection has all tokens already contiguously packaged."})]})},{title:(0,t.jsx)(t.Fragment,{children:"Forward and Backward MoE Kernels"}),paragraph:(0,t.jsx)(t.Fragment,{children:(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["Let us turn to the kernel design in the SonicMoE paper.  A total of ",(0,t.jsx)(s.default,{latex:"8",className:"mt-10"})," kernels are associated with the MoE layer. The forward computation has up-projection, down-projection, and expert aggregation kernels. Recall that in general, backwards gradients are computed with respective to the activations as well as for the weights. In the case of MoE, the backwards activation gradients are computed with respect to down-projection, up-projection and the MoE input. Two gradients with respect to up and down projection weight matrices are also computed. Moreover a top-k routing kernel is given in SonicMoE."]})})},{title:(0,t.jsx)(t.Fragment,{children:"Kernel (Epilogue) Fusion"}),paragraph:(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:"The first strategy is a kernel fusion of the input gather (during token routing) with input loading from global memory (HBM) to shared memory (SMEM). During this phase the token indices are first gathered, then the activations at those indicies are obtained via the cp.async PTX instruction. The authors of SonicMoE note that on Blackwell architecture, the 2-CTA GEMM requires the leader CTA to wait for gather to be complete in both CTAs, which leads to the following pipeline structure of the two CTAs: 1 warp to fetch token indices, 4 warps to gather, then 1 warp to relay the signal and perform MMA."}),(0,t.jsx)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:"The second fusion occurs during the epilogue associated with the MoE layer. Specifically, the SwiGLU is fused with forward up-projection, while the backward of SwiGLU is fused with the down-projection gradient computation."}),(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["Thirdly fusion involves the ",(0,t.jsx)(s.default,{latex:"dS",className:"mt-10"})," and ",(0,t.jsx)(s.default,{latex:"dH",className:"mt-10"})," associated with  the scores ",(0,t.jsx)(s.default,{latex:"S",className:"mt-10"})," and down-project ",(0,t.jsx)(s.default,{latex:"H",className:"mt-10"}),", respectively."]})]})},{title:(0,t.jsx)(t.Fragment,{children:"Computation and Async IO Overlap"}),paragraph:(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:"A useful strategy for achieving high tensor core throughput with intense epilogue is called ping-pong scheduling where warp groups that collectively issues WGMMA overlap IO and GEMM at a given moment and exchange roles at a future time. Such a strategy was used in Flash Attention 3, and is used in SonicMoE's down projection activation gradient computation, and the down project forward epilogue."}),(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["The paper also leverages async TMA for data movement between SMEM and GMEM in a subset of the ",(0,t.jsx)(s.default,{latex:"8",className:"mt-10"})," kernels, for example to store forward down project, among other places."]}),(0,t.jsx)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:"On Blackwell, the ping-poing strategy is used together with the TMEM and UMMA for better pipelining."})]})},{title:(0,t.jsx)(t.Fragment,{children:"Top-K Sorting Kernel"}),paragraph:(0,t.jsx)(t.Fragment,{children:(0,t.jsx)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:"The SonicMoE paper also provides a top-K kernel using bitonic sort, and has optional softmax fusion."})})}]};function o(){let e,s=(0,a.c)(1);return s[0]===Symbol.for("react.memo_cache_sentinel")?(e=(0,t.jsx)(i.default,{title:n.title,date:n.date,body:n.content}),s[0]=e):e=s[0],e}e.s(["default",()=>o],72281)}]);