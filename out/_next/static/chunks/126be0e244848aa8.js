(globalThis.TURBOPACK||(globalThis.TURBOPACK=[])).push(["object"==typeof document?document.currentScript:void 0,646,(e,t,a)=>{"use strict";Object.defineProperty(a,"__esModule",{value:!0}),Object.defineProperty(a,"warnOnce",{enumerable:!0,get:function(){return s}});let s=e=>{}},13068,e=>{"use strict";var t=e.i(45360),a=e.i(87715),s=e.i(32439),l=e.i(16160);let i={title:(0,t.jsx)(t.Fragment,{children:"Pretraining LLMs with NVFP4"}),date:null,tags:null,content:[{title:(0,t.jsx)(t.Fragment,{children:"Definition"}),paragraph:(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["Hardware represents floating point numbers in the form ",(0,t.jsx)(l.default,{latex:"(-1)^S\\times(1+M)\\times 2^{E-\\text{bias}}",displayMode:!0,className:"mt-10"})," where ",(0,t.jsx)(l.default,{latex:"S",className:"mt-10"})," determines the sign of the number and is stored with one bit. Depending on the datatype, a choice of ",(0,t.jsx)(l.default,{latex:"m",className:"mt-10"})," bits are devoted to the mantissa ",(0,t.jsx)(l.default,{latex:"M",className:"mt-10"}),", and ",(0,t.jsx)(l.default,{latex:"e",className:"mt-10"})," bits are devoted to the exponent ",(0,t.jsx)(l.default,{latex:"E",className:"mt-10"}),". The datatype thus determined requires ",(0,t.jsx)(l.default,{latex:"1+m+e",className:"mt-10"})," bits to represent a float, and is denoted EeMm. For instance, FP32 is E8M23, FP16 is E5M10 where as BF16 is E8M7."]}),(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["Quantization is the operation that maps numbers repreented in a given datatype to numbers in another datatype requiring less number ",(0,t.jsx)(l.default,{latex:"1+m+e",className:"mt-10"})," of bits. Thus quantization is a compression mechanism that reduces storage and communication footprint, and increase compute throughput. The efficiency gained via compression is to be trade-off with degradation in accuracy in one form or antoher."]}),(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["In practice one quantizes a set ",(0,t.jsx)(l.default,{latex:"A",className:"mt-10"})," of ",(0,t.jsx)(l.default,{latex:"n",className:"mt-10"})," numbers together. For example, to quantize real numbers ",(0,t.jsx)(l.default,{latex:"a\\in A",className:"mt-10"})," into ",(0,t.jsx)(l.default,{latex:"m",className:"mt-10"})," bit integers ",(0,t.jsx)(l.default,{latex:"-2^{m-1}\\leq q(a)\\leq 2^{m-1}-1",className:"mt-10"}),", we can choose the mapping ",(0,t.jsx)(l.default,{latex:"q(a)= \\text{NearestInteger}\\left(\\frac{2^{m-1}-1}{\\max_{i,j} |A_{ij}|}\\times a\\right)",displayMode:!0,className:"mt-10"})," In particular the scale factor ",(0,t.jsx)(l.default,{latex:"s=({2^{m-1}-1})/{\\max_{i,j} |A_{ij}|}",className:"mt-10"})," is chosen so that the maximum element of ",(0,t.jsx)(l.default,{latex:"A",className:"mt-10"})," gets mapped to ",(0,t.jsx)(l.default,{latex:"2^{m-1}-1",className:"mt-10"}),". Since the mapping ",(0,t.jsx)(l.default,{latex:"q",className:"mt-10"})," is many-to-one we can only hope to dequantize approximately, with error. More precisely, given an integer ",(0,t.jsx)(l.default,{latex:"k=q(a)",className:"mt-10"})," we can map it to ",(0,t.jsx)(l.default,{latex:"Q(k) = \\frac{\\max_{i,j} |A_{ij}|}{2^{m-1}-1}\\times k",displayMode:!0,className:"mt-10"})," and this introduces an error ",(0,t.jsx)(l.default,{latex:"|Q(q(a))-a|",className:"mt-10"}),"."]}),(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["In general given a pair of quantization and dequantization maps ",(0,t.jsx)(l.default,{latex:"(q, Q)",className:"mt-10"})," one can measure the ",(0,t.jsx)(l.default,{latex:"L^2",className:"mt-10"})," error  ",(0,t.jsx)(l.default,{latex:"\\mathbb{E}[||Q(q(x))-x||^2]",className:"mt-10"}),"."]})]})},{title:(0,t.jsx)(t.Fragment,{children:"Trainng in NVFP4"}),paragraph:(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:"Based on the papers and documentations I can find, here is a sketch of training LLMs in NVFP4."}),(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["Like FP4 and MXFP4, NVFP4 has the bit strcuture of E2M1. The distinction lies in how NVFP4 represent a set ",(0,t.jsx)(l.default,{latex:"T",className:"mt-10"})," of numbers, which we refer to as a tensor. In particular, NVFP4 partitions ",(0,t.jsx)(l.default,{latex:"T",className:"mt-10"})," into subsets of ",(0,t.jsx)(l.default,{latex:"16",className:"mt-10"})," numbers each called a block. Each block is associated with an 8-bit E4M3 number called a block scale factor ",(0,t.jsx)(l.default,{latex:"s",className:"mt-10"})," such that each one of the ",(0,t.jsx)(l.default,{latex:"16",className:"mt-10"})," four bit numbers ",(0,t.jsx)(l.default,{latex:"x_{quantized}",className:"mt-10"})," belonging to the same block is reconstructed with ",(0,t.jsx)(l.default,{latex:"x=s\\times x_{quantized}",className:"mt-10"})," . Additionally, a FP32 number is asspciated with the tensor ",(0,t.jsx)(l.default,{latex:"T",className:"mt-10"})," itself, called the tensor scale factor."]}),(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["By contrast each MXFP4 partitions consists of ",(0,t.jsx)(l.default,{latex:"32",className:"mt-10"})," numbers, and its block scale factor is E8M0 (i.e. round to the nearest power of two). It can be shown that the expected square error with E8M0 is larger than that of E4M3, with the tradeoff being E8M0 has less overhead. The said parititon and scaling in NVFP4 is handled by specialized tensor core harware."]}),(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["Given that E2M1 and E4M3 can represent numbers with maximum absolute value of ",(0,t.jsx)(l.default,{latex:"6",className:"mt-10"})," and ",(0,t.jsx)(l.default,{latex:"448",className:"mt-10"})," respectively, the tensor scale factor for a tensor ",(0,t.jsx)(l.default,{latex:"T_I",className:"mt-10"})," indexed by a set ",(0,t.jsx)(l.default,{latex:"I",className:"mt-10"})," is ",(0,t.jsx)(l.default,{latex:"s_{tensor} = \\frac{6\\times 448}{\\max_{i\\in I}|T_i|}",displayMode:!0,className:"mt-10"})," the tensor dequantization scale is ",(0,t.jsx)(l.default,{latex:"1/s_{tensor}",className:"mt-10"})," and is stored in FP32. Let ",(0,t.jsx)(l.default,{latex:"J\\subset I",className:"mt-10"})," be an indexing set for a block in ",(0,t.jsx)(l.default,{latex:"T",className:"mt-10"}),", the corresponding block scale factor that is ",(0,t.jsx)(l.default,{latex:"s_{block\\, J}=\\frac{6}{\\max_{j\\in J}|T_j|}",displayMode:!0,className:"mt-10"})," In fact, the block dequantization scale factor ",(0,t.jsx)(l.default,{latex:"\\delta_{\\text{block, J}}",className:"mt-10"})," is stored in FP8 on the tensor core as ",(0,t.jsx)(l.default,{latex:"\\delta_{\\text{block, J}} = e4m3\\left(\\frac{s_{tensor}}{s_{block\\, J}}\\right)",displayMode:!0,className:"mt-10"})]}),(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["Each block ",(0,t.jsx)(l.default,{latex:"T_J",className:"mt-10"})," gets quantized as ",(0,t.jsx)(l.default,{latex:"\\widehat{T}_J = \\text{fp4}(s_{block\\, J}\\cdot T_J)",displayMode:!0,className:"mt-10"})," and partial dot during GEMM product is computed as ",(0,t.jsx)(l.default,{latex:"\\lambda_{JK} (\\widehat{T_J} \\cdot \\widehat{T_K}) ",displayMode:!0,className:"mt-10"})," where ",(0,t.jsx)(l.default,{latex:"\\lambda_{JK}=\\delta_{\\text{block, J}} \\times \\delta_{\\text{block, K}}",displayMode:!0,className:"mt-10"})," After GEMM the tensor dequantization scales are applied."]}),(0,t.jsx)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:"There are experiments showing NVFP4 should be used in earlier layers of the forward pass direction of a transformer, while keeping later layers in higher precision."})]})},{title:(0,t.jsx)(t.Fragment,{children:"Random Hadamard Transform"}),paragraph:(0,t.jsx)(t.Fragment,{children:(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["Hadamard matrices of dimension ",(0,t.jsx)(l.default,{latex:"d=2^{k}",className:"mt-10"})," for an integer ",(0,t.jsx)(l.default,{latex:"k",className:"mt-10"})," satisfies ",(0,t.jsx)(l.default,{latex:"H_d={1}/{\\sqrt{2}}H_2\\otimes H_{d/2}",displayMode:!0,className:"mt-10"})," and ",(0,t.jsx)(l.default,{latex:"HH^\\top=I",className:"mt-10"}),". We shall consider a randomized Hadamard matrix ",(0,t.jsx)(l.default,{latex:"S_dH_d",className:"mt-10"})," where ",(0,t.jsx)(l.default,{latex:"S_d",className:"mt-10"})," is a diagonal matrix of values ",(0,t.jsx)(l.default,{latex:"\\pm 1",className:"mt-10"})," chosen uniformly at random. In training, instead of operating on tensors ",(0,t.jsx)(l.default,{latex:"T_I",className:"mt-10"})," one qpplies the above NVFP4 on the random Hadamard transformed tiles ",(0,t.jsx)(l.default,{latex:"(S_dH_d)T_J",displayMode:!0,className:"mt-10"})," of the tensor. In some experiments ",(0,t.jsx)(l.default,{latex:"d=16",className:"mt-10"})," is applied to inputs of weight gradient GEMM."]})})}]};function n(){let e,l=(0,a.c)(1);return l[0]===Symbol.for("react.memo_cache_sentinel")?(e=(0,t.jsx)(s.default,{title:i.title,date:i.date,body:i.content}),l[0]=e):e=l[0],e}e.s(["default",()=>n],13068)}]);