(globalThis.TURBOPACK||(globalThis.TURBOPACK=[])).push(["object"==typeof document?document.currentScript:void 0,646,(e,t,a)=>{"use strict";Object.defineProperty(a,"__esModule",{value:!0}),Object.defineProperty(a,"warnOnce",{enumerable:!0,get:function(){return i}});let i=e=>{}},72281,e=>{"use strict";var t=e.i(45360),a=e.i(87715),i=e.i(90320),s=e.i(96045);let n={title:(0,t.jsx)(t.Fragment,{children:"GPU Kernel Level Optimization for Efficient MoE Training"}),date:"Jan 20",tags:null,content:[{title:(0,t.jsx)(t.Fragment,{children:"Variable Length Grouped General Matrix Multiply"}),paragraph:(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["Recall that the definition of the general matrix multiply (GEMM) is the operation ",(0,t.jsx)(s.default,{latex:"\\alpha AB + \\beta Z",displayMode:!0,className:"mt-10"})," on matrices ",(0,t.jsx)(s.default,{latex:"A, B, Z",className:"mt-10"})," and scalers ",(0,t.jsx)(s.default,{latex:"\\alpha, \\beta",className:"mt-10"}),". We shall be concerned with the special case ",(0,t.jsx)(s.default,{latex:"C = AB",displayMode:!0,className:"mt-10"})," where ",(0,t.jsx)(s.default,{latex:"A",className:"mt-10"})," has dimensions ",(0,t.jsx)(s.default,{latex:"M\\times K",className:"mt-10"})," and ",(0,t.jsx)(s.default,{latex:"B",className:"mt-10"})," has dimensions ",(0,t.jsx)(s.default,{latex:"K\\times N",className:"mt-10"}),", and we say will denote the problem size by ",(0,t.jsx)(s.default,{latex:"(M, N, K)",className:"mt-10"}),"."]}),(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["This special case is relevant in the mixture of experts layers where we have a set of such multiplications for the different tokens routed to the various experts. Within an MoE layer, these multiplications have possibly with different shapes due to the tokens routed to each expert. For example if ",(0,t.jsx)(s.default,{latex:"M",className:"mt-10"})," is the token dimension, we shall refer to the problem as variable length M grouped GEMM."]}),(0,t.jsx)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:"The up-projection of a given expert requires tokens routed to it to be gathered, while the down-projection has all tokens already contiguously packaged."})]})},{title:(0,t.jsx)(t.Fragment,{children:"Kernel Design"}),paragraph:(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:"Let us turn to the kernel design in the SonicMoE paper. The first strategy is a kernel fusion of the input gather (during token routing) with input loading from global memory (HBM) to shared memory (SMEM). During this phase the token indices are first gathered, then the activations at those indicies are obtained via the cp.async PTX instruction. The authors of SonicMoE note that on Blackwell architecture, the 2-CTA GEMM requires the leader CTA to wait for gather to be complete in both CTAs, which leads to the following pipeline structure of the two CTAs: 1 warp to fetch token indices, 4 warps to gather, then 1 warp to relay the signal and perform MMA."}),(0,t.jsx)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:"The second fusion occurs during the epilogue associated with the MoE layer. Specifically, the SwiGLU is fused with forward up-projection, while the backward of SwiGLU is fused with the down-projection gradient computation."})]})},{title:(0,t.jsx)(t.Fragment,{children:"Top-K Sorting Kernel"}),paragraph:(0,t.jsx)(t.Fragment,{})}]};function l(){let e,s=(0,a.c)(1);return s[0]===Symbol.for("react.memo_cache_sentinel")?(e=(0,t.jsx)(i.default,{title:n.title,date:n.date,body:n.content}),s[0]=e):e=s[0],e}e.s(["default",()=>l],72281)}]);