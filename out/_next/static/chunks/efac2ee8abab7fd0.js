(globalThis.TURBOPACK||(globalThis.TURBOPACK=[])).push(["object"==typeof document?document.currentScript:void 0,646,(e,t,a)=>{"use strict";Object.defineProperty(a,"__esModule",{value:!0}),Object.defineProperty(a,"warnOnce",{enumerable:!0,get:function(){return s}});let s=e=>{}},90320,e=>{"use strict";var t=e.i(45360),a=e.i(87715),s=e.i(1565),l=e.i(56691);function i(e){let s,l,i,m=(0,a.c)(7),{sectionTitle:n,sectionContent:d}=e;return m[0]!==n?(s=n&&(0,t.jsx)("div",{className:" pt-20 font-bold text-2xl",id:"section-2",children:n}),m[0]=n,m[1]=s):s=m[1],m[2]!==d?(l=(0,t.jsx)("div",{className:"indent-10 md:mt-10 grow text-lg font-medium text-base text-justify",children:d}),m[2]=d,m[3]=l):l=m[3],m[4]!==s||m[5]!==l?(i=(0,t.jsxs)("div",{children:[s,l]}),m[4]=s,m[5]=l,m[6]=i):i=m[6],i}function m(e){let i,m,d,r,o,x,c,f,u=(0,a.c)(16),{title:h,date:j,body:p}=e;return u[0]===Symbol.for("react.memo_cache_sentinel")?(i=(0,t.jsx)("div",{className:"flex flex-row justify-center pt-15",children:(0,t.jsx)(s.default,{})}),u[0]=i):i=u[0],u[1]!==h?(m=(0,t.jsx)("div",{className:"text-3xl font-bold flex justify-center",children:h}),u[1]=h,u[2]=m):m=u[2],u[3]!==j?(d=(0,t.jsx)("div",{className:"text-sm flex justify-center pt-5",children:j}),u[3]=j,u[4]=d):d=u[4],u[5]!==p?(r=p.map(n),u[5]=p,u[6]=r):r=u[6],u[7]!==r?(o=(0,t.jsx)("div",{className:"",children:r}),u[7]=r,u[8]=o):o=u[8],u[9]!==m||u[10]!==d||u[11]!==o?(x=(0,t.jsx)("div",{className:"grow flex justify-center w-full pt-10 md:pt-15 px-3",children:(0,t.jsxs)("div",{className:"flex flex-col lg:min-w-1/2 lg:max-w-1/2",children:[m,d,o]})}),u[9]=m,u[10]=d,u[11]=o,u[12]=x):x=u[12],u[13]===Symbol.for("react.memo_cache_sentinel")?(c=(0,t.jsx)(l.default,{}),u[13]=c):c=u[13],u[14]!==x?(f=(0,t.jsxs)("div",{className:"flex flex-col justify-center min-h-screen",children:[i,x,c]}),u[14]=x,u[15]=f):f=u[15],f}function n(e,a){return(0,t.jsx)(i,{sectionTitle:e.title,sectionContent:e.paragraph},a)}e.s(["default",()=>m])},13068,e=>{"use strict";var t=e.i(45360),a=e.i(87715),s=e.i(90320),l=e.i(96045);let i={title:(0,t.jsx)(t.Fragment,{children:"Pretraining LLM in NVFP4"}),date:"Last Modified Jan 31",tags:null,content:[{paragraph:(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["Consider how a real number ",(0,t.jsx)(l.default,{latex:"x",className:"mt-10"})," can be represented on a computer. If ",(0,t.jsx)(l.default,{latex:"x \\neq 0",className:"mt-10"})," then ",(0,t.jsx)(l.default,{latex:"x=\\frac{x}{|x|}\\times |x|=(-1)^S\\times |x|",displayMode:!0,className:"mt-10"})," where ",(0,t.jsx)(l.default,{latex:"(-1)^S = x/|x|",className:"mt-10"})," is the sign of ",(0,t.jsx)(l.default,{latex:"x",className:"mt-10"})," and can be represented with one bit ",(0,t.jsx)(l.default,{latex:"S\\in\\{0, 1\\}",className:"mt-10"}),". For the positive number ",(0,t.jsx)(l.default,{latex:"a=|x|",className:"mt-10"})," there exists a smallest ",(0,t.jsx)(l.default,{latex:"E\\in\\mathbb{Z}",className:"mt-10"})," such that ",(0,t.jsx)(l.default,{latex:"x\\in[0,\\, 2^{E+1}]",className:"mt-10"}),". Now we can divide this interval into two equal halves. By definition of ",(0,t.jsx)(l.default,{latex:"E",className:"mt-10"})," we know that ",(0,t.jsx)(l.default,{latex:"a\\in [2^{E}, 2^{E+1}]",className:"mt-10"}),". We can again divide the interval ",(0,t.jsx)(l.default,{latex:"[2^{E}, 2^{E+1}]",className:"mt-10"})," into two equal halves and ",(0,t.jsx)(l.default,{latex:"x",className:"mt-10"})," lies in either the left half or the right half (to break the tie, if ",(0,t.jsx)(l.default,{latex:"a",className:"mt-10"})," lies in the middle, then we say it lies on the right half). As we make the sequence of interval partitions, the positive real number ",(0,t.jsx)(l.default,{latex:"a",className:"mt-10"})," lies in either the left or the right half. Associate left with ",(0,t.jsx)(l.default,{latex:"0",className:"mt-10"})," right with ",(0,t.jsx)(l.default,{latex:"1",className:"mt-10"})," we obtain a binary sequence ",(0,t.jsx)(l.default,{latex:"\\{a_j\\}_{j \\geq 1}",className:"mt-10"}),". It is clear that for the ",(0,t.jsx)(l.default,{latex:"n",className:"mt-10"}),"-th partition the situation is ",(0,t.jsx)(l.default,{latex:"a = 2^{E} + a_1\\times\\frac{2^{E}}{2}+\\dots+a_n\\times\\frac{2^{E}}{2^n}+\\varepsilon_n",displayMode:!0,className:"mt-10"})," where ",(0,t.jsx)(l.default,{latex:"0\\leq \\varepsilon_n < 2^{E-n}",className:"mt-10"})," is less than the length of the half interval in the ",(0,t.jsx)(l.default,{latex:"n",className:"mt-10"}),"-th partition. So the positive real number ",(0,t.jsx)(l.default,{latex:"a",className:"mt-10"}),", and a desired precision ",(0,t.jsx)(l.default,{latex:"\\varepsilon>0",className:"mt-10"}),", we can choose an integer ",(0,t.jsx)(l.default,{latex:"m > 0",className:"mt-10"})," such that ",(0,t.jsx)(l.default,{latex:"2^{E-m}<\\varepsilon",className:"mt-10"})," so that the number ",(0,t.jsx)(l.default,{latex:"b_m = 2^{E} + a_1\\times\\frac{2^{E}}{2}+\\dots+a_m\\times\\frac{2^{E}}{2^m}",displayMode:!0,className:"mt-10"})," which is representable by a binary sequence of length ",(0,t.jsx)(l.default,{latex:"m",className:"mt-10"}),", approximates ",(0,t.jsx)(l.default,{latex:"a",className:"mt-10"})," to within ",(0,t.jsx)(l.default,{latex:"\\varepsilon",className:"mt-10"}),", namely ",(0,t.jsx)(l.default,{latex:"0\\leq a -b_m < 2^{E-m}< \\varepsilon",displayMode:!0,className:"mt-10"})," Since ",(0,t.jsx)(l.default,{latex:"x=(-1)^S\\times b_m + \\varepsilon_m",className:"mt-10"})," we can write ",(0,t.jsx)(l.default,{latex:"x=(-1)^S\\times \\left(1+ \\frac{a_1}{2}+\\dots+\\frac{a_m}{2^m}\\right) \\times 2^E + \\varepsilon_m",displayMode:!0,className:"mt-10"})," Hardware approximates ",(0,t.jsx)(l.default,{latex:"x",className:"mt-10"})," by storing one bit ",(0,t.jsx)(l.default,{latex:"S",className:"mt-10"})," for the ",(0,t.jsx)("span",{className:"font-semibold",children:"sign"})," of ",(0,t.jsx)(l.default,{latex:"x",className:"mt-10"}),", another ",(0,t.jsx)(l.default,{latex:"m",className:"mt-10"})," bits ",(0,t.jsx)(l.default,{latex:"a_1\\dots a_m",className:"mt-10"})," called the mantissa of ",(0,t.jsx)(l.default,{latex:"x",className:"mt-10"}),", the bit length of which determines an upper bounds on the error ",(0,t.jsx)(l.default,{latex:"\\varepsilon_m",className:"mt-10"})," of approximating ",(0,t.jsx)(l.default,{latex:"|x|",className:"mt-10"})," by ",(0,t.jsx)(l.default,{latex:"b_m",className:"mt-10"}),", together with a representation of the ",(0,t.jsx)("span",{className:"font-semibold",children:"exponent"})," ",(0,t.jsx)(l.default,{latex:"E\\in\\mathbb{Z}",className:"mt-10"}),"."]}),(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["Now we need a scheme for representing the integer ",(0,t.jsx)(l.default,{latex:"E",className:"mt-10"})," in bits. Suppose that ",(0,t.jsx)(l.default,{latex:"k",className:"mt-10"})," bits are designated to store the exponent, then there are ",(0,t.jsx)(l.default,{latex:"2^k",className:"mt-10"})," configurations, meaning that the real number the float can approximate spans ",(0,t.jsx)(l.default,{latex:"k",className:"mt-10"})," orders of magnitudes. Naturally we want to choose ",(0,t.jsx)(l.default,{latex:"k",className:"mt-10"})," orders of magnitudes about the multiplicative unit ",(0,t.jsx)(l.default,{latex:"2^0=1",className:"mt-10"}),". Customarily we designate ",(0,t.jsx)(l.default,{latex:"2^{k-1}",className:"mt-10"})," of the ",(0,t.jsx)(l.default,{latex:"2^k",className:"mt-10"})," orders of magnitudes to be at or below the unit, and another ",(0,t.jsx)(l.default,{latex:"2^{k-1}",className:"mt-10"})," orders of magnitudes to be above the unit. The hardware implementation is to store the integer exponent ",(0,t.jsx)(l.default,{latex:"E",className:"mt-10"})," with an non-negative integer ",(0,t.jsx)(l.default,{latex:"E'",className:"mt-10"})," such that ",(0,t.jsx)(l.default,{latex:"E' = E + 2^{k-1}-1",displayMode:!0,className:"mt-10"})," this way the smallest order of magnitude (with respect this dtype) ",(0,t.jsx)(l.default,{latex:"E=1-2^{k-1}",className:"mt-10"})," is stored as ",(0,t.jsx)(l.default,{latex:"E'=0",className:"mt-10"})," whereas the largest order of magnitude ",(0,t.jsx)(l.default,{latex:"E=2^{k-1}",className:"mt-10"})," is stored as ",(0,t.jsx)(l.default,{latex:"E'=2^k",className:"mt-10"}),". This scheme is nothing but a bijection from ",(0,t.jsx)(l.default,{latex:"\\{-(2^{k-1}-1),\\dots, -1, 0, 1, \\dots, 2^{k-1}\\}\\longrightarrow \\{0, 1, \\dots, 2^k-1\\}",displayMode:!0,className:"mt-10"})," The quantity ",(0,t.jsx)(l.default,{latex:"2^{k-1}-1",className:"mt-10"})," is sometimes given the nondescript name ",(0,t.jsx)("span",{className:"font-semibold",children:"bias"})," just to be confusing! The mantissa ",(0,t.jsx)(l.default,{latex:"m",className:"mt-10"}),", which we have until now defined as the bit string ",(0,t.jsx)(l.default,{latex:"a_1\\dots a_m",className:"mt-10"})," should really be identified with the real number that it represents ",(0,t.jsx)(l.default,{latex:"M=\\frac{a_1}{2}+\\dots+\\frac{a_m}{2^m}",displayMode:!0,className:"mt-10"})," and together with ",(0,t.jsx)(l.default,{latex:"E' = E + 2^{k-1}-1",className:"mt-10"})," and the sign ",(0,t.jsx)(l.default,{latex:"S",className:"mt-10"})," represents the real number (approximately) on hardware as ",(0,t.jsx)(l.default,{latex:"(-1)^S\\times (1+M)\\times 2^{E' - \\text{bias}}",displayMode:!0,className:"mt-10"})]}),(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["The above analysis applies to ",(0,t.jsx)(l.default,{latex:"x\\neq 0",className:"mt-10"}),". Since ",(0,t.jsx)(l.default,{latex:"x=0",className:"mt-10"})," is a limit point of the set ",(0,t.jsx)(l.default,{latex:"\\{2^n\\}_{n\\in\\mathbb{Z}}",className:"mt-10"})," there does not exist a ",(0,t.jsx)("span",{className:"italic",children:"smallest"})," integer ",(0,t.jsx)(l.default,{latex:"k",className:"mt-10"})," such that ",(0,t.jsx)(l.default,{latex:"0\\in [0, 2^k]",className:"mt-10"}),". Suppose that a float dtype has ",(0,t.jsx)(l.default,{latex:"K",className:"mt-10"})," bits to store the mantissa, it is natural to approximate zero with the smallest element in the set ",(0,t.jsx)(l.default,{latex:"\\left\\{(1+M)\\times 2^{E'-\\text{bias}}:\\, 0 \\leq M \\leq \\sum_{j=1}^{K} 2^{-j}\\, ,\\,   E' \\in [0,\\,2^K -1]\\right\\}",displayMode:!0,className:"mt-10"})," the minimum value being ",(0,t.jsx)(l.default,{latex:"2^{1-2^{K-1}}",className:"mt-10"})," and corresponds ",(0,t.jsx)(l.default,{latex:"E'=0",className:"mt-10"})," and ",(0,t.jsx)(l.default,{latex:"M=0",className:"mt-10"}),". Equivalently zero can be approximated by the maximum of the set consisting the negative elements of the above. Therefore we have two representations ",(0,t.jsx)(l.default,{latex:"\\pm 0",className:"mt-10"})," of zero by the dtype. Now this argument is true to first-order, and needs slight modification when we introduce the so called ",(0,t.jsx)("span",{className:"font-semibold",children:"subnormal"})," numbers in practical floating point implementations. The idea is that ",(0,t.jsx)(l.default,{latex:"2^{-\\text{bias}}",className:"mt-10"})," is not the smallest order of magnitude we can represent if in our implementation we cutomzarily agree that when ",(0,t.jsx)(l.default,{latex:"E'=0",className:"mt-10"})," we replace ",(0,t.jsx)(l.default,{latex:"(1+M)\\times 2^{-\\text{bias}}",className:"mt-10"})," by ",(0,t.jsx)(l.default,{latex:"M\\times 2^{1-\\text{bias}}",className:"mt-10"})," where observe that ",(0,t.jsx)(l.default,{latex:"0\\leq M < 1",className:"mt-10"})," whereby ",(0,t.jsx)(l.default,{latex:"2^{1-\\text{bias}}",className:"mt-10"})," is the smallest ",(0,t.jsx)("span",{className:"italic",children:"normal"})," order of magnitude. If the mantissa has ",(0,t.jsx)(l.default,{latex:"K",className:"mt-10"})," bits, then the smallest subnormal number is ",(0,t.jsx)(l.default,{latex:"2^{-K}\\times 2^{1-\\text{bias}}",className:"mt-10"})," and the largest subnormal is ",(0,t.jsx)(l.default,{latex:"2^{1-\\text{bias}}\\times\\sum_{j=1}^K 2^{-j}",className:"mt-10"})," corresponding ",(0,t.jsx)(l.default,{latex:"E'=0",className:"mt-10"})," with mantissa ",(0,t.jsx)(l.default,{latex:"(0.0\\dots01)_2",className:"mt-10"})," and ",(0,t.jsx)(l.default,{latex:"(0.1\\dots1)_2",className:"mt-10"})," respecitively. The case ",(0,t.jsx)(l.default,{latex:"E'=0",className:"mt-10"})," with ",(0,t.jsx)(l.default,{latex:"M=0",className:"mt-10"})," is reserved to represent zero. Observe that such smaller than ",(0,t.jsx)(l.default,{latex:"2^{-\\text{bias}}",className:"mt-10"})," magnitudes are only expressible near zero, with subnormal numbers. That is, for a general real numbers ",(0,t.jsx)(l.default,{latex:"x\\neq 0",className:"mt-10"}),", the upper bound of error of approximation is still ",(0,t.jsx)(l.default,{latex:"0\\leq \\varepsilon_K < 2^{E'-\\text{bias}-K}",className:"mt-10"})," even though for numbers below the smallest normal the error of approximation has bound ",(0,t.jsx)(l.default,{latex:"0\\leq \\varepsilon_K < 2^{1-\\text{bias}-K}",className:"mt-10"}),"."]}),(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["Since any hardware floating point number consists of a finite number of bits, the float has a limited range of expressible order of magnitudes. For numbers outside this range, a data type sometimes designate bit patters for the sign, mantissa, and exponent to represent ",(0,t.jsx)(l.default,{latex:"\\pm \\infty",className:"mt-10"}),". However this is optional, for example in the so called ",(0,t.jsx)("span",{className:"italic",children:"microscaled"})," (MX) classes of floats, infinities are excluded in E4M3 MXFP8 but are included in E5M2 MXFP8. Other special values for a float are NaN or not a number, which designate the results of undefined operations such as ",(0,t.jsx)(l.default,{latex:"0/0",className:"mt-10"}),". How to implement NaN for a given float is a choice. In some designs, NaN is omitted, while in others only one bit pattern (up to equivalence in sign) ",(0,t.jsx)(l.default,{latex:"(M, E')",className:"mt-10"})," denotes NaN, and yet in others, multiple such pairs are treated NaN (compare E2M3, E4M3, E5M2 microscaled floats)."]}),(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["Here is a hand wavy but sometimes useful intuition regarding the size of gaps between int and floats. Let ",(0,t.jsx)(l.default,{latex:"k\\in\\mathbb{N}",className:"mt-10"}),", the gaps between consecutive Int-",(0,t.jsx)(l.default,{latex:"k",className:"mt-10"})," numbers are the same, whereas for Float-",(0,t.jsx)(l.default,{latex:"k",className:"mt-10"})," numbers with ",(0,t.jsx)(l.default,{latex:"K",className:"mt-10"})," bit mantissa, the ratio of gap sizes between nearby numbers are evenly spaced on a log-scale. This is meant in the sense that for a float ",(0,t.jsx)(l.default,{latex:"f=(S, M, E)",className:"mt-10"})," the nearby elements are spaced ",(0,t.jsx)(l.default,{latex:"\\pm 2^{E-K}\\,, \\pm 2^{E+1-K}, \\pm 2^{E+2-K}\\,,\\dots\\,,\\pm2^{E+K}",className:"mt-10"}),". In particular these gaps are larger by a factor of ",(0,t.jsx)(l.default,{latex:"2^E",className:"mt-10"})," depending on the exponent ",(0,t.jsx)(l.default,{latex:"E",className:"mt-10"})," for the float ",(0,t.jsx)(l.default,{latex:"f",className:"mt-10"}),"."]})]})},{title:(0,t.jsx)(t.Fragment,{children:"Definition"}),paragraph:(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["Hardware represents floating point numbers in the form ",(0,t.jsx)(l.default,{latex:"(-1)^S\\times(1+M)\\times 2^{E-\\text{bias}}",displayMode:!0,className:"mt-10"})," where ",(0,t.jsx)(l.default,{latex:"S",className:"mt-10"})," determines the sign of the number and is stored with one bit. Depending on the datatype, a choice of ",(0,t.jsx)(l.default,{latex:"m",className:"mt-10"})," bits are devoted to the mantissa ",(0,t.jsx)(l.default,{latex:"M",className:"mt-10"}),", and ",(0,t.jsx)(l.default,{latex:"e",className:"mt-10"})," bits are devoted to the exponent ",(0,t.jsx)(l.default,{latex:"E",className:"mt-10"}),". The datatype thus determined requires ",(0,t.jsx)(l.default,{latex:"1+m+e",className:"mt-10"})," bits to represent a float, and is denoted EeMm. For instance, FP32 is E8M23, FP16 is E5M10 where as BF16 is E8M7."]}),(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["Quantization is the operation that maps numbers repreented in a given datatype to numbers in another datatype requiring less number ",(0,t.jsx)(l.default,{latex:"1+m+e",className:"mt-10"})," of bits. Thus quantization is a compression mechanism that reduces storage and communication footprint, and increase compute throughput. The efficiency gained via compression is to be trade-off with degradation in accuracy in one form or antoher."]}),(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["In practice one quantizes a set ",(0,t.jsx)(l.default,{latex:"A",className:"mt-10"})," of ",(0,t.jsx)(l.default,{latex:"n",className:"mt-10"})," numbers together. For example, to quantize real numbers ",(0,t.jsx)(l.default,{latex:"a\\in A",className:"mt-10"})," into ",(0,t.jsx)(l.default,{latex:"m",className:"mt-10"})," bit integers ",(0,t.jsx)(l.default,{latex:"-2^{m-1}\\leq q(a)\\leq 2^{m-1}-1",className:"mt-10"}),", we can choose the mapping ",(0,t.jsx)(l.default,{latex:"q(a)= \\text{NearestInteger}\\left(\\frac{2^{m-1}-1}{\\max_{i,j} |A_{ij}|}\\times a\\right)",displayMode:!0,className:"mt-10"})," In particular the scale factor ",(0,t.jsx)(l.default,{latex:"s=({2^{m-1}-1})/{\\max_{i,j} |A_{ij}|}",className:"mt-10"})," is chosen so that the maximum element of ",(0,t.jsx)(l.default,{latex:"A",className:"mt-10"})," gets mapped to ",(0,t.jsx)(l.default,{latex:"2^{m-1}-1",className:"mt-10"}),". Since the mapping ",(0,t.jsx)(l.default,{latex:"q",className:"mt-10"})," is many-to-one we can only hope to dequantize approximately, with error. More precisely, given an integer ",(0,t.jsx)(l.default,{latex:"k=q(a)",className:"mt-10"})," we can map it to ",(0,t.jsx)(l.default,{latex:"Q(k) = \\frac{\\max_{i,j} |A_{ij}|}{2^{m-1}-1}\\times k",displayMode:!0,className:"mt-10"})," and this introduces an error ",(0,t.jsx)(l.default,{latex:"|Q(q(a))-a|",className:"mt-10"}),"."]}),(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["In general given a pair of quantization and dequantization maps ",(0,t.jsx)(l.default,{latex:"(q, Q)",className:"mt-10"})," one can measure the ",(0,t.jsx)(l.default,{latex:"L^2",className:"mt-10"})," error  ",(0,t.jsx)(l.default,{latex:"\\mathbb{E}[||Q(q(x))-x||^2]",className:"mt-10"}),"."]})]})},{title:(0,t.jsx)(t.Fragment,{children:"Trainng in NVFP4"}),paragraph:(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:"Based on the papers and documentations I can find, here is a sketch of training LLMs in NVFP4."}),(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["Like FP4 and MXFP4, NVFP4 has the bit strcuture of E2M1. The distinction lies in how NVFP4 represent a set ",(0,t.jsx)(l.default,{latex:"T",className:"mt-10"})," of numbers, which we refer to as a tensor. In particular, NVFP4 partitions ",(0,t.jsx)(l.default,{latex:"T",className:"mt-10"})," into subsets of ",(0,t.jsx)(l.default,{latex:"16",className:"mt-10"})," numbers each called a block. Each block is associated with an 8-bit E4M3 number called a block scale factor ",(0,t.jsx)(l.default,{latex:"s",className:"mt-10"})," such that each one of the ",(0,t.jsx)(l.default,{latex:"16",className:"mt-10"})," four bit numbers ",(0,t.jsx)(l.default,{latex:"x_{quantized}",className:"mt-10"})," belonging to the same block is reconstructed with ",(0,t.jsx)(l.default,{latex:"x=s\\times x_{quantized}",className:"mt-10"})," . Additionally, a FP32 number is asspciated with the tensor ",(0,t.jsx)(l.default,{latex:"T",className:"mt-10"})," itself, called the tensor scale factor."]}),(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["By contrast each MXFP4 partitions consists of ",(0,t.jsx)(l.default,{latex:"32",className:"mt-10"})," numbers, and its block scale factor is E8M0 (i.e. round to the nearest power of two). It can be shown that the expected square error with E8M0 is larger than that of E4M3, with the tradeoff being E8M0 has less overhead. The said parititon and scaling in NVFP4 is handled by specialized tensor core harware."]}),(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["Given that E2M1 and E4M3 can represent numbers with maximum absolute value of ",(0,t.jsx)(l.default,{latex:"6",className:"mt-10"})," and ",(0,t.jsx)(l.default,{latex:"448",className:"mt-10"})," respectively, the tensor scale factor for a tensor ",(0,t.jsx)(l.default,{latex:"T_I",className:"mt-10"})," indexed by a set ",(0,t.jsx)(l.default,{latex:"I",className:"mt-10"})," is ",(0,t.jsx)(l.default,{latex:"s_{tensor} = \\frac{6\\times 448}{\\max_{i\\in I}|T_i|}",displayMode:!0,className:"mt-10"})," the tensor dequantization scale is ",(0,t.jsx)(l.default,{latex:"1/s_{tensor}",className:"mt-10"})," and is stored in FP32. Let ",(0,t.jsx)(l.default,{latex:"J\\subset I",className:"mt-10"})," be an indexing set for a block in ",(0,t.jsx)(l.default,{latex:"T",className:"mt-10"}),", the corresponding block scale factor that is ",(0,t.jsx)(l.default,{latex:"s_{block\\, J}=\\frac{6}{\\max_{j\\in J}|T_j|}",displayMode:!0,className:"mt-10"})," In fact, the block dequantization scale factor ",(0,t.jsx)(l.default,{latex:"\\delta_{\\text{block, J}}",className:"mt-10"})," is stored in FP8 on the tensor core as ",(0,t.jsx)(l.default,{latex:"\\delta_{\\text{block, J}} = e4m3\\left(\\frac{s_{tensor}}{s_{block\\, J}}\\right)",displayMode:!0,className:"mt-10"})]}),(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["Each block ",(0,t.jsx)(l.default,{latex:"T_J",className:"mt-10"})," gets quantized as ",(0,t.jsx)(l.default,{latex:"\\widehat{T}_J = \\text{fp4}(s_{block\\, J}\\cdot T_J)",displayMode:!0,className:"mt-10"})," and partial dot during GEMM product is computed as ",(0,t.jsx)(l.default,{latex:"\\lambda_{JK} (\\widehat{T_J} \\cdot \\widehat{T_K}) ",displayMode:!0,className:"mt-10"})," where ",(0,t.jsx)(l.default,{latex:"\\lambda_{JK}=\\delta_{\\text{block, J}} \\times \\delta_{\\text{block, K}}",displayMode:!0,className:"mt-10"})," After GEMM the tensor dequantization scales are applied."]}),(0,t.jsx)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:"There are experiments showing NVFP4 should be used in earlier layers of the forward pass direction of a transformer, while keeping later layers in higher precision."})]})},{title:(0,t.jsx)(t.Fragment,{children:"Random Hadamard Transform"}),paragraph:(0,t.jsx)(t.Fragment,{children:(0,t.jsxs)("div",{className:"indent-10 md:mt-5 grow text-lg font-medium text-base text-justify",children:["Hadamard matrices of dimension ",(0,t.jsx)(l.default,{latex:"d=2^{k}",className:"mt-10"})," for an integer ",(0,t.jsx)(l.default,{latex:"k",className:"mt-10"})," satisfies ",(0,t.jsx)(l.default,{latex:"H_d={1}/{\\sqrt{2}}H_2\\otimes H_{d/2}",displayMode:!0,className:"mt-10"})," and ",(0,t.jsx)(l.default,{latex:"HH^\\top=I",className:"mt-10"}),". We shall consider a randomized Hadamard matrix ",(0,t.jsx)(l.default,{latex:"S_dH_d",className:"mt-10"})," where ",(0,t.jsx)(l.default,{latex:"S_d",className:"mt-10"})," is a diagonal matrix of values ",(0,t.jsx)(l.default,{latex:"\\pm 1",className:"mt-10"})," chosen uniformly at random. In training, instead of operating on tensors ",(0,t.jsx)(l.default,{latex:"T_I",className:"mt-10"})," one qpplies the above NVFP4 on the random Hadamard transformed tiles ",(0,t.jsx)(l.default,{latex:"(S_dH_d)T_J",displayMode:!0,className:"mt-10"})," of the tensor. In some experiments ",(0,t.jsx)(l.default,{latex:"d=16",className:"mt-10"})," is applied to inputs of weight gradient GEMM."]})})}]};function m(){let e,l=(0,a.c)(1);return l[0]===Symbol.for("react.memo_cache_sentinel")?(e=(0,t.jsx)(s.default,{title:i.title,date:i.date,body:i.content}),l[0]=e):e=l[0],e}e.s(["default",()=>m],13068)}]);